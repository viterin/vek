// Code generated by command: go run gen.go -out ../internal/functions/accel_avx2_amd64.s -stubs ../internal/functions/accel_avx2_amd64.go -pkg functions. DO NOT EDIT.

#include "textflag.h"

// func Add_AVX2_F64(x []float64, y []float64)
// Requires: AVX
TEXT ·Add_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB0_7
	CMPQ  DX, $0x10
	JAE   LBB0_3
	XORL  AX, AX
	JMP   LBB0_6

LBB0_3:
	MOVQ DX, AX
	ANDQ $-16, AX
	XORL CX, CX

LBB0_4:
	VMOVUPD (DI)(CX*8), Y0
	VMOVUPD 32(DI)(CX*8), Y1
	VMOVUPD 64(DI)(CX*8), Y2
	VMOVUPD 96(DI)(CX*8), Y3
	VADDPD  (SI)(CX*8), Y0, Y0
	VADDPD  32(SI)(CX*8), Y1, Y1
	VADDPD  64(SI)(CX*8), Y2, Y2
	VADDPD  96(SI)(CX*8), Y3, Y3
	VMOVUPD Y0, (DI)(CX*8)
	VMOVUPD Y1, 32(DI)(CX*8)
	VMOVUPD Y2, 64(DI)(CX*8)
	VMOVUPD Y3, 96(DI)(CX*8)
	ADDQ    $0x10, CX
	CMPQ    AX, CX
	JNE     LBB0_4
	CMPQ    AX, DX
	JE      LBB0_7

LBB0_6:
	VMOVSD (DI)(AX*8), X0
	VADDSD (SI)(AX*8), X0, X0
	VMOVSD X0, (DI)(AX*8)
	ADDQ   $0x01, AX
	CMPQ   DX, AX
	JNE    LBB0_6

LBB0_7:
	VZEROUPPER
	RET

// func Add_AVX2_F32(x []float32, y []float32)
// Requires: AVX
TEXT ·Add_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB1_7
	CMPQ  DX, $0x20
	JAE   LBB1_3
	XORL  AX, AX
	JMP   LBB1_6

LBB1_3:
	MOVQ DX, AX
	ANDQ $-32, AX
	XORL CX, CX

LBB1_4:
	VMOVUPS (DI)(CX*4), Y0
	VMOVUPS 32(DI)(CX*4), Y1
	VMOVUPS 64(DI)(CX*4), Y2
	VMOVUPS 96(DI)(CX*4), Y3
	VADDPS  (SI)(CX*4), Y0, Y0
	VADDPS  32(SI)(CX*4), Y1, Y1
	VADDPS  64(SI)(CX*4), Y2, Y2
	VADDPS  96(SI)(CX*4), Y3, Y3
	VMOVUPS Y0, (DI)(CX*4)
	VMOVUPS Y1, 32(DI)(CX*4)
	VMOVUPS Y2, 64(DI)(CX*4)
	VMOVUPS Y3, 96(DI)(CX*4)
	ADDQ    $0x20, CX
	CMPQ    AX, CX
	JNE     LBB1_4
	CMPQ    AX, DX
	JE      LBB1_7

LBB1_6:
	VMOVSS (DI)(AX*4), X0
	VADDSS (SI)(AX*4), X0, X0
	VMOVSS X0, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   DX, AX
	JNE    LBB1_6

LBB1_7:
	VZEROUPPER
	RET

// func AddNumber_AVX2_F64(x []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·AddNumber_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVSD a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB2_11
	CMPQ  SI, $0x10
	JAE   LBB2_3
	XORL  AX, AX
	JMP   LBB2_10

LBB2_3:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	LEAQ         -16(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x04, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB2_4
	MOVQ         R8, DX
	ANDQ         $-2, DX
	XORL         CX, CX

LBB2_6:
	VADDPD  (DI)(CX*8), Y1, Y2
	VADDPD  32(DI)(CX*8), Y1, Y3
	VADDPD  64(DI)(CX*8), Y1, Y4
	VADDPD  96(DI)(CX*8), Y1, Y5
	VMOVUPD Y2, (DI)(CX*8)
	VMOVUPD Y3, 32(DI)(CX*8)
	VMOVUPD Y4, 64(DI)(CX*8)
	VMOVUPD Y5, 96(DI)(CX*8)
	VADDPD  128(DI)(CX*8), Y1, Y2
	VADDPD  160(DI)(CX*8), Y1, Y3
	VADDPD  192(DI)(CX*8), Y1, Y4
	VADDPD  224(DI)(CX*8), Y1, Y5
	VMOVUPD Y2, 128(DI)(CX*8)
	VMOVUPD Y3, 160(DI)(CX*8)
	VMOVUPD Y4, 192(DI)(CX*8)
	VMOVUPD Y5, 224(DI)(CX*8)
	ADDQ    $0x20, CX
	ADDQ    $-2, DX
	JNE     LBB2_6
	TESTB   $0x01, R8
	JE      LBB2_9

LBB2_8:
	VADDPD  (DI)(CX*8), Y1, Y2
	VADDPD  32(DI)(CX*8), Y1, Y3
	VADDPD  64(DI)(CX*8), Y1, Y4
	VADDPD  96(DI)(CX*8), Y1, Y1
	VMOVUPD Y2, (DI)(CX*8)
	VMOVUPD Y3, 32(DI)(CX*8)
	VMOVUPD Y4, 64(DI)(CX*8)
	VMOVUPD Y1, 96(DI)(CX*8)

LBB2_9:
	CMPQ AX, SI
	JE   LBB2_11

LBB2_10:
	VADDSD (DI)(AX*8), X0, X1
	VMOVSD X1, (DI)(AX*8)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB2_10

LBB2_11:
	VZEROUPPER
	RET

LBB2_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB2_8
	JMP   LBB2_9

// func AddNumber_AVX2_F32(x []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·AddNumber_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), DI
	MOVSS a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB3_11
	CMPQ  SI, $0x20
	JAE   LBB3_3
	XORL  AX, AX
	JMP   LBB3_10

LBB3_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	LEAQ         -32(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x05, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB3_4
	MOVQ         R8, DX
	ANDQ         $-2, DX
	XORL         CX, CX

LBB3_6:
	VADDPS  (DI)(CX*4), Y1, Y2
	VADDPS  32(DI)(CX*4), Y1, Y3
	VADDPS  64(DI)(CX*4), Y1, Y4
	VADDPS  96(DI)(CX*4), Y1, Y5
	VMOVUPS Y2, (DI)(CX*4)
	VMOVUPS Y3, 32(DI)(CX*4)
	VMOVUPS Y4, 64(DI)(CX*4)
	VMOVUPS Y5, 96(DI)(CX*4)
	VADDPS  128(DI)(CX*4), Y1, Y2
	VADDPS  160(DI)(CX*4), Y1, Y3
	VADDPS  192(DI)(CX*4), Y1, Y4
	VADDPS  224(DI)(CX*4), Y1, Y5
	VMOVUPS Y2, 128(DI)(CX*4)
	VMOVUPS Y3, 160(DI)(CX*4)
	VMOVUPS Y4, 192(DI)(CX*4)
	VMOVUPS Y5, 224(DI)(CX*4)
	ADDQ    $0x40, CX
	ADDQ    $-2, DX
	JNE     LBB3_6
	TESTB   $0x01, R8
	JE      LBB3_9

LBB3_8:
	VADDPS  (DI)(CX*4), Y1, Y2
	VADDPS  32(DI)(CX*4), Y1, Y3
	VADDPS  64(DI)(CX*4), Y1, Y4
	VADDPS  96(DI)(CX*4), Y1, Y1
	VMOVUPS Y2, (DI)(CX*4)
	VMOVUPS Y3, 32(DI)(CX*4)
	VMOVUPS Y4, 64(DI)(CX*4)
	VMOVUPS Y1, 96(DI)(CX*4)

LBB3_9:
	CMPQ AX, SI
	JE   LBB3_11

LBB3_10:
	VADDSS (DI)(AX*4), X0, X1
	VMOVSS X1, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB3_10

LBB3_11:
	VZEROUPPER
	RET

LBB3_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB3_8
	JMP   LBB3_9

// func Sub_AVX2_F64(x []float64, y []float64)
// Requires: AVX
TEXT ·Sub_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB4_7
	CMPQ  DX, $0x10
	JAE   LBB4_3
	XORL  AX, AX
	JMP   LBB4_6

LBB4_3:
	MOVQ DX, AX
	ANDQ $-16, AX
	XORL CX, CX

LBB4_4:
	VMOVUPD (DI)(CX*8), Y0
	VMOVUPD 32(DI)(CX*8), Y1
	VMOVUPD 64(DI)(CX*8), Y2
	VMOVUPD 96(DI)(CX*8), Y3
	VSUBPD  (SI)(CX*8), Y0, Y0
	VSUBPD  32(SI)(CX*8), Y1, Y1
	VSUBPD  64(SI)(CX*8), Y2, Y2
	VSUBPD  96(SI)(CX*8), Y3, Y3
	VMOVUPD Y0, (DI)(CX*8)
	VMOVUPD Y1, 32(DI)(CX*8)
	VMOVUPD Y2, 64(DI)(CX*8)
	VMOVUPD Y3, 96(DI)(CX*8)
	ADDQ    $0x10, CX
	CMPQ    AX, CX
	JNE     LBB4_4
	CMPQ    AX, DX
	JE      LBB4_7

LBB4_6:
	VMOVSD (DI)(AX*8), X0
	VSUBSD (SI)(AX*8), X0, X0
	VMOVSD X0, (DI)(AX*8)
	ADDQ   $0x01, AX
	CMPQ   DX, AX
	JNE    LBB4_6

LBB4_7:
	VZEROUPPER
	RET

// func Sub_AVX2_F32(x []float32, y []float32)
// Requires: AVX
TEXT ·Sub_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB5_7
	CMPQ  DX, $0x20
	JAE   LBB5_3
	XORL  AX, AX
	JMP   LBB5_6

LBB5_3:
	MOVQ DX, AX
	ANDQ $-32, AX
	XORL CX, CX

LBB5_4:
	VMOVUPS (DI)(CX*4), Y0
	VMOVUPS 32(DI)(CX*4), Y1
	VMOVUPS 64(DI)(CX*4), Y2
	VMOVUPS 96(DI)(CX*4), Y3
	VSUBPS  (SI)(CX*4), Y0, Y0
	VSUBPS  32(SI)(CX*4), Y1, Y1
	VSUBPS  64(SI)(CX*4), Y2, Y2
	VSUBPS  96(SI)(CX*4), Y3, Y3
	VMOVUPS Y0, (DI)(CX*4)
	VMOVUPS Y1, 32(DI)(CX*4)
	VMOVUPS Y2, 64(DI)(CX*4)
	VMOVUPS Y3, 96(DI)(CX*4)
	ADDQ    $0x20, CX
	CMPQ    AX, CX
	JNE     LBB5_4
	CMPQ    AX, DX
	JE      LBB5_7

LBB5_6:
	VMOVSS (DI)(AX*4), X0
	VSUBSS (SI)(AX*4), X0, X0
	VMOVSS X0, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   DX, AX
	JNE    LBB5_6

LBB5_7:
	VZEROUPPER
	RET

// func SubNumber_AVX2_F64(x []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·SubNumber_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVSD a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB6_11
	CMPQ  SI, $0x10
	JAE   LBB6_3
	XORL  AX, AX
	JMP   LBB6_10

LBB6_3:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	LEAQ         -16(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x04, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB6_4
	MOVQ         R8, DX
	ANDQ         $-2, DX
	XORL         CX, CX

LBB6_6:
	VMOVUPD (DI)(CX*8), Y2
	VMOVUPD 32(DI)(CX*8), Y3
	VMOVUPD 64(DI)(CX*8), Y4
	VMOVUPD 96(DI)(CX*8), Y5
	VSUBPD  Y1, Y2, Y2
	VSUBPD  Y1, Y3, Y3
	VSUBPD  Y1, Y4, Y4
	VSUBPD  Y1, Y5, Y5
	VMOVUPD Y2, (DI)(CX*8)
	VMOVUPD Y3, 32(DI)(CX*8)
	VMOVUPD Y4, 64(DI)(CX*8)
	VMOVUPD Y5, 96(DI)(CX*8)
	VMOVUPD 128(DI)(CX*8), Y2
	VMOVUPD 160(DI)(CX*8), Y3
	VMOVUPD 192(DI)(CX*8), Y4
	VMOVUPD 224(DI)(CX*8), Y5
	VSUBPD  Y1, Y2, Y2
	VSUBPD  Y1, Y3, Y3
	VSUBPD  Y1, Y4, Y4
	VSUBPD  Y1, Y5, Y5
	VMOVUPD Y2, 128(DI)(CX*8)
	VMOVUPD Y3, 160(DI)(CX*8)
	VMOVUPD Y4, 192(DI)(CX*8)
	VMOVUPD Y5, 224(DI)(CX*8)
	ADDQ    $0x20, CX
	ADDQ    $-2, DX
	JNE     LBB6_6
	TESTB   $0x01, R8
	JE      LBB6_9

LBB6_8:
	VMOVUPD (DI)(CX*8), Y2
	VMOVUPD 32(DI)(CX*8), Y3
	VMOVUPD 64(DI)(CX*8), Y4
	VMOVUPD 96(DI)(CX*8), Y5
	VSUBPD  Y1, Y2, Y2
	VSUBPD  Y1, Y3, Y3
	VSUBPD  Y1, Y4, Y4
	VSUBPD  Y1, Y5, Y1
	VMOVUPD Y2, (DI)(CX*8)
	VMOVUPD Y3, 32(DI)(CX*8)
	VMOVUPD Y4, 64(DI)(CX*8)
	VMOVUPD Y1, 96(DI)(CX*8)

LBB6_9:
	CMPQ AX, SI
	JE   LBB6_11

LBB6_10:
	VMOVSD (DI)(AX*8), X1
	VSUBSD X0, X1, X1
	VMOVSD X1, (DI)(AX*8)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB6_10

LBB6_11:
	VZEROUPPER
	RET

LBB6_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB6_8
	JMP   LBB6_9

// func SubNumber_AVX2_F32(x []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·SubNumber_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), DI
	MOVSS a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB7_11
	CMPQ  SI, $0x20
	JAE   LBB7_3
	XORL  AX, AX
	JMP   LBB7_10

LBB7_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	LEAQ         -32(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x05, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB7_4
	MOVQ         R8, DX
	ANDQ         $-2, DX
	XORL         CX, CX

LBB7_6:
	VMOVUPS (DI)(CX*4), Y2
	VMOVUPS 32(DI)(CX*4), Y3
	VMOVUPS 64(DI)(CX*4), Y4
	VMOVUPS 96(DI)(CX*4), Y5
	VSUBPS  Y1, Y2, Y2
	VSUBPS  Y1, Y3, Y3
	VSUBPS  Y1, Y4, Y4
	VSUBPS  Y1, Y5, Y5
	VMOVUPS Y2, (DI)(CX*4)
	VMOVUPS Y3, 32(DI)(CX*4)
	VMOVUPS Y4, 64(DI)(CX*4)
	VMOVUPS Y5, 96(DI)(CX*4)
	VMOVUPS 128(DI)(CX*4), Y2
	VMOVUPS 160(DI)(CX*4), Y3
	VMOVUPS 192(DI)(CX*4), Y4
	VMOVUPS 224(DI)(CX*4), Y5
	VSUBPS  Y1, Y2, Y2
	VSUBPS  Y1, Y3, Y3
	VSUBPS  Y1, Y4, Y4
	VSUBPS  Y1, Y5, Y5
	VMOVUPS Y2, 128(DI)(CX*4)
	VMOVUPS Y3, 160(DI)(CX*4)
	VMOVUPS Y4, 192(DI)(CX*4)
	VMOVUPS Y5, 224(DI)(CX*4)
	ADDQ    $0x40, CX
	ADDQ    $-2, DX
	JNE     LBB7_6
	TESTB   $0x01, R8
	JE      LBB7_9

LBB7_8:
	VMOVUPS (DI)(CX*4), Y2
	VMOVUPS 32(DI)(CX*4), Y3
	VMOVUPS 64(DI)(CX*4), Y4
	VMOVUPS 96(DI)(CX*4), Y5
	VSUBPS  Y1, Y2, Y2
	VSUBPS  Y1, Y3, Y3
	VSUBPS  Y1, Y4, Y4
	VSUBPS  Y1, Y5, Y1
	VMOVUPS Y2, (DI)(CX*4)
	VMOVUPS Y3, 32(DI)(CX*4)
	VMOVUPS Y4, 64(DI)(CX*4)
	VMOVUPS Y1, 96(DI)(CX*4)

LBB7_9:
	CMPQ AX, SI
	JE   LBB7_11

LBB7_10:
	VMOVSS (DI)(AX*4), X1
	VSUBSS X0, X1, X1
	VMOVSS X1, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB7_10

LBB7_11:
	VZEROUPPER
	RET

LBB7_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB7_8
	JMP   LBB7_9

// func Mul_AVX2_F64(x []float64, y []float64)
// Requires: AVX
TEXT ·Mul_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB8_7
	CMPQ  DX, $0x10
	JAE   LBB8_3
	XORL  AX, AX
	JMP   LBB8_6

LBB8_3:
	MOVQ DX, AX
	ANDQ $-16, AX
	XORL CX, CX

LBB8_4:
	VMOVUPD (DI)(CX*8), Y0
	VMOVUPD 32(DI)(CX*8), Y1
	VMOVUPD 64(DI)(CX*8), Y2
	VMOVUPD 96(DI)(CX*8), Y3
	VMULPD  (SI)(CX*8), Y0, Y0
	VMULPD  32(SI)(CX*8), Y1, Y1
	VMULPD  64(SI)(CX*8), Y2, Y2
	VMULPD  96(SI)(CX*8), Y3, Y3
	VMOVUPD Y0, (DI)(CX*8)
	VMOVUPD Y1, 32(DI)(CX*8)
	VMOVUPD Y2, 64(DI)(CX*8)
	VMOVUPD Y3, 96(DI)(CX*8)
	ADDQ    $0x10, CX
	CMPQ    AX, CX
	JNE     LBB8_4
	CMPQ    AX, DX
	JE      LBB8_7

LBB8_6:
	VMOVSD (DI)(AX*8), X0
	VMULSD (SI)(AX*8), X0, X0
	VMOVSD X0, (DI)(AX*8)
	ADDQ   $0x01, AX
	CMPQ   DX, AX
	JNE    LBB8_6

LBB8_7:
	VZEROUPPER
	RET

// func Mul_AVX2_F32(x []float32, y []float32)
// Requires: AVX
TEXT ·Mul_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB9_7
	CMPQ  DX, $0x20
	JAE   LBB9_3
	XORL  AX, AX
	JMP   LBB9_6

LBB9_3:
	MOVQ DX, AX
	ANDQ $-32, AX
	XORL CX, CX

LBB9_4:
	VMOVUPS (DI)(CX*4), Y0
	VMOVUPS 32(DI)(CX*4), Y1
	VMOVUPS 64(DI)(CX*4), Y2
	VMOVUPS 96(DI)(CX*4), Y3
	VMULPS  (SI)(CX*4), Y0, Y0
	VMULPS  32(SI)(CX*4), Y1, Y1
	VMULPS  64(SI)(CX*4), Y2, Y2
	VMULPS  96(SI)(CX*4), Y3, Y3
	VMOVUPS Y0, (DI)(CX*4)
	VMOVUPS Y1, 32(DI)(CX*4)
	VMOVUPS Y2, 64(DI)(CX*4)
	VMOVUPS Y3, 96(DI)(CX*4)
	ADDQ    $0x20, CX
	CMPQ    AX, CX
	JNE     LBB9_4
	CMPQ    AX, DX
	JE      LBB9_7

LBB9_6:
	VMOVSS (DI)(AX*4), X0
	VMULSS (SI)(AX*4), X0, X0
	VMOVSS X0, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   DX, AX
	JNE    LBB9_6

LBB9_7:
	VZEROUPPER
	RET

// func MulNumber_AVX2_F64(x []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·MulNumber_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVSD a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB10_11
	CMPQ  SI, $0x10
	JAE   LBB10_3
	XORL  AX, AX
	JMP   LBB10_10

LBB10_3:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	LEAQ         -16(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x04, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB10_4
	MOVQ         R8, DX
	ANDQ         $-2, DX
	XORL         CX, CX

LBB10_6:
	VMULPD  (DI)(CX*8), Y1, Y2
	VMULPD  32(DI)(CX*8), Y1, Y3
	VMULPD  64(DI)(CX*8), Y1, Y4
	VMULPD  96(DI)(CX*8), Y1, Y5
	VMOVUPD Y2, (DI)(CX*8)
	VMOVUPD Y3, 32(DI)(CX*8)
	VMOVUPD Y4, 64(DI)(CX*8)
	VMOVUPD Y5, 96(DI)(CX*8)
	VMULPD  128(DI)(CX*8), Y1, Y2
	VMULPD  160(DI)(CX*8), Y1, Y3
	VMULPD  192(DI)(CX*8), Y1, Y4
	VMULPD  224(DI)(CX*8), Y1, Y5
	VMOVUPD Y2, 128(DI)(CX*8)
	VMOVUPD Y3, 160(DI)(CX*8)
	VMOVUPD Y4, 192(DI)(CX*8)
	VMOVUPD Y5, 224(DI)(CX*8)
	ADDQ    $0x20, CX
	ADDQ    $-2, DX
	JNE     LBB10_6
	TESTB   $0x01, R8
	JE      LBB10_9

LBB10_8:
	VMULPD  (DI)(CX*8), Y1, Y2
	VMULPD  32(DI)(CX*8), Y1, Y3
	VMULPD  64(DI)(CX*8), Y1, Y4
	VMULPD  96(DI)(CX*8), Y1, Y1
	VMOVUPD Y2, (DI)(CX*8)
	VMOVUPD Y3, 32(DI)(CX*8)
	VMOVUPD Y4, 64(DI)(CX*8)
	VMOVUPD Y1, 96(DI)(CX*8)

LBB10_9:
	CMPQ AX, SI
	JE   LBB10_11

LBB10_10:
	VMULSD (DI)(AX*8), X0, X1
	VMOVSD X1, (DI)(AX*8)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB10_10

LBB10_11:
	VZEROUPPER
	RET

LBB10_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB10_8
	JMP   LBB10_9

// func MulNumber_AVX2_F32(x []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·MulNumber_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), DI
	MOVSS a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB11_11
	CMPQ  SI, $0x20
	JAE   LBB11_3
	XORL  AX, AX
	JMP   LBB11_10

LBB11_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	LEAQ         -32(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x05, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB11_4
	MOVQ         R8, DX
	ANDQ         $-2, DX
	XORL         CX, CX

LBB11_6:
	VMULPS  (DI)(CX*4), Y1, Y2
	VMULPS  32(DI)(CX*4), Y1, Y3
	VMULPS  64(DI)(CX*4), Y1, Y4
	VMULPS  96(DI)(CX*4), Y1, Y5
	VMOVUPS Y2, (DI)(CX*4)
	VMOVUPS Y3, 32(DI)(CX*4)
	VMOVUPS Y4, 64(DI)(CX*4)
	VMOVUPS Y5, 96(DI)(CX*4)
	VMULPS  128(DI)(CX*4), Y1, Y2
	VMULPS  160(DI)(CX*4), Y1, Y3
	VMULPS  192(DI)(CX*4), Y1, Y4
	VMULPS  224(DI)(CX*4), Y1, Y5
	VMOVUPS Y2, 128(DI)(CX*4)
	VMOVUPS Y3, 160(DI)(CX*4)
	VMOVUPS Y4, 192(DI)(CX*4)
	VMOVUPS Y5, 224(DI)(CX*4)
	ADDQ    $0x40, CX
	ADDQ    $-2, DX
	JNE     LBB11_6
	TESTB   $0x01, R8
	JE      LBB11_9

LBB11_8:
	VMULPS  (DI)(CX*4), Y1, Y2
	VMULPS  32(DI)(CX*4), Y1, Y3
	VMULPS  64(DI)(CX*4), Y1, Y4
	VMULPS  96(DI)(CX*4), Y1, Y1
	VMOVUPS Y2, (DI)(CX*4)
	VMOVUPS Y3, 32(DI)(CX*4)
	VMOVUPS Y4, 64(DI)(CX*4)
	VMOVUPS Y1, 96(DI)(CX*4)

LBB11_9:
	CMPQ AX, SI
	JE   LBB11_11

LBB11_10:
	VMULSS (DI)(AX*4), X0, X1
	VMOVSS X1, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB11_10

LBB11_11:
	VZEROUPPER
	RET

LBB11_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB11_8
	JMP   LBB11_9

// func Div_AVX2_F64(x []float64, y []float64)
// Requires: AVX
TEXT ·Div_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB12_11
	CMPQ  DX, $0x04
	JAE   LBB12_3
	XORL  AX, AX
	JMP   LBB12_10

LBB12_3:
	MOVQ  DX, AX
	ANDQ  $-4, AX
	LEAQ  -4(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x02, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB12_4
	MOVQ  R8, R9
	ANDQ  $-2, R9
	XORL  CX, CX

LBB12_6:
	VMOVUPD (DI)(CX*8), Y0
	VDIVPD  (SI)(CX*8), Y0, Y0
	VMOVUPD 32(DI)(CX*8), Y1
	VMOVUPD Y0, (DI)(CX*8)
	VDIVPD  32(SI)(CX*8), Y1, Y0
	VMOVUPD Y0, 32(DI)(CX*8)
	ADDQ    $0x08, CX
	ADDQ    $-2, R9
	JNE     LBB12_6
	TESTB   $0x01, R8
	JE      LBB12_9

LBB12_8:
	VMOVUPD (DI)(CX*8), Y0
	VDIVPD  (SI)(CX*8), Y0, Y0
	VMOVUPD Y0, (DI)(CX*8)

LBB12_9:
	CMPQ AX, DX
	JE   LBB12_11

LBB12_10:
	VMOVSD (DI)(AX*8), X0
	VDIVSD (SI)(AX*8), X0, X0
	VMOVSD X0, (DI)(AX*8)
	ADDQ   $0x01, AX
	CMPQ   DX, AX
	JNE    LBB12_10

LBB12_11:
	VZEROUPPER
	RET

LBB12_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB12_8
	JMP   LBB12_9

// func Div_AVX2_F32(x []float32, y []float32)
// Requires: AVX, FMA3
TEXT ·Div_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB13_7
	CMPQ  DX, $0x20
	JAE   LBB13_3
	XORL  AX, AX
	JMP   LBB13_6

LBB13_3:
	MOVQ DX, AX
	ANDQ $-32, AX
	XORL CX, CX

LBB13_4:
	VMOVUPS      (SI)(CX*4), Y0
	VMOVUPS      32(SI)(CX*4), Y1
	VMOVUPS      64(SI)(CX*4), Y2
	VRCPPS       Y0, Y3
	VMOVUPS      96(SI)(CX*4), Y4
	VMOVUPS      (DI)(CX*4), Y5
	VMOVUPS      32(DI)(CX*4), Y6
	VMOVUPS      64(DI)(CX*4), Y7
	VMOVUPS      96(DI)(CX*4), Y8
	VMULPS       Y3, Y5, Y9
	VFMSUB213PS  Y5, Y9, Y0
	VFNMADD213PS Y9, Y3, Y0
	VRCPPS       Y1, Y3
	VMULPS       Y3, Y6, Y5
	VFMSUB213PS  Y6, Y5, Y1
	VRCPPS       Y2, Y6
	VFNMADD213PS Y5, Y3, Y1
	VMULPS       Y6, Y7, Y3
	VFMSUB213PS  Y7, Y3, Y2
	VFNMADD213PS Y3, Y6, Y2
	VRCPPS       Y4, Y3
	VMULPS       Y3, Y8, Y5
	VFMSUB213PS  Y8, Y5, Y4
	VFNMADD213PS Y5, Y3, Y4
	VMOVUPS      Y0, (DI)(CX*4)
	VMOVUPS      Y1, 32(DI)(CX*4)
	VMOVUPS      Y2, 64(DI)(CX*4)
	VMOVUPS      Y4, 96(DI)(CX*4)
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB13_4
	CMPQ         AX, DX
	JE           LBB13_7

LBB13_6:
	VMOVSS (DI)(AX*4), X0
	VDIVSS (SI)(AX*4), X0, X0
	VMOVSS X0, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   DX, AX
	JNE    LBB13_6

LBB13_7:
	VZEROUPPER
	RET

DATA dataDivNumberF64<>+0(SB)/8, $0x3ff0000000000000
GLOBL dataDivNumberF64<>(SB), RODATA|NOPTR, $8

// func DivNumber_AVX2_F64(x []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·DivNumber_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVSD a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB14_12
	CMPQ  SI, $0x04
	JAE   LBB14_3
	XORL  AX, AX
	JMP   LBB14_10

LBB14_3:
	MOVQ         SI, AX
	ANDQ         $-4, AX
	VBROADCASTSD X0, Y1
	LEAQ         -4(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x02, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB14_4
	MOVQ         R8, CX
	ANDQ         $-2, CX
	VBROADCASTSD dataDivNumberF64<>+0(SB), Y2
	VDIVPD       Y1, Y2, Y2
	XORL         DX, DX

LBB14_6:
	VMULPD  (DI)(DX*8), Y2, Y3
	VMOVUPD Y3, (DI)(DX*8)
	VMULPD  32(DI)(DX*8), Y2, Y3
	VMOVUPD Y3, 32(DI)(DX*8)
	ADDQ    $0x08, DX
	ADDQ    $-2, CX
	JNE     LBB14_6
	TESTB   $0x01, R8
	JE      LBB14_9

LBB14_8:
	VMOVUPD (DI)(DX*8), Y2
	VDIVPD  Y1, Y2, Y1
	VMOVUPD Y1, (DI)(DX*8)

LBB14_9:
	CMPQ AX, SI
	JE   LBB14_12

LBB14_10:
	VMOVSD dataDivNumberF64<>+0(SB), X1
	VDIVSD X0, X1, X0

LBB14_11:
	VMULSD (DI)(AX*8), X0, X1
	VMOVSD X1, (DI)(AX*8)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB14_11

LBB14_12:
	VZEROUPPER
	RET

LBB14_4:
	XORL  DX, DX
	TESTB $0x01, R8
	JNE   LBB14_8
	JMP   LBB14_9

DATA dataDivNumberF32<>+0(SB)/4, $0x3f800000
GLOBL dataDivNumberF32<>(SB), RODATA|NOPTR, $4

// func DivNumber_AVX2_F32(x []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·DivNumber_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), DI
	MOVSS a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB15_8
	CMPQ  SI, $0x20
	JAE   LBB15_3
	XORL  AX, AX
	JMP   LBB15_6

LBB15_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	VMOVSS       dataDivNumberF32<>+0(SB), X1
	VDIVSS       X0, X1, X1
	VBROADCASTSS X1, Y1
	XORL         CX, CX

LBB15_4:
	VMULPS  (DI)(CX*4), Y1, Y2
	VMULPS  32(DI)(CX*4), Y1, Y3
	VMULPS  64(DI)(CX*4), Y1, Y4
	VMULPS  96(DI)(CX*4), Y1, Y5
	VMOVUPS Y2, (DI)(CX*4)
	VMOVUPS Y3, 32(DI)(CX*4)
	VMOVUPS Y4, 64(DI)(CX*4)
	VMOVUPS Y5, 96(DI)(CX*4)
	ADDQ    $0x20, CX
	CMPQ    AX, CX
	JNE     LBB15_4
	CMPQ    AX, SI
	JE      LBB15_8

LBB15_6:
	VMOVSS dataDivNumberF32<>+0(SB), X1
	VDIVSS X0, X1, X0

LBB15_7:
	VMULSS (DI)(AX*4), X0, X1
	VMOVSS X1, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB15_7

LBB15_8:
	VZEROUPPER
	RET

DATA dataModNumberF64<>+0(SB)/8, $0x3ff0000000000000
DATA dataModNumberF64<>+8(SB)/8, $0x7ff8000020000000
GLOBL dataModNumberF64<>(SB), RODATA|NOPTR, $16

// func ModNumber_4x_AVX2_F64(x []float64, a float64)
// Requires: AVX, AVX2, FMA3, SSE2
TEXT ·ModNumber_4x_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ         x_base+0(FP), DI
	MOVSD        a+24(FP), X0
	MOVQ         x_len+8(FP), SI
	ANDQ         $-4, SI
	JE           LBB22_9
	VXORPD       X1, X1, X1
	VUCOMISD     X1, X0
	JBE          LBB22_4
	VMOVSD       dataModNumberF64<>+0(SB), X1
	VDIVSD       X0, X1, X1
	VBROADCASTSD X1, Y1
	VBROADCASTSD X0, Y0
	XORL         AX, AX
	VXORPD       X2, X2, X2

LBB22_3:
	VMOVUPD      (DI)(AX*8), Y3
	VMULPD       Y1, Y3, Y4
	VROUNDPD     $0x09, Y4, Y4
	VFNMADD213PD Y3, Y0, Y4
	VCMPPD       $0x02, Y4, Y0, Y3
	VCMPPD       $0x01, Y2, Y4, Y5
	VANDPD       Y0, Y3, Y3
	VSUBPD       Y3, Y4, Y3
	VANDPD       Y0, Y5, Y4
	VADDPD       Y4, Y3, Y3
	VMOVUPD      Y3, (DI)(AX*8)
	ADDQ         $0x04, AX
	CMPQ         AX, SI
	JB           LBB22_3
	JMP          LBB22_9

LBB22_4:
	DECQ SI
	MOVQ SI, CX
	SHRQ $0x02, CX
	INCQ CX
	MOVL CX, AX
	ANDL $0x07, AX
	CMPQ SI, $0x1c
	JAE  LBB22_10
	XORL DX, DX
	JMP  LBB22_6

LBB22_10:
	ANDQ         $-8, CX
	XORL         DX, DX
	VBROADCASTSD dataModNumberF64<>+8(SB), Y0

LBB22_11:
	VMOVUPD Y0, (DI)(DX*8)
	VMOVUPD Y0, 32(DI)(DX*8)
	VMOVUPD Y0, 64(DI)(DX*8)
	VMOVUPD Y0, 96(DI)(DX*8)
	VMOVUPD Y0, 128(DI)(DX*8)
	VMOVUPD Y0, 160(DI)(DX*8)
	VMOVUPD Y0, 192(DI)(DX*8)
	VMOVUPD Y0, 224(DI)(DX*8)
	ADDQ    $0x20, DX
	ADDQ    $-8, CX
	JNE     LBB22_11

LBB22_6:
	TESTQ        AX, AX
	JE           LBB22_9
	LEAQ         (DI)(DX*8), CX
	SHLQ         $0x05, AX
	XORL         DX, DX
	VBROADCASTSD dataModNumberF64<>+8(SB), Y0

LBB22_8:
	VMOVUPD Y0, (CX)(DX*1)
	ADDQ    $0x20, DX
	CMPQ    AX, DX
	JNE     LBB22_8

LBB22_9:
	VZEROUPPER
	RET

DATA dataModNumberF32<>+0(SB)/4, $0x3f800000
DATA dataModNumberF32<>+4(SB)/4, $0x7fc00001
GLOBL dataModNumberF32<>(SB), RODATA|NOPTR, $8

// func ModNumber_8x_AVX2_F32(x []float32, a float32)
// Requires: AVX, AVX2, FMA3, SSE
TEXT ·ModNumber_8x_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ         x_base+0(FP), DI
	MOVSS        a+24(FP), X0
	MOVQ         x_len+8(FP), SI
	ANDQ         $-8, SI
	JE           LBB23_9
	VXORPS       X1, X1, X1
	VUCOMISS     X1, X0
	JBE          LBB23_4
	VMOVSS       dataModNumberF32<>+0(SB), X1
	VDIVSS       X0, X1, X1
	VBROADCASTSS X1, Y1
	VBROADCASTSS X0, Y0
	XORL         AX, AX
	VXORPS       X2, X2, X2

LBB23_3:
	VMOVUPS      (DI)(AX*4), Y3
	VMULPS       Y1, Y3, Y4
	VROUNDPS     $0x09, Y4, Y4
	VFNMADD213PS Y3, Y0, Y4
	VCMPPS       $0x02, Y4, Y0, Y3
	VCMPPS       $0x01, Y2, Y4, Y5
	VANDPS       Y0, Y3, Y3
	VSUBPS       Y3, Y4, Y3
	VANDPS       Y0, Y5, Y4
	VADDPS       Y4, Y3, Y3
	VMOVUPS      Y3, (DI)(AX*4)
	ADDQ         $0x08, AX
	CMPQ         AX, SI
	JB           LBB23_3
	JMP          LBB23_9

LBB23_4:
	DECQ SI
	MOVQ SI, CX
	SHRQ $0x03, CX
	INCQ CX
	MOVL CX, AX
	ANDL $0x07, AX
	CMPQ SI, $0x38
	JAE  LBB23_10
	XORL DX, DX
	JMP  LBB23_6

LBB23_10:
	ANDQ         $-8, CX
	XORL         DX, DX
	VBROADCASTSS dataModNumberF32<>+4(SB), Y0

LBB23_11:
	VMOVUPS Y0, (DI)(DX*4)
	VMOVUPS Y0, 32(DI)(DX*4)
	VMOVUPS Y0, 64(DI)(DX*4)
	VMOVUPS Y0, 96(DI)(DX*4)
	VMOVUPS Y0, 128(DI)(DX*4)
	VMOVUPS Y0, 160(DI)(DX*4)
	VMOVUPS Y0, 192(DI)(DX*4)
	VMOVUPS Y0, 224(DI)(DX*4)
	ADDQ    $0x40, DX
	ADDQ    $-8, CX
	JNE     LBB23_11

LBB23_6:
	TESTQ        AX, AX
	JE           LBB23_9
	LEAQ         (DI)(DX*4), CX
	SHLQ         $0x05, AX
	XORL         DX, DX
	VBROADCASTSS dataModNumberF32<>+4(SB), Y0

LBB23_8:
	VMOVUPS Y0, (CX)(DX*1)
	ADDQ    $0x20, DX
	CMPQ    AX, DX
	JNE     LBB23_8

LBB23_9:
	VZEROUPPER
	RET

DATA dataAbsF64<>+0(SB)/8, $0x7fffffffffffffff
DATA dataAbsF64<>+8(SB)/8, $0x7fffffffffffffff
DATA dataAbsF64<>+16(SB)/8, $0x7fffffffffffffff
GLOBL dataAbsF64<>(SB), RODATA|NOPTR, $24

// func Abs_AVX2_F64(x []float64)
// Requires: AVX
TEXT ·Abs_AVX2_F64(SB), NOSPLIT, $0-24
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB16_8
	CMPQ  SI, $0x10
	JAE   LBB16_3
	XORL  AX, AX
	JMP   LBB16_6

LBB16_3:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	XORL         CX, CX
	VBROADCASTSD dataAbsF64<>+0(SB), Y0

LBB16_4:
	VANDPS  (DI)(CX*8), Y0, Y1
	VANDPS  32(DI)(CX*8), Y0, Y2
	VANDPS  64(DI)(CX*8), Y0, Y3
	VANDPS  96(DI)(CX*8), Y0, Y4
	VMOVUPS Y1, (DI)(CX*8)
	VMOVUPS Y2, 32(DI)(CX*8)
	VMOVUPS Y3, 64(DI)(CX*8)
	VMOVUPS Y4, 96(DI)(CX*8)
	ADDQ    $0x10, CX
	CMPQ    AX, CX
	JNE     LBB16_4
	CMPQ    AX, SI
	JE      LBB16_8

LBB16_6:
	VMOVUPS dataAbsF64<>+8(SB), X0

LBB16_7:
	VMOVSD  (DI)(AX*8), X1
	VANDPS  X0, X1, X1
	VMOVLPS X1, (DI)(AX*8)
	ADDQ    $0x01, AX
	CMPQ    SI, AX
	JNE     LBB16_7

LBB16_8:
	VZEROUPPER
	RET

DATA dataAbsF32<>+0(SB)/4, $0x7fffffff
GLOBL dataAbsF32<>(SB), RODATA|NOPTR, $4

// func Abs_AVX2_F32(x []float32)
// Requires: AVX
TEXT ·Abs_AVX2_F32(SB), NOSPLIT, $0-24
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB17_8
	CMPQ  SI, $0x20
	JAE   LBB17_3
	XORL  AX, AX
	JMP   LBB17_6

LBB17_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	XORL         CX, CX
	VBROADCASTSS dataAbsF32<>+0(SB), Y0

LBB17_4:
	VANDPS  (DI)(CX*4), Y0, Y1
	VANDPS  32(DI)(CX*4), Y0, Y2
	VANDPS  64(DI)(CX*4), Y0, Y3
	VANDPS  96(DI)(CX*4), Y0, Y4
	VMOVUPS Y1, (DI)(CX*4)
	VMOVUPS Y2, 32(DI)(CX*4)
	VMOVUPS Y3, 64(DI)(CX*4)
	VMOVUPS Y4, 96(DI)(CX*4)
	ADDQ    $0x20, CX
	CMPQ    AX, CX
	JNE     LBB17_4
	CMPQ    AX, SI
	JE      LBB17_8

LBB17_6:
	VBROADCASTSS dataAbsF32<>+0(SB), X0

LBB17_7:
	VMOVSS (DI)(AX*4), X1
	VANDPS X0, X1, X1
	VMOVSS X1, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB17_7

LBB17_8:
	VZEROUPPER
	RET

DATA dataNegF64<>+0(SB)/8, $0x8000000000000000
DATA dataNegF64<>+8(SB)/8, $0x8000000000000000
DATA dataNegF64<>+16(SB)/8, $0x8000000000000000
GLOBL dataNegF64<>(SB), RODATA|NOPTR, $24

// func Neg_AVX2_F64(x []float64)
// Requires: AVX
TEXT ·Neg_AVX2_F64(SB), NOSPLIT, $0-24
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB18_12
	CMPQ  SI, $0x10
	JAE   LBB18_3
	XORL  AX, AX
	JMP   LBB18_10

LBB18_3:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	LEAQ         -16(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x04, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB18_4
	MOVQ         R8, DX
	ANDQ         $-2, DX
	XORL         CX, CX
	VBROADCASTSD dataNegF64<>+0(SB), Y0

LBB18_6:
	VXORPS  (DI)(CX*8), Y0, Y1
	VXORPS  32(DI)(CX*8), Y0, Y2
	VXORPS  64(DI)(CX*8), Y0, Y3
	VXORPS  96(DI)(CX*8), Y0, Y4
	VMOVUPS Y1, (DI)(CX*8)
	VMOVUPS Y2, 32(DI)(CX*8)
	VMOVUPS Y3, 64(DI)(CX*8)
	VMOVUPS Y4, 96(DI)(CX*8)
	VXORPS  128(DI)(CX*8), Y0, Y1
	VXORPS  160(DI)(CX*8), Y0, Y2
	VXORPS  192(DI)(CX*8), Y0, Y3
	VXORPS  224(DI)(CX*8), Y0, Y4
	VMOVUPS Y1, 128(DI)(CX*8)
	VMOVUPS Y2, 160(DI)(CX*8)
	VMOVUPS Y3, 192(DI)(CX*8)
	VMOVUPS Y4, 224(DI)(CX*8)
	ADDQ    $0x20, CX
	ADDQ    $-2, DX
	JNE     LBB18_6
	TESTB   $0x01, R8
	JE      LBB18_9

LBB18_8:
	VBROADCASTSD dataNegF64<>+0(SB), Y0
	VXORPS       (DI)(CX*8), Y0, Y1
	VXORPS       32(DI)(CX*8), Y0, Y2
	VXORPS       64(DI)(CX*8), Y0, Y3
	VXORPS       96(DI)(CX*8), Y0, Y0
	VMOVUPS      Y1, (DI)(CX*8)
	VMOVUPS      Y2, 32(DI)(CX*8)
	VMOVUPS      Y3, 64(DI)(CX*8)
	VMOVUPS      Y0, 96(DI)(CX*8)

LBB18_9:
	CMPQ AX, SI
	JE   LBB18_12

LBB18_10:
	VMOVUPS dataNegF64<>+8(SB), X0

LBB18_11:
	VMOVSD  (DI)(AX*8), X1
	VXORPS  X0, X1, X1
	VMOVLPS X1, (DI)(AX*8)
	ADDQ    $0x01, AX
	CMPQ    SI, AX
	JNE     LBB18_11

LBB18_12:
	VZEROUPPER
	RET

LBB18_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB18_8
	JMP   LBB18_9

DATA dataNegF32<>+0(SB)/4, $0x80000000
GLOBL dataNegF32<>(SB), RODATA|NOPTR, $4

// func Neg_AVX2_F32(x []float32)
// Requires: AVX
TEXT ·Neg_AVX2_F32(SB), NOSPLIT, $0-24
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB19_12
	CMPQ  SI, $0x20
	JAE   LBB19_3
	XORL  AX, AX
	JMP   LBB19_10

LBB19_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	LEAQ         -32(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x05, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB19_4
	MOVQ         R8, DX
	ANDQ         $-2, DX
	XORL         CX, CX
	VBROADCASTSS dataNegF32<>+0(SB), Y0

LBB19_6:
	VXORPS  (DI)(CX*4), Y0, Y1
	VXORPS  32(DI)(CX*4), Y0, Y2
	VXORPS  64(DI)(CX*4), Y0, Y3
	VXORPS  96(DI)(CX*4), Y0, Y4
	VMOVUPS Y1, (DI)(CX*4)
	VMOVUPS Y2, 32(DI)(CX*4)
	VMOVUPS Y3, 64(DI)(CX*4)
	VMOVUPS Y4, 96(DI)(CX*4)
	VXORPS  128(DI)(CX*4), Y0, Y1
	VXORPS  160(DI)(CX*4), Y0, Y2
	VXORPS  192(DI)(CX*4), Y0, Y3
	VXORPS  224(DI)(CX*4), Y0, Y4
	VMOVUPS Y1, 128(DI)(CX*4)
	VMOVUPS Y2, 160(DI)(CX*4)
	VMOVUPS Y3, 192(DI)(CX*4)
	VMOVUPS Y4, 224(DI)(CX*4)
	ADDQ    $0x40, CX
	ADDQ    $-2, DX
	JNE     LBB19_6
	TESTB   $0x01, R8
	JE      LBB19_9

LBB19_8:
	VBROADCASTSS dataNegF32<>+0(SB), Y0
	VXORPS       (DI)(CX*4), Y0, Y1
	VXORPS       32(DI)(CX*4), Y0, Y2
	VXORPS       64(DI)(CX*4), Y0, Y3
	VXORPS       96(DI)(CX*4), Y0, Y0
	VMOVUPS      Y1, (DI)(CX*4)
	VMOVUPS      Y2, 32(DI)(CX*4)
	VMOVUPS      Y3, 64(DI)(CX*4)
	VMOVUPS      Y0, 96(DI)(CX*4)

LBB19_9:
	CMPQ AX, SI
	JE   LBB19_12

LBB19_10:
	VBROADCASTSS dataNegF32<>+0(SB), X0

LBB19_11:
	VMOVSS (DI)(AX*4), X1
	VXORPS X0, X1, X1
	VMOVSS X1, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB19_11

LBB19_12:
	VZEROUPPER
	RET

LBB19_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB19_8
	JMP   LBB19_9

DATA dataInvF64<>+0(SB)/8, $0x3ff0000000000000
GLOBL dataInvF64<>(SB), RODATA|NOPTR, $8

// func Inv_AVX2_F64(x []float64)
// Requires: AVX
TEXT ·Inv_AVX2_F64(SB), NOSPLIT, $0-24
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB20_12
	CMPQ  SI, $0x04
	JAE   LBB20_3
	XORL  AX, AX
	JMP   LBB20_10

LBB20_3:
	MOVQ         SI, AX
	ANDQ         $-4, AX
	LEAQ         -4(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x02, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB20_4
	MOVQ         R8, CX
	ANDQ         $-2, CX
	XORL         DX, DX
	VBROADCASTSD dataInvF64<>+0(SB), Y0

LBB20_6:
	VDIVPD  (DI)(DX*8), Y0, Y1
	VMOVUPD Y1, (DI)(DX*8)
	VDIVPD  32(DI)(DX*8), Y0, Y1
	VMOVUPD Y1, 32(DI)(DX*8)
	ADDQ    $0x08, DX
	ADDQ    $-2, CX
	JNE     LBB20_6
	TESTB   $0x01, R8
	JE      LBB20_9

LBB20_8:
	VBROADCASTSD dataInvF64<>+0(SB), Y0
	VDIVPD       (DI)(DX*8), Y0, Y0
	VMOVUPD      Y0, (DI)(DX*8)

LBB20_9:
	CMPQ AX, SI
	JE   LBB20_12

LBB20_10:
	VMOVSD dataInvF64<>+0(SB), X0

LBB20_11:
	VDIVSD (DI)(AX*8), X0, X1
	VMOVSD X1, (DI)(AX*8)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB20_11

LBB20_12:
	VZEROUPPER
	RET

LBB20_4:
	XORL  DX, DX
	TESTB $0x01, R8
	JNE   LBB20_8
	JMP   LBB20_9

DATA dataInvF32<>+0(SB)/4, $0x3f800000
GLOBL dataInvF32<>(SB), RODATA|NOPTR, $4

// func Inv_AVX2_F32(x []float32)
// Requires: AVX, FMA3
TEXT ·Inv_AVX2_F32(SB), NOSPLIT, $0-24
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB21_8
	CMPQ  SI, $0x20
	JAE   LBB21_3
	XORL  AX, AX
	JMP   LBB21_6

LBB21_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	XORL         CX, CX
	VBROADCASTSS dataInvF32<>+0(SB), Y0

LBB21_4:
	VMOVUPS      (DI)(CX*4), Y1
	VMOVUPS      32(DI)(CX*4), Y2
	VMOVUPS      64(DI)(CX*4), Y3
	VRCPPS       Y1, Y4
	VFMSUB213PS  Y0, Y4, Y1
	VRCPPS       Y2, Y5
	VFNMADD132PS Y4, Y4, Y1
	VMOVUPS      96(DI)(CX*4), Y4
	VFMSUB213PS  Y0, Y5, Y2
	VFNMADD132PS Y5, Y5, Y2
	VRCPPS       Y3, Y5
	VFMSUB213PS  Y0, Y5, Y3
	VFNMADD132PS Y5, Y5, Y3
	VRCPPS       Y4, Y5
	VFMSUB213PS  Y0, Y5, Y4
	VFNMADD132PS Y5, Y5, Y4
	VMOVUPS      Y1, (DI)(CX*4)
	VMOVUPS      Y2, 32(DI)(CX*4)
	VMOVUPS      Y3, 64(DI)(CX*4)
	VMOVUPS      Y4, 96(DI)(CX*4)
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB21_4
	CMPQ         AX, SI
	JE           LBB21_8

LBB21_6:
	VMOVSS dataInvF32<>+0(SB), X0

LBB21_7:
	VDIVSS (DI)(AX*4), X0, X1
	VMOVSS X1, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB21_7

LBB21_8:
	VZEROUPPER
	RET

// func Sum_AVX2_F64(x []float64) float64
// Requires: AVX, SSE2
TEXT ·Sum_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB0_1
	CMPQ   SI, $0x10
	JAE    LBB0_4
	VXORPD X0, X0, X0
	XORL   AX, AX
	JMP    LBB0_11

LBB0_1:
	VXORPS X0, X0, X0
	MOVSD  X0, ret+24(FP)
	RET

LBB0_4:
	MOVQ   SI, AX
	ANDQ   $-16, AX
	LEAQ   -16(AX), CX
	MOVQ   CX, R8
	SHRQ   $0x04, R8
	ADDQ   $0x01, R8
	TESTQ  CX, CX
	JE     LBB0_5
	MOVQ   R8, CX
	ANDQ   $-2, CX
	VXORPD X0, X0, X0
	XORL   DX, DX
	VXORPD X1, X1, X1
	VXORPD X2, X2, X2
	VXORPD X3, X3, X3

LBB0_7:
	VADDPD (DI)(DX*8), Y0, Y0
	VADDPD 32(DI)(DX*8), Y1, Y1
	VADDPD 64(DI)(DX*8), Y2, Y2
	VADDPD 96(DI)(DX*8), Y3, Y3
	VADDPD 128(DI)(DX*8), Y0, Y0
	VADDPD 160(DI)(DX*8), Y1, Y1
	VADDPD 192(DI)(DX*8), Y2, Y2
	VADDPD 224(DI)(DX*8), Y3, Y3
	ADDQ   $0x20, DX
	ADDQ   $-2, CX
	JNE    LBB0_7
	TESTB  $0x01, R8
	JE     LBB0_10

LBB0_9:
	VADDPD (DI)(DX*8), Y0, Y0
	VADDPD 32(DI)(DX*8), Y1, Y1
	VADDPD 64(DI)(DX*8), Y2, Y2
	VADDPD 96(DI)(DX*8), Y3, Y3

LBB0_10:
	VADDPD       Y3, Y1, Y1
	VADDPD       Y2, Y0, Y0
	VADDPD       Y1, Y0, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPD       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDSD       X1, X0, X0
	CMPQ         AX, SI
	JE           LBB0_12

LBB0_11:
	VADDSD (DI)(AX*8), X0, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB0_11

LBB0_12:
	VZEROUPPER
	MOVSD X0, ret+24(FP)
	RET

LBB0_5:
	VXORPD X0, X0, X0
	XORL   DX, DX
	VXORPD X1, X1, X1
	VXORPD X2, X2, X2
	VXORPD X3, X3, X3
	TESTB  $0x01, R8
	JNE    LBB0_9
	JMP    LBB0_10

// func Sum_AVX2_F32(x []float32) float32
// Requires: AVX, SSE
TEXT ·Sum_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB1_1
	CMPQ   SI, $0x20
	JAE    LBB1_4
	VXORPS X0, X0, X0
	XORL   AX, AX
	JMP    LBB1_11

LBB1_1:
	VXORPS X0, X0, X0
	MOVSS  X0, ret+24(FP)
	RET

LBB1_4:
	MOVQ   SI, AX
	ANDQ   $-32, AX
	LEAQ   -32(AX), CX
	MOVQ   CX, R8
	SHRQ   $0x05, R8
	ADDQ   $0x01, R8
	TESTQ  CX, CX
	JE     LBB1_5
	MOVQ   R8, CX
	ANDQ   $-2, CX
	VXORPS X0, X0, X0
	XORL   DX, DX
	VXORPS X1, X1, X1
	VXORPS X2, X2, X2
	VXORPS X3, X3, X3

LBB1_7:
	VADDPS (DI)(DX*4), Y0, Y0
	VADDPS 32(DI)(DX*4), Y1, Y1
	VADDPS 64(DI)(DX*4), Y2, Y2
	VADDPS 96(DI)(DX*4), Y3, Y3
	VADDPS 128(DI)(DX*4), Y0, Y0
	VADDPS 160(DI)(DX*4), Y1, Y1
	VADDPS 192(DI)(DX*4), Y2, Y2
	VADDPS 224(DI)(DX*4), Y3, Y3
	ADDQ   $0x40, DX
	ADDQ   $-2, CX
	JNE    LBB1_7
	TESTB  $0x01, R8
	JE     LBB1_10

LBB1_9:
	VADDPS (DI)(DX*4), Y0, Y0
	VADDPS 32(DI)(DX*4), Y1, Y1
	VADDPS 64(DI)(DX*4), Y2, Y2
	VADDPS 96(DI)(DX*4), Y3, Y3

LBB1_10:
	VADDPS       Y3, Y1, Y1
	VADDPS       Y2, Y0, Y0
	VADDPS       Y1, Y0, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPS       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDPS       X1, X0, X0
	VMOVSHDUP    X0, X1
	VADDSS       X1, X0, X0
	CMPQ         AX, SI
	JE           LBB1_12

LBB1_11:
	VADDSS (DI)(AX*4), X0, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB1_11

LBB1_12:
	VZEROUPPER
	MOVSS X0, ret+24(FP)
	RET

LBB1_5:
	VXORPS X0, X0, X0
	XORL   DX, DX
	VXORPS X1, X1, X1
	VXORPS X2, X2, X2
	VXORPS X3, X3, X3
	TESTB  $0x01, R8
	JNE    LBB1_9
	JMP    LBB1_10

// func CumSum_AVX2_F64(x []float64)
// Requires: AVX
TEXT ·CumSum_AVX2_F64(SB), NOSPLIT, $0-24
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB2_8
	LEAQ   -1(SI), CX
	MOVL   SI, AX
	ANDL   $0x03, AX
	CMPQ   CX, $0x03
	JAE    LBB2_3
	VXORPD X0, X0, X0
	XORL   CX, CX
	JMP    LBB2_5

LBB2_3:
	ANDQ   $-4, SI
	VXORPD X0, X0, X0
	XORL   CX, CX

LBB2_4:
	VADDSD (DI)(CX*8), X0, X0
	VMOVSD X0, (DI)(CX*8)
	VADDSD 8(DI)(CX*8), X0, X0
	VMOVSD X0, 8(DI)(CX*8)
	VADDSD 16(DI)(CX*8), X0, X0
	VMOVSD X0, 16(DI)(CX*8)
	VADDSD 24(DI)(CX*8), X0, X0
	VMOVSD X0, 24(DI)(CX*8)
	ADDQ   $0x04, CX
	CMPQ   SI, CX
	JNE    LBB2_4

LBB2_5:
	TESTQ AX, AX
	JE    LBB2_8
	LEAQ  (DI)(CX*8), CX
	XORL  DX, DX

LBB2_7:
	VADDSD (CX)(DX*8), X0, X0
	VMOVSD X0, (CX)(DX*8)
	ADDQ   $0x01, DX
	CMPQ   AX, DX
	JNE    LBB2_7

LBB2_8:
	RET

// func CumSum_AVX2_F32(x []float32)
// Requires: AVX
TEXT ·CumSum_AVX2_F32(SB), NOSPLIT, $0-24
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB3_8
	LEAQ   -1(SI), CX
	MOVL   SI, AX
	ANDL   $0x03, AX
	CMPQ   CX, $0x03
	JAE    LBB3_3
	VXORPS X0, X0, X0
	XORL   CX, CX
	JMP    LBB3_5

LBB3_3:
	ANDQ   $-4, SI
	VXORPS X0, X0, X0
	XORL   CX, CX

LBB3_4:
	VADDSS (DI)(CX*4), X0, X0
	VMOVSS X0, (DI)(CX*4)
	VADDSS 4(DI)(CX*4), X0, X0
	VMOVSS X0, 4(DI)(CX*4)
	VADDSS 8(DI)(CX*4), X0, X0
	VMOVSS X0, 8(DI)(CX*4)
	VADDSS 12(DI)(CX*4), X0, X0
	VMOVSS X0, 12(DI)(CX*4)
	ADDQ   $0x04, CX
	CMPQ   SI, CX
	JNE    LBB3_4

LBB3_5:
	TESTQ AX, AX
	JE    LBB3_8
	LEAQ  (DI)(CX*4), CX
	XORL  DX, DX

LBB3_7:
	VADDSS (CX)(DX*4), X0, X0
	VMOVSS X0, (CX)(DX*4)
	ADDQ   $0x01, DX
	CMPQ   AX, DX
	JNE    LBB3_7

LBB3_8:
	RET

DATA dataProdF64<>+0(SB)/8, $0x3ff0000000000000
GLOBL dataProdF64<>(SB), RODATA|NOPTR, $8

// func Prod_AVX2_F64(x []float64) float64
// Requires: AVX, SSE2
TEXT ·Prod_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB4_1
	CMPQ   SI, $0x10
	JAE    LBB4_4
	VMOVSD dataProdF64<>+0(SB), X0
	XORL   AX, AX
	JMP    LBB4_11

LBB4_1:
	VMOVSD dataProdF64<>+0(SB), X0
	MOVSD  X0, ret+24(FP)
	RET

LBB4_4:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	LEAQ         -16(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x04, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB4_5
	MOVQ         R8, CX
	ANDQ         $-2, CX
	VBROADCASTSD dataProdF64<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPD      Y0, Y1
	VMOVAPD      Y0, Y2
	VMOVAPD      Y0, Y3

LBB4_7:
	VMULPD (DI)(DX*8), Y0, Y0
	VMULPD 32(DI)(DX*8), Y1, Y1
	VMULPD 64(DI)(DX*8), Y2, Y2
	VMULPD 96(DI)(DX*8), Y3, Y3
	VMULPD 128(DI)(DX*8), Y0, Y0
	VMULPD 160(DI)(DX*8), Y1, Y1
	VMULPD 192(DI)(DX*8), Y2, Y2
	VMULPD 224(DI)(DX*8), Y3, Y3
	ADDQ   $0x20, DX
	ADDQ   $-2, CX
	JNE    LBB4_7
	TESTB  $0x01, R8
	JE     LBB4_10

LBB4_9:
	VMULPD (DI)(DX*8), Y0, Y0
	VMULPD 32(DI)(DX*8), Y1, Y1
	VMULPD 64(DI)(DX*8), Y2, Y2
	VMULPD 96(DI)(DX*8), Y3, Y3

LBB4_10:
	VMULPD       Y3, Y1, Y1
	VMULPD       Y2, Y0, Y0
	VMULPD       Y1, Y0, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VMULPD       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VMULSD       X1, X0, X0
	CMPQ         AX, SI
	JE           LBB4_12

LBB4_11:
	VMULSD (DI)(AX*8), X0, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB4_11

LBB4_12:
	VZEROUPPER
	MOVSD X0, ret+24(FP)
	RET

LBB4_5:
	VBROADCASTSD dataProdF64<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPD      Y0, Y1
	VMOVAPD      Y0, Y2
	VMOVAPD      Y0, Y3
	TESTB        $0x01, R8
	JNE          LBB4_9
	JMP          LBB4_10

DATA dataProdF32<>+0(SB)/4, $0x3f800000
GLOBL dataProdF32<>(SB), RODATA|NOPTR, $4

// func Prod_AVX2_F32(x []float32) float32
// Requires: AVX, SSE
TEXT ·Prod_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB5_1
	CMPQ   SI, $0x20
	JAE    LBB5_4
	VMOVSS dataProdF32<>+0(SB), X0
	XORL   AX, AX
	JMP    LBB5_11

LBB5_1:
	VMOVSS dataProdF32<>+0(SB), X0
	MOVSS  X0, ret+24(FP)
	RET

LBB5_4:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	LEAQ         -32(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x05, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB5_5
	MOVQ         R8, CX
	ANDQ         $-2, CX
	VBROADCASTSS dataProdF32<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPS      Y0, Y1
	VMOVAPS      Y0, Y2
	VMOVAPS      Y0, Y3

LBB5_7:
	VMULPS (DI)(DX*4), Y0, Y0
	VMULPS 32(DI)(DX*4), Y1, Y1
	VMULPS 64(DI)(DX*4), Y2, Y2
	VMULPS 96(DI)(DX*4), Y3, Y3
	VMULPS 128(DI)(DX*4), Y0, Y0
	VMULPS 160(DI)(DX*4), Y1, Y1
	VMULPS 192(DI)(DX*4), Y2, Y2
	VMULPS 224(DI)(DX*4), Y3, Y3
	ADDQ   $0x40, DX
	ADDQ   $-2, CX
	JNE    LBB5_7
	TESTB  $0x01, R8
	JE     LBB5_10

LBB5_9:
	VMULPS (DI)(DX*4), Y0, Y0
	VMULPS 32(DI)(DX*4), Y1, Y1
	VMULPS 64(DI)(DX*4), Y2, Y2
	VMULPS 96(DI)(DX*4), Y3, Y3

LBB5_10:
	VMULPS       Y3, Y1, Y1
	VMULPS       Y2, Y0, Y0
	VMULPS       Y1, Y0, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VMULPS       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VMULPS       X1, X0, X0
	VMOVSHDUP    X0, X1
	VMULSS       X1, X0, X0
	CMPQ         AX, SI
	JE           LBB5_12

LBB5_11:
	VMULSS (DI)(AX*4), X0, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB5_11

LBB5_12:
	VZEROUPPER
	MOVSS X0, ret+24(FP)
	RET

LBB5_5:
	VBROADCASTSS dataProdF32<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPS      Y0, Y1
	VMOVAPS      Y0, Y2
	VMOVAPS      Y0, Y3
	TESTB        $0x01, R8
	JNE          LBB5_9
	JMP          LBB5_10

DATA dataCumProdF64<>+0(SB)/8, $0x3ff0000000000000
GLOBL dataCumProdF64<>(SB), RODATA|NOPTR, $8

// func CumProd_AVX2_F64(x []float64)
// Requires: AVX
TEXT ·CumProd_AVX2_F64(SB), NOSPLIT, $0-24
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB6_8
	LEAQ   -1(SI), CX
	MOVL   SI, AX
	ANDL   $0x03, AX
	CMPQ   CX, $0x03
	JAE    LBB6_3
	VMOVSD dataCumProdF64<>+0(SB), X0
	XORL   CX, CX
	JMP    LBB6_5

LBB6_3:
	ANDQ   $-4, SI
	VMOVSD dataCumProdF64<>+0(SB), X0
	XORL   CX, CX

LBB6_4:
	VMULSD (DI)(CX*8), X0, X0
	VMOVSD X0, (DI)(CX*8)
	VMULSD 8(DI)(CX*8), X0, X0
	VMOVSD X0, 8(DI)(CX*8)
	VMULSD 16(DI)(CX*8), X0, X0
	VMOVSD X0, 16(DI)(CX*8)
	VMULSD 24(DI)(CX*8), X0, X0
	VMOVSD X0, 24(DI)(CX*8)
	ADDQ   $0x04, CX
	CMPQ   SI, CX
	JNE    LBB6_4

LBB6_5:
	TESTQ AX, AX
	JE    LBB6_8
	LEAQ  (DI)(CX*8), CX
	XORL  DX, DX

LBB6_7:
	VMULSD (CX)(DX*8), X0, X0
	VMOVSD X0, (CX)(DX*8)
	ADDQ   $0x01, DX
	CMPQ   AX, DX
	JNE    LBB6_7

LBB6_8:
	RET

DATA dataCumProdF32<>+0(SB)/4, $0x3f800000
GLOBL dataCumProdF32<>(SB), RODATA|NOPTR, $4

// func CumProd_AVX2_F32(x []float32)
// Requires: AVX
TEXT ·CumProd_AVX2_F32(SB), NOSPLIT, $0-24
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB7_8
	LEAQ   -1(SI), CX
	MOVL   SI, AX
	ANDL   $0x03, AX
	CMPQ   CX, $0x03
	JAE    LBB7_3
	VMOVSS dataCumProdF32<>+0(SB), X0
	XORL   CX, CX
	JMP    LBB7_5

LBB7_3:
	ANDQ   $-4, SI
	VMOVSS dataCumProdF32<>+0(SB), X0
	XORL   CX, CX

LBB7_4:
	VMULSS (DI)(CX*4), X0, X0
	VMOVSS X0, (DI)(CX*4)
	VMULSS 4(DI)(CX*4), X0, X0
	VMOVSS X0, 4(DI)(CX*4)
	VMULSS 8(DI)(CX*4), X0, X0
	VMOVSS X0, 8(DI)(CX*4)
	VMULSS 12(DI)(CX*4), X0, X0
	VMOVSS X0, 12(DI)(CX*4)
	ADDQ   $0x04, CX
	CMPQ   SI, CX
	JNE    LBB7_4

LBB7_5:
	TESTQ AX, AX
	JE    LBB7_8
	LEAQ  (DI)(CX*4), CX
	XORL  DX, DX

LBB7_7:
	VMULSS (CX)(DX*4), X0, X0
	VMOVSS X0, (CX)(DX*4)
	ADDQ   $0x01, DX
	CMPQ   AX, DX
	JNE    LBB7_7

LBB7_8:
	RET

// func Dot_AVX2_F64(x []float64, y []float64) float64
// Requires: AVX, FMA3, SSE2
TEXT ·Dot_AVX2_F64(SB), NOSPLIT, $0-56
	MOVQ   x_base+0(FP), DI
	MOVQ   y_base+24(FP), SI
	MOVQ   x_len+8(FP), DX
	TESTQ  DX, DX
	JE     LBB0_1
	CMPQ   DX, $0x10
	JAE    LBB0_4
	VXORPD X0, X0, X0
	XORL   AX, AX
	JMP    LBB0_7

LBB0_1:
	VXORPS X0, X0, X0
	MOVSD  X0, ret+48(FP)
	RET

LBB0_4:
	MOVQ   DX, AX
	ANDQ   $-16, AX
	VXORPD X0, X0, X0
	XORL   CX, CX
	VXORPD X1, X1, X1
	VXORPD X2, X2, X2
	VXORPD X3, X3, X3

LBB0_5:
	VMOVUPD      (SI)(CX*8), Y4
	VMOVUPD      32(SI)(CX*8), Y5
	VMOVUPD      64(SI)(CX*8), Y6
	VMOVUPD      96(SI)(CX*8), Y7
	VFMADD231PD  (DI)(CX*8), Y4, Y0
	VFMADD231PD  32(DI)(CX*8), Y5, Y1
	VFMADD231PD  64(DI)(CX*8), Y6, Y2
	VFMADD231PD  96(DI)(CX*8), Y7, Y3
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB0_5
	VADDPD       Y0, Y1, Y0
	VADDPD       Y0, Y2, Y0
	VADDPD       Y0, Y3, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPD       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDSD       X1, X0, X0
	CMPQ         AX, DX
	JE           LBB0_8

LBB0_7:
	VMOVSD      (SI)(AX*8), X1
	VFMADD231SD (DI)(AX*8), X1, X0
	ADDQ        $0x01, AX
	CMPQ        DX, AX
	JNE         LBB0_7

LBB0_8:
	VZEROUPPER
	MOVSD X0, ret+48(FP)
	RET

// func Dot_AVX2_F32(x []float32, y []float32) float32
// Requires: AVX, FMA3, SSE
TEXT ·Dot_AVX2_F32(SB), NOSPLIT, $0-52
	MOVQ   x_base+0(FP), DI
	MOVQ   y_base+24(FP), SI
	MOVQ   x_len+8(FP), DX
	TESTQ  DX, DX
	JE     LBB1_1
	CMPQ   DX, $0x20
	JAE    LBB1_4
	VXORPS X0, X0, X0
	XORL   AX, AX
	JMP    LBB1_7

LBB1_1:
	VXORPS X0, X0, X0
	MOVSS  X0, ret+48(FP)
	RET

LBB1_4:
	MOVQ   DX, AX
	ANDQ   $-32, AX
	VXORPS X0, X0, X0
	XORL   CX, CX
	VXORPS X1, X1, X1
	VXORPS X2, X2, X2
	VXORPS X3, X3, X3

LBB1_5:
	VMOVUPS      (SI)(CX*4), Y4
	VMOVUPS      32(SI)(CX*4), Y5
	VMOVUPS      64(SI)(CX*4), Y6
	VMOVUPS      96(SI)(CX*4), Y7
	VFMADD231PS  (DI)(CX*4), Y4, Y0
	VFMADD231PS  32(DI)(CX*4), Y5, Y1
	VFMADD231PS  64(DI)(CX*4), Y6, Y2
	VFMADD231PS  96(DI)(CX*4), Y7, Y3
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB1_5
	VADDPS       Y0, Y1, Y0
	VADDPS       Y0, Y2, Y0
	VADDPS       Y0, Y3, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPS       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDPS       X1, X0, X0
	VMOVSHDUP    X0, X1
	VADDSS       X1, X0, X0
	CMPQ         AX, DX
	JE           LBB1_8

LBB1_7:
	VMOVSS      (SI)(AX*4), X1
	VFMADD231SS (DI)(AX*4), X1, X0
	ADDQ        $0x01, AX
	CMPQ        DX, AX
	JNE         LBB1_7

LBB1_8:
	VZEROUPPER
	MOVSS X0, ret+48(FP)
	RET

// func Norm_AVX2_F64(x []float64) float64
// Requires: AVX, FMA3, SSE2
TEXT ·Norm_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB2_1
	CMPQ   SI, $0x10
	JAE    LBB2_4
	VXORPD X0, X0, X0
	XORL   AX, AX
	JMP    LBB2_11

LBB2_1:
	VXORPD  X0, X0, X0
	VSQRTSD X0, X0, X0
	MOVSD   X0, ret+24(FP)
	RET

LBB2_4:
	MOVQ   SI, AX
	ANDQ   $-16, AX
	LEAQ   -16(AX), CX
	MOVQ   CX, R8
	SHRQ   $0x04, R8
	ADDQ   $0x01, R8
	TESTQ  CX, CX
	JE     LBB2_5
	MOVQ   R8, CX
	ANDQ   $-2, CX
	VXORPD X0, X0, X0
	XORL   DX, DX
	VXORPD X1, X1, X1
	VXORPD X2, X2, X2
	VXORPD X3, X3, X3

LBB2_7:
	VMOVUPD     (DI)(DX*8), Y4
	VMOVUPD     32(DI)(DX*8), Y5
	VMOVUPD     64(DI)(DX*8), Y6
	VMOVUPD     96(DI)(DX*8), Y7
	VFMADD213PD Y0, Y4, Y4
	VFMADD213PD Y1, Y5, Y5
	VFMADD213PD Y2, Y6, Y6
	VFMADD213PD Y3, Y7, Y7
	VMOVUPD     128(DI)(DX*8), Y0
	VMOVUPD     160(DI)(DX*8), Y1
	VMOVUPD     192(DI)(DX*8), Y2
	VMOVUPD     224(DI)(DX*8), Y3
	VFMADD213PD Y4, Y0, Y0
	VFMADD213PD Y5, Y1, Y1
	VFMADD213PD Y6, Y2, Y2
	VFMADD213PD Y7, Y3, Y3
	ADDQ        $0x20, DX
	ADDQ        $-2, CX
	JNE         LBB2_7
	TESTB       $0x01, R8
	JE          LBB2_10

LBB2_9:
	VMOVUPD     (DI)(DX*8), Y4
	VMOVUPD     32(DI)(DX*8), Y5
	VMOVUPD     64(DI)(DX*8), Y6
	VMOVUPD     96(DI)(DX*8), Y7
	VFMADD231PD Y4, Y4, Y0
	VFMADD231PD Y5, Y5, Y1
	VFMADD231PD Y6, Y6, Y2
	VFMADD231PD Y7, Y7, Y3

LBB2_10:
	VADDPD       Y3, Y1, Y1
	VADDPD       Y2, Y0, Y0
	VADDPD       Y1, Y0, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPD       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDSD       X1, X0, X0
	CMPQ         AX, SI
	JE           LBB2_12

LBB2_11:
	VMOVSD      (DI)(AX*8), X1
	VFMADD231SD X1, X1, X0
	ADDQ        $0x01, AX
	CMPQ        SI, AX
	JNE         LBB2_11

LBB2_12:
	VSQRTSD X0, X0, X0
	VZEROUPPER
	MOVSD   X0, ret+24(FP)
	RET

LBB2_5:
	VXORPD X0, X0, X0
	XORL   DX, DX
	VXORPD X1, X1, X1
	VXORPD X2, X2, X2
	VXORPD X3, X3, X3
	TESTB  $0x01, R8
	JNE    LBB2_9
	JMP    LBB2_10

DATA dataNormF32<>+0(SB)/4, $0xc0400000
DATA dataNormF32<>+4(SB)/4, $0xbf000000
DATA dataNormF32<>+8(SB)/4, $0x7fffffff
DATA dataNormF32<>+12(SB)/4, $0x00800000
GLOBL dataNormF32<>(SB), RODATA|NOPTR, $16

// func Norm_AVX2_F32(x []float32) float32
// Requires: AVX, FMA3, SSE
TEXT ·Norm_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB3_1
	CMPQ   SI, $0x20
	JAE    LBB3_4
	VXORPS X0, X0, X0
	XORL   AX, AX
	JMP    LBB3_11

LBB3_1:
	VXORPS X0, X0, X0
	JMP    LBB3_12

LBB3_4:
	MOVQ   SI, AX
	ANDQ   $-32, AX
	LEAQ   -32(AX), CX
	MOVQ   CX, R8
	SHRQ   $0x05, R8
	ADDQ   $0x01, R8
	TESTQ  CX, CX
	JE     LBB3_5
	MOVQ   R8, CX
	ANDQ   $-2, CX
	VXORPS X0, X0, X0
	XORL   DX, DX
	VXORPS X1, X1, X1
	VXORPS X2, X2, X2
	VXORPS X3, X3, X3

LBB3_7:
	VMOVUPS     (DI)(DX*4), Y4
	VMOVUPS     32(DI)(DX*4), Y5
	VMOVUPS     64(DI)(DX*4), Y6
	VMOVUPS     96(DI)(DX*4), Y7
	VFMADD213PS Y0, Y4, Y4
	VFMADD213PS Y1, Y5, Y5
	VFMADD213PS Y2, Y6, Y6
	VFMADD213PS Y3, Y7, Y7
	VMOVUPS     128(DI)(DX*4), Y0
	VMOVUPS     160(DI)(DX*4), Y1
	VMOVUPS     192(DI)(DX*4), Y2
	VMOVUPS     224(DI)(DX*4), Y3
	VFMADD213PS Y4, Y0, Y0
	VFMADD213PS Y5, Y1, Y1
	VFMADD213PS Y6, Y2, Y2
	VFMADD213PS Y7, Y3, Y3
	ADDQ        $0x40, DX
	ADDQ        $-2, CX
	JNE         LBB3_7
	TESTB       $0x01, R8
	JE          LBB3_10

LBB3_9:
	VMOVUPS     (DI)(DX*4), Y4
	VMOVUPS     32(DI)(DX*4), Y5
	VMOVUPS     64(DI)(DX*4), Y6
	VMOVUPS     96(DI)(DX*4), Y7
	VFMADD231PS Y4, Y4, Y0
	VFMADD231PS Y5, Y5, Y1
	VFMADD231PS Y6, Y6, Y2
	VFMADD231PS Y7, Y7, Y3

LBB3_10:
	VADDPS       Y3, Y1, Y1
	VADDPS       Y2, Y0, Y0
	VADDPS       Y1, Y0, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPS       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDPS       X1, X0, X0
	VMOVSHDUP    X0, X1
	VADDSS       X1, X0, X0
	CMPQ         AX, SI
	JE           LBB3_12

LBB3_11:
	VMOVSS      (DI)(AX*4), X1
	VFMADD231SS X1, X1, X0
	ADDQ        $0x01, AX
	CMPQ        SI, AX
	JNE         LBB3_11

LBB3_12:
	VRSQRTSS     X0, X0, X1
	VMULSS       X1, X0, X2
	VFMADD213SS  dataNormF32<>+0(SB), X2, X1
	VMULSS       dataNormF32<>+4(SB), X2, X2
	VMULSS       X1, X2, X1
	VBROADCASTSS dataNormF32<>+8(SB), X2
	VANDPS       X2, X0, X0
	VCMPSS       $0x01, dataNormF32<>+12(SB), X0, X0
	VANDNPS      X1, X0, X0
	VZEROUPPER
	MOVSS        X0, ret+24(FP)
	RET

LBB3_5:
	VXORPS X0, X0, X0
	XORL   DX, DX
	VXORPS X1, X1, X1
	VXORPS X2, X2, X2
	VXORPS X3, X3, X3
	TESTB  $0x01, R8
	JNE    LBB3_9
	JMP    LBB3_10

// func Distance_AVX2_F64(x []float64, y []float64) float64
// Requires: AVX, FMA3, SSE2
TEXT ·Distance_AVX2_F64(SB), NOSPLIT, $0-56
	MOVQ   x_base+0(FP), DI
	MOVQ   y_base+24(FP), SI
	MOVQ   x_len+8(FP), DX
	TESTQ  DX, DX
	JE     LBB4_1
	CMPQ   DX, $0x10
	JAE    LBB4_4
	VXORPD X0, X0, X0
	XORL   AX, AX
	JMP    LBB4_7

LBB4_1:
	VXORPD  X0, X0, X0
	VSQRTSD X0, X0, X0
	MOVSD   X0, ret+48(FP)
	RET

LBB4_4:
	MOVQ   DX, AX
	ANDQ   $-16, AX
	VXORPD X0, X0, X0
	XORL   CX, CX
	VXORPD X1, X1, X1
	VXORPD X2, X2, X2
	VXORPD X3, X3, X3

LBB4_5:
	VMOVUPD      (DI)(CX*8), Y4
	VMOVUPD      32(DI)(CX*8), Y5
	VMOVUPD      64(DI)(CX*8), Y6
	VMOVUPD      96(DI)(CX*8), Y7
	VSUBPD       (SI)(CX*8), Y4, Y4
	VSUBPD       32(SI)(CX*8), Y5, Y5
	VSUBPD       64(SI)(CX*8), Y6, Y6
	VSUBPD       96(SI)(CX*8), Y7, Y7
	VFMADD231PD  Y4, Y4, Y0
	VFMADD231PD  Y5, Y5, Y1
	VFMADD231PD  Y6, Y6, Y2
	VFMADD231PD  Y7, Y7, Y3
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB4_5
	VADDPD       Y0, Y1, Y0
	VADDPD       Y0, Y2, Y0
	VADDPD       Y0, Y3, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPD       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDSD       X1, X0, X0
	CMPQ         AX, DX
	JE           LBB4_8

LBB4_7:
	VMOVSD      (DI)(AX*8), X1
	VSUBSD      (SI)(AX*8), X1, X1
	VFMADD231SD X1, X1, X0
	ADDQ        $0x01, AX
	CMPQ        DX, AX
	JNE         LBB4_7

LBB4_8:
	VSQRTSD X0, X0, X0
	VZEROUPPER
	MOVSD   X0, ret+48(FP)
	RET

DATA dataDistanceF32<>+0(SB)/4, $0xc0400000
DATA dataDistanceF32<>+4(SB)/4, $0xbf000000
DATA dataDistanceF32<>+8(SB)/4, $0x7fffffff
DATA dataDistanceF32<>+12(SB)/4, $0x00800000
GLOBL dataDistanceF32<>(SB), RODATA|NOPTR, $16

// func Distance_AVX2_F32(x []float32, y []float32) float32
// Requires: AVX, FMA3, SSE
TEXT ·Distance_AVX2_F32(SB), NOSPLIT, $0-52
	MOVQ   x_base+0(FP), DI
	MOVQ   y_base+24(FP), SI
	MOVQ   x_len+8(FP), DX
	TESTQ  DX, DX
	JE     LBB5_1
	CMPQ   DX, $0x20
	JAE    LBB5_4
	VXORPS X0, X0, X0
	XORL   AX, AX
	JMP    LBB5_7

LBB5_1:
	VXORPS X0, X0, X0
	JMP    LBB5_8

LBB5_4:
	MOVQ   DX, AX
	ANDQ   $-32, AX
	VXORPS X0, X0, X0
	XORL   CX, CX
	VXORPS X1, X1, X1
	VXORPS X2, X2, X2
	VXORPS X3, X3, X3

LBB5_5:
	VMOVUPS      (DI)(CX*4), Y4
	VMOVUPS      32(DI)(CX*4), Y5
	VMOVUPS      64(DI)(CX*4), Y6
	VMOVUPS      96(DI)(CX*4), Y7
	VSUBPS       (SI)(CX*4), Y4, Y4
	VSUBPS       32(SI)(CX*4), Y5, Y5
	VSUBPS       64(SI)(CX*4), Y6, Y6
	VSUBPS       96(SI)(CX*4), Y7, Y7
	VFMADD231PS  Y4, Y4, Y0
	VFMADD231PS  Y5, Y5, Y1
	VFMADD231PS  Y6, Y6, Y2
	VFMADD231PS  Y7, Y7, Y3
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB5_5
	VADDPS       Y0, Y1, Y0
	VADDPS       Y0, Y2, Y0
	VADDPS       Y0, Y3, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPS       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDPS       X1, X0, X0
	VMOVSHDUP    X0, X1
	VADDSS       X1, X0, X0
	CMPQ         AX, DX
	JE           LBB5_8

LBB5_7:
	VMOVSS      (DI)(AX*4), X1
	VSUBSS      (SI)(AX*4), X1, X1
	VFMADD231SS X1, X1, X0
	ADDQ        $0x01, AX
	CMPQ        DX, AX
	JNE         LBB5_7

LBB5_8:
	VRSQRTSS     X0, X0, X1
	VMULSS       X1, X0, X2
	VFMADD213SS  dataDistanceF32<>+0(SB), X2, X1
	VMULSS       dataDistanceF32<>+4(SB), X2, X2
	VMULSS       X1, X2, X1
	VBROADCASTSS dataDistanceF32<>+8(SB), X2
	VANDPS       X2, X0, X0
	VCMPSS       $0x01, dataDistanceF32<>+12(SB), X0, X0
	VANDNPS      X1, X0, X0
	VZEROUPPER
	MOVSS        X0, ret+48(FP)
	RET

DATA dataManhattanNormF64<>+0(SB)/8, $0x7fffffffffffffff
DATA dataManhattanNormF64<>+8(SB)/8, $0x7fffffffffffffff
DATA dataManhattanNormF64<>+16(SB)/8, $0x7fffffffffffffff
GLOBL dataManhattanNormF64<>(SB), RODATA|NOPTR, $24

// func ManhattanNorm_AVX2_F64(x []float64) float64
// Requires: AVX, SSE2
TEXT ·ManhattanNorm_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB6_1
	CMPQ   SI, $0x10
	JAE    LBB6_4
	VXORPD X0, X0, X0
	XORL   AX, AX
	JMP    LBB6_7

LBB6_1:
	VXORPS X0, X0, X0
	MOVSD  X0, ret+24(FP)
	RET

LBB6_4:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	VXORPD       X0, X0, X0
	VBROADCASTSD dataManhattanNormF64<>+0(SB), Y1
	XORL         CX, CX
	VXORPD       X2, X2, X2
	VXORPD       X3, X3, X3
	VXORPD       X4, X4, X4

LBB6_5:
	VANDPD       (DI)(CX*8), Y1, Y5
	VADDPD       Y0, Y5, Y0
	VANDPD       32(DI)(CX*8), Y1, Y5
	VADDPD       Y2, Y5, Y2
	VANDPD       64(DI)(CX*8), Y1, Y5
	VANDPD       96(DI)(CX*8), Y1, Y6
	VADDPD       Y3, Y5, Y3
	VADDPD       Y4, Y6, Y4
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB6_5
	VADDPD       Y0, Y2, Y0
	VADDPD       Y0, Y3, Y0
	VADDPD       Y0, Y4, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPD       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDSD       X1, X0, X0
	CMPQ         AX, SI
	JE           LBB6_9

LBB6_7:
	VMOVUPD dataManhattanNormF64<>+8(SB), X1

LBB6_8:
	VMOVSD (DI)(AX*8), X2
	VANDPD X1, X2, X2
	VADDSD X0, X2, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB6_8

LBB6_9:
	VZEROUPPER
	MOVSD X0, ret+24(FP)
	RET

DATA dataManhattanNormF32<>+0(SB)/4, $0x7fffffff
GLOBL dataManhattanNormF32<>(SB), RODATA|NOPTR, $4

// func ManhattanNorm_AVX2_F32(x []float32) float32
// Requires: AVX, SSE
TEXT ·ManhattanNorm_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB7_1
	CMPQ   SI, $0x20
	JAE    LBB7_4
	VXORPS X0, X0, X0
	XORL   AX, AX
	JMP    LBB7_7

LBB7_1:
	VXORPS X0, X0, X0
	MOVSS  X0, ret+24(FP)
	RET

LBB7_4:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	VXORPS       X0, X0, X0
	VBROADCASTSS dataManhattanNormF32<>+0(SB), Y1
	XORL         CX, CX
	VXORPS       X2, X2, X2
	VXORPS       X3, X3, X3
	VXORPS       X4, X4, X4

LBB7_5:
	VANDPS       (DI)(CX*4), Y1, Y5
	VADDPS       Y0, Y5, Y0
	VANDPS       32(DI)(CX*4), Y1, Y5
	VADDPS       Y2, Y5, Y2
	VANDPS       64(DI)(CX*4), Y1, Y5
	VANDPS       96(DI)(CX*4), Y1, Y6
	VADDPS       Y3, Y5, Y3
	VADDPS       Y4, Y6, Y4
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB7_5
	VADDPS       Y0, Y2, Y0
	VADDPS       Y0, Y3, Y0
	VADDPS       Y0, Y4, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPS       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDPS       X1, X0, X0
	VMOVSHDUP    X0, X1
	VADDSS       X1, X0, X0
	CMPQ         AX, SI
	JE           LBB7_9

LBB7_7:
	VBROADCASTSS dataManhattanNormF32<>+0(SB), X1

LBB7_8:
	VMOVSS (DI)(AX*4), X2
	VANDPS X1, X2, X2
	VADDSS X0, X2, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB7_8

LBB7_9:
	VZEROUPPER
	MOVSS X0, ret+24(FP)
	RET

DATA dataManhattanDistanceF64<>+0(SB)/8, $0x7fffffffffffffff
DATA dataManhattanDistanceF64<>+8(SB)/8, $0x7fffffffffffffff
DATA dataManhattanDistanceF64<>+16(SB)/8, $0x7fffffffffffffff
GLOBL dataManhattanDistanceF64<>(SB), RODATA|NOPTR, $24

// func ManhattanDistance_AVX2_F64(x []float64, y []float64) float64
// Requires: AVX, SSE2
TEXT ·ManhattanDistance_AVX2_F64(SB), NOSPLIT, $0-56
	MOVQ   x_base+0(FP), DI
	MOVQ   y_base+24(FP), SI
	MOVQ   x_len+8(FP), DX
	TESTQ  DX, DX
	JE     LBB8_1
	CMPQ   DX, $0x10
	JAE    LBB8_4
	VXORPD X0, X0, X0
	XORL   AX, AX
	JMP    LBB8_7

LBB8_1:
	VXORPS X0, X0, X0
	MOVSD  X0, ret+48(FP)
	RET

LBB8_4:
	MOVQ         DX, AX
	ANDQ         $-16, AX
	VXORPD       X0, X0, X0
	VBROADCASTSD dataManhattanDistanceF64<>+0(SB), Y1
	XORL         CX, CX
	VXORPD       X2, X2, X2
	VXORPD       X3, X3, X3
	VXORPD       X4, X4, X4

LBB8_5:
	VMOVUPD      (DI)(CX*8), Y5
	VMOVUPD      32(DI)(CX*8), Y6
	VMOVUPD      64(DI)(CX*8), Y7
	VMOVUPD      96(DI)(CX*8), Y8
	VSUBPD       (SI)(CX*8), Y5, Y5
	VSUBPD       32(SI)(CX*8), Y6, Y6
	VSUBPD       64(SI)(CX*8), Y7, Y7
	VSUBPD       96(SI)(CX*8), Y8, Y8
	VANDPD       Y1, Y5, Y5
	VADDPD       Y0, Y5, Y0
	VANDPD       Y1, Y6, Y5
	VADDPD       Y2, Y5, Y2
	VANDPD       Y1, Y7, Y5
	VADDPD       Y3, Y5, Y3
	VANDPD       Y1, Y8, Y5
	VADDPD       Y4, Y5, Y4
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB8_5
	VADDPD       Y0, Y2, Y0
	VADDPD       Y0, Y3, Y0
	VADDPD       Y0, Y4, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPD       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDSD       X1, X0, X0
	CMPQ         AX, DX
	JE           LBB8_9

LBB8_7:
	VMOVUPD dataManhattanDistanceF64<>+8(SB), X1

LBB8_8:
	VMOVSD (DI)(AX*8), X2
	VSUBSD (SI)(AX*8), X2, X2
	VANDPD X1, X2, X2
	VADDSD X0, X2, X0
	ADDQ   $0x01, AX
	CMPQ   DX, AX
	JNE    LBB8_8

LBB8_9:
	VZEROUPPER
	MOVSD X0, ret+48(FP)
	RET

DATA dataManhattanDistanceF32<>+0(SB)/4, $0x7fffffff
GLOBL dataManhattanDistanceF32<>(SB), RODATA|NOPTR, $4

// func ManhattanDistance_AVX2_F32(x []float32, y []float32) float32
// Requires: AVX, SSE
TEXT ·ManhattanDistance_AVX2_F32(SB), NOSPLIT, $0-52
	MOVQ   x_base+0(FP), DI
	MOVQ   y_base+24(FP), SI
	MOVQ   x_len+8(FP), DX
	TESTQ  DX, DX
	JE     LBB9_1
	CMPQ   DX, $0x20
	JAE    LBB9_4
	VXORPS X0, X0, X0
	XORL   AX, AX
	JMP    LBB9_7

LBB9_1:
	VXORPS X0, X0, X0
	MOVSS  X0, ret+48(FP)
	RET

LBB9_4:
	MOVQ         DX, AX
	ANDQ         $-32, AX
	VXORPS       X0, X0, X0
	VBROADCASTSS dataManhattanDistanceF32<>+0(SB), Y1
	XORL         CX, CX
	VXORPS       X2, X2, X2
	VXORPS       X3, X3, X3
	VXORPS       X4, X4, X4

LBB9_5:
	VMOVUPS      (DI)(CX*4), Y5
	VMOVUPS      32(DI)(CX*4), Y6
	VMOVUPS      64(DI)(CX*4), Y7
	VMOVUPS      96(DI)(CX*4), Y8
	VSUBPS       (SI)(CX*4), Y5, Y5
	VSUBPS       32(SI)(CX*4), Y6, Y6
	VSUBPS       64(SI)(CX*4), Y7, Y7
	VSUBPS       96(SI)(CX*4), Y8, Y8
	VANDPS       Y1, Y5, Y5
	VADDPS       Y0, Y5, Y0
	VANDPS       Y1, Y6, Y5
	VADDPS       Y2, Y5, Y2
	VANDPS       Y1, Y7, Y5
	VADDPS       Y3, Y5, Y3
	VANDPS       Y1, Y8, Y5
	VADDPS       Y4, Y5, Y4
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB9_5
	VADDPS       Y0, Y2, Y0
	VADDPS       Y0, Y3, Y0
	VADDPS       Y0, Y4, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPS       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDPS       X1, X0, X0
	VMOVSHDUP    X0, X1
	VADDSS       X1, X0, X0
	CMPQ         AX, DX
	JE           LBB9_9

LBB9_7:
	VBROADCASTSS dataManhattanDistanceF32<>+0(SB), X1

LBB9_8:
	VMOVSS (DI)(AX*4), X2
	VSUBSS (SI)(AX*4), X2, X2
	VANDPS X1, X2, X2
	VADDSS X0, X2, X0
	ADDQ   $0x01, AX
	CMPQ   DX, AX
	JNE    LBB9_8

LBB9_9:
	VZEROUPPER
	MOVSS X0, ret+48(FP)
	RET

// func CosineSimilarity_AVX2_F64(x []float64, y []float64) float64
// Requires: AVX, FMA3, SSE2
TEXT ·CosineSimilarity_AVX2_F64(SB), NOSPLIT, $0-56
	MOVQ   x_base+0(FP), DI
	MOVQ   y_base+24(FP), SI
	MOVQ   x_len+8(FP), DX
	TESTQ  DX, DX
	JE     LBB2_1
	CMPQ   DX, $0x08
	JAE    LBB2_5
	VXORPD X1, X1, X1
	XORL   AX, AX
	VXORPD X2, X2, X2
	VXORPD X0, X0, X0
	JMP    LBB2_4

LBB2_1:
	VXORPD  X0, X0, X0
	VXORPD  X1, X1, X1
	VSQRTSD X1, X1, X1
	VDIVSD  X1, X0, X0
	MOVSD   X0, ret+48(FP)
	RET

LBB2_5:
	MOVQ   DX, AX
	ANDQ   $-8, AX
	VXORPD X1, X1, X1
	XORL   CX, CX
	VXORPD X3, X3, X3
	VXORPD X2, X2, X2
	VXORPD X4, X4, X4
	VXORPD X0, X0, X0
	VXORPD X5, X5, X5

LBB2_6:
	VMOVUPD      (DI)(CX*8), Y6
	VMOVUPD      32(DI)(CX*8), Y7
	VMOVUPD      (SI)(CX*8), Y8
	VMOVUPD      32(SI)(CX*8), Y9
	VFMADD231PD  Y6, Y8, Y0
	VFMADD231PD  Y7, Y9, Y5
	VFMADD231PD  Y6, Y6, Y2
	VFMADD231PD  Y7, Y7, Y4
	VFMADD231PD  Y8, Y8, Y1
	VFMADD231PD  Y9, Y9, Y3
	ADDQ         $0x08, CX
	CMPQ         AX, CX
	JNE          LBB2_6
	VADDPD       Y0, Y5, Y0
	VEXTRACTF128 $0x01, Y0, X5
	VADDPD       X5, X0, X0
	VPERMILPD    $0x01, X0, X5
	VADDSD       X5, X0, X0
	VADDPD       Y2, Y4, Y2
	VEXTRACTF128 $0x01, Y2, X4
	VADDPD       X4, X2, X2
	VPERMILPD    $0x01, X2, X4
	VADDSD       X4, X2, X2
	VADDPD       Y1, Y3, Y1
	VEXTRACTF128 $0x01, Y1, X3
	VADDPD       X3, X1, X1
	VPERMILPD    $0x01, X1, X3
	VADDSD       X3, X1, X1
	CMPQ         AX, DX
	JE           LBB2_8

LBB2_4:
	VMOVSD      (DI)(AX*8), X3
	VMOVSD      (SI)(AX*8), X4
	VFMADD231SD X3, X4, X0
	VFMADD231SD X3, X3, X2
	VFMADD231SD X4, X4, X1
	ADDQ        $0x01, AX
	CMPQ        DX, AX
	JNE         LBB2_4

LBB2_8:
	VMULSD  X2, X1, X1
	VSQRTSD X1, X1, X1
	VDIVSD  X1, X0, X0
	VZEROUPPER
	MOVSD   X0, ret+48(FP)
	RET

DATA dataCosineSimilarityF32<>+0(SB)/4, $0xc0400000
DATA dataCosineSimilarityF32<>+4(SB)/4, $0xbf000000
GLOBL dataCosineSimilarityF32<>(SB), RODATA|NOPTR, $8

// func CosineSimilarity_AVX2_F32(x []float32, y []float32) float32
// Requires: AVX, FMA3, SSE
TEXT ·CosineSimilarity_AVX2_F32(SB), NOSPLIT, $0-52
	MOVQ   x_base+0(FP), DI
	MOVQ   y_base+24(FP), SI
	MOVQ   x_len+8(FP), DX
	TESTQ  DX, DX
	JE     LBB3_1
	CMPQ   DX, $0x10
	JAE    LBB3_5
	VXORPS X1, X1, X1
	XORL   AX, AX
	VXORPS X2, X2, X2
	VXORPS X0, X0, X0
	JMP    LBB3_4

LBB3_1:
	VXORPS X0, X0, X0
	VXORPS X1, X1, X1
	JMP    LBB3_9

LBB3_5:
	MOVQ   DX, AX
	ANDQ   $-16, AX
	VXORPS X1, X1, X1
	XORL   CX, CX
	VXORPS X3, X3, X3
	VXORPS X2, X2, X2
	VXORPS X4, X4, X4
	VXORPS X0, X0, X0
	VXORPS X5, X5, X5

LBB3_6:
	VMOVUPS      (DI)(CX*4), Y6
	VMOVUPS      32(DI)(CX*4), Y7
	VMOVUPS      (SI)(CX*4), Y8
	VMOVUPS      32(SI)(CX*4), Y9
	VFMADD231PS  Y6, Y8, Y0
	VFMADD231PS  Y7, Y9, Y5
	VFMADD231PS  Y6, Y6, Y2
	VFMADD231PS  Y7, Y7, Y4
	VFMADD231PS  Y8, Y8, Y1
	VFMADD231PS  Y9, Y9, Y3
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB3_6
	VADDPS       Y0, Y5, Y0
	VEXTRACTF128 $0x01, Y0, X5
	VADDPS       X5, X0, X0
	VPERMILPD    $0x01, X0, X5
	VADDPS       X5, X0, X0
	VMOVSHDUP    X0, X5
	VADDSS       X5, X0, X0
	VADDPS       Y2, Y4, Y2
	VEXTRACTF128 $0x01, Y2, X4
	VADDPS       X4, X2, X2
	VPERMILPD    $0x01, X2, X4
	VADDPS       X4, X2, X2
	VMOVSHDUP    X2, X4
	VADDSS       X4, X2, X2
	VADDPS       Y1, Y3, Y1
	VEXTRACTF128 $0x01, Y1, X3
	VADDPS       X3, X1, X1
	VPERMILPD    $0x01, X1, X3
	VADDPS       X3, X1, X1
	VMOVSHDUP    X1, X3
	VADDSS       X3, X1, X1
	CMPQ         AX, DX
	JE           LBB3_8

LBB3_4:
	VMOVSS      (DI)(AX*4), X3
	VMOVSS      (SI)(AX*4), X4
	VFMADD231SS X3, X4, X0
	VFMADD231SS X3, X3, X2
	VFMADD231SS X4, X4, X1
	ADDQ        $0x01, AX
	CMPQ        DX, AX
	JNE         LBB3_4

LBB3_8:
	VMULSS X2, X1, X1

LBB3_9:
	VRSQRTSS    X1, X1, X2
	VMULSS      X2, X1, X1
	VFMADD213SS dataCosineSimilarityF32<>+0(SB), X2, X1
	VMULSS      dataCosineSimilarityF32<>+4(SB), X2, X2
	VMULSS      X0, X2, X0
	VMULSS      X0, X1, X0
	VZEROUPPER
	MOVSS       X0, ret+48(FP)
	RET

// func Mat4Mul_AVX2_F64(x []float64, y []float64, z []float64)
// Requires: AVX, FMA3
TEXT ·Mat4Mul_AVX2_F64(SB), NOSPLIT, $0-72
	MOVQ         x_base+0(FP), DI
	MOVQ         y_base+24(FP), SI
	MOVQ         z_base+48(FP), DX
	VBROADCASTSD (SI), Y0
	VMOVUPD      (DX), Y1
	VMOVUPD      32(DX), Y2
	VMOVUPD      64(DX), Y3
	VMOVUPD      96(DX), Y4
	VMULPD       Y0, Y1, Y0
	VBROADCASTSD 8(SI), Y5
	VFMADD213PD  Y0, Y2, Y5
	VBROADCASTSD 16(SI), Y0
	VFMADD213PD  Y5, Y3, Y0
	VBROADCASTSD 24(SI), Y5
	VFMADD213PD  Y0, Y4, Y5
	VMOVUPD      Y5, (DI)
	VBROADCASTSD 32(SI), Y0
	VMULPD       Y0, Y1, Y0
	VBROADCASTSD 40(SI), Y1
	VFMADD213PD  Y0, Y2, Y1
	VBROADCASTSD 48(SI), Y0
	VFMADD213PD  Y1, Y3, Y0
	VBROADCASTSD 56(SI), Y1
	VFMADD213PD  Y0, Y4, Y1
	VMOVUPD      Y1, 32(DI)
	VBROADCASTSD 64(SI), Y0
	VMOVUPD      (DX), Y1
	VMOVUPD      32(DX), Y2
	VMOVUPD      64(DX), Y3
	VMOVUPD      96(DX), Y4
	VMULPD       Y0, Y1, Y0
	VBROADCASTSD 72(SI), Y5
	VFMADD213PD  Y0, Y2, Y5
	VBROADCASTSD 80(SI), Y0
	VFMADD213PD  Y5, Y3, Y0
	VBROADCASTSD 88(SI), Y5
	VFMADD213PD  Y0, Y4, Y5
	VMOVUPD      Y5, 64(DI)
	VBROADCASTSD 96(SI), Y0
	VMULPD       Y0, Y1, Y0
	VBROADCASTSD 104(SI), Y1
	VFMADD213PD  Y0, Y2, Y1
	VBROADCASTSD 112(SI), Y0
	VFMADD213PD  Y1, Y3, Y0
	VBROADCASTSD 120(SI), Y1
	VFMADD213PD  Y0, Y4, Y1
	VMOVUPD      Y1, 96(DI)
	VZEROUPPER
	RET

// func Mat4Mul_AVX2_F32(x []float32, y []float32, z []float32)
// Requires: AVX, AVX2, FMA3
TEXT ·Mat4Mul_AVX2_F32(SB), NOSPLIT, $0-72
	MOVQ           x_base+0(FP), DI
	MOVQ           y_base+24(FP), SI
	MOVQ           z_base+48(FP), DX
	VBROADCASTF128 (DX), Y0
	VBROADCASTF128 16(DX), Y1
	VBROADCASTF128 32(DX), Y2
	VBROADCASTF128 48(DX), Y3
	VMOVSS         16(SI), X4
	VMOVSS         (SI), X5
	VSHUFPS        $0x00, X4, X5, X4
	VMOVSS         4(SI), X5
	VMOVSS         8(SI), X6
	VMOVSS         12(SI), X7
	VPERMPD        $0x50, Y4, Y4
	VMULPS         Y4, Y0, Y0
	VMOVSS         20(SI), X4
	VSHUFPS        $0x00, X4, X5, X4
	VPERMPD        $0x50, Y4, Y4
	VFMADD213PS    Y0, Y1, Y4
	VMOVSS         24(SI), X0
	VSHUFPS        $0x00, X0, X6, X0
	VPERMPD        $0x50, Y0, Y0
	VFMADD213PS    Y4, Y2, Y0
	VMOVSS         28(SI), X1
	VSHUFPS        $0x00, X1, X7, X1
	VPERMPD        $0x50, Y1, Y1
	VFMADD213PS    Y0, Y3, Y1
	VBROADCASTF128 (DX), Y0
	VBROADCASTF128 16(DX), Y2
	VBROADCASTF128 32(DX), Y3
	VMOVUPS        Y1, (DI)
	VBROADCASTF128 48(DX), Y1
	VMOVSS         48(SI), X4
	VMOVSS         32(SI), X5
	VSHUFPS        $0x00, X4, X5, X4
	VMOVSS         36(SI), X5
	VMOVSS         40(SI), X6
	VMOVSS         44(SI), X7
	VPERMPD        $0x50, Y4, Y4
	VMULPS         Y4, Y0, Y0
	VMOVSS         52(SI), X4
	VSHUFPS        $0x00, X4, X5, X4
	VPERMPD        $0x50, Y4, Y4
	VFMADD213PS    Y0, Y2, Y4
	VMOVSS         56(SI), X0
	VSHUFPS        $0x00, X0, X6, X0
	VPERMPD        $0x50, Y0, Y0
	VFMADD213PS    Y4, Y3, Y0
	VMOVSS         60(SI), X2
	VSHUFPS        $0x00, X2, X7, X2
	VPERMPD        $0x50, Y2, Y2
	VFMADD213PS    Y0, Y1, Y2
	VMOVUPS        Y2, 32(DI)
	VZEROUPPER
	RET

// func MatMul_AVX2_F64(x []float64, y []float64, z []float64, a int, b int, c int)
// Requires: AVX, AVX2, FMA3
TEXT ·MatMul_AVX2_F64(SB), $8-96
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  a+72(FP), CX
	MOVQ  b+80(FP), R8
	MOVQ  c+88(FP), R9
	PUSHQ BP
	PUSHQ R15
	PUSHQ R14
	PUSHQ R13
	PUSHQ R12
	PUSHQ BX
	MOVQ  DX, -16(SP)
	MOVQ  CX, -8(SP)
	TESTQ CX, CX
	JE    LBB4_13
	TESTQ R8, R8
	JE    LBB4_13
	TESTQ R9, R9
	JE    LBB4_13
	MOVQ  R9, R12
	ANDQ  $-16, R12
	MOVQ  -16(SP), AX
	LEAQ  96(AX), CX
	XORQ  R15, R15
	LEAQ  (R15)(R9*8), R11
	LEAQ  96(DI), BX
	XORL  R14, R14
	JMP   LBB4_4

LBB4_12:
	ADDQ $0x01, R14
	ADDQ R11, BX
	ADDQ R11, DI
	CMPQ R14, -8(SP)
	JE   LBB4_13

LBB4_4:
	MOVQ  R14, R15
	IMULQ R8, R15
	MOVQ  -16(SP), R13
	MOVQ  CX, AX
	XORL  BP, BP
	JMP   LBB4_5

LBB4_11:
	ADDQ $0x01, BP
	ADDQ R11, AX
	ADDQ R11, R13
	CMPQ BP, R8
	JE   LBB4_12

LBB4_5:
	LEAQ   (R15)(BP*1), DX
	VMOVSD (SI)(DX*8), X0
	CMPQ   R9, $0x10
	JAE    LBB4_7
	XORL   DX, DX
	JMP    LBB4_10

LBB4_7:
	VBROADCASTSD X0, Y1
	XORL         R10, R10

LBB4_8:
	VMOVUPD     -96(AX)(R10*8), Y2
	VMOVUPD     -64(AX)(R10*8), Y3
	VMOVUPD     -32(AX)(R10*8), Y4
	VMOVUPD     (AX)(R10*8), Y5
	VFMADD213PD -96(BX)(R10*8), Y1, Y2
	VFMADD213PD -64(BX)(R10*8), Y1, Y3
	VFMADD213PD -32(BX)(R10*8), Y1, Y4
	VFMADD213PD (BX)(R10*8), Y1, Y5
	VMOVUPD     Y2, -96(BX)(R10*8)
	VMOVUPD     Y3, -64(BX)(R10*8)
	VMOVUPD     Y4, -32(BX)(R10*8)
	VMOVUPD     Y5, (BX)(R10*8)
	ADDQ        $0x10, R10
	CMPQ        R12, R10
	JNE         LBB4_8
	MOVQ        R12, DX
	CMPQ        R12, R9
	JE          LBB4_11

LBB4_10:
	VMOVSD      (R13)(DX*8), X1
	VFMADD213SD (DI)(DX*8), X0, X1
	VMOVSD      X1, (DI)(DX*8)
	ADDQ        $0x01, DX
	CMPQ        R9, DX
	JNE         LBB4_10
	JMP         LBB4_11

LBB4_13:
	POPQ BX
	POPQ R12
	POPQ R13
	POPQ R14
	POPQ R15
	POPQ BP
	VZEROUPPER
	RET

// func MatMul_AVX2_F32(x []float32, y []float32, z []float32, a int, b int, c int)
// Requires: AVX, AVX2, FMA3
TEXT ·MatMul_AVX2_F32(SB), $8-96
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  a+72(FP), CX
	MOVQ  b+80(FP), R8
	MOVQ  c+88(FP), R9
	PUSHQ BP
	PUSHQ R15
	PUSHQ R14
	PUSHQ R13
	PUSHQ R12
	PUSHQ BX
	MOVQ  DX, -16(SP)
	MOVQ  CX, -8(SP)
	TESTQ CX, CX
	JE    LBB5_13
	TESTQ R8, R8
	JE    LBB5_13
	TESTQ R9, R9
	JE    LBB5_13
	MOVQ  R9, R12
	ANDQ  $-32, R12
	MOVQ  -16(SP), AX
	LEAQ  96(AX), CX
	XORQ  R15, R15
	LEAQ  (R15)(R9*4), R11
	LEAQ  96(DI), BX
	XORL  R14, R14
	JMP   LBB5_4

LBB5_12:
	ADDQ $0x01, R14
	ADDQ R11, BX
	ADDQ R11, DI
	CMPQ R14, -8(SP)
	JE   LBB5_13

LBB5_4:
	MOVQ  R14, R15
	IMULQ R8, R15
	MOVQ  -16(SP), R13
	MOVQ  CX, AX
	XORL  BP, BP
	JMP   LBB5_5

LBB5_11:
	ADDQ $0x01, BP
	ADDQ R11, AX
	ADDQ R11, R13
	CMPQ BP, R8
	JE   LBB5_12

LBB5_5:
	LEAQ   (R15)(BP*1), DX
	VMOVSS (SI)(DX*4), X0
	CMPQ   R9, $0x20
	JAE    LBB5_7
	XORL   DX, DX
	JMP    LBB5_10

LBB5_7:
	VBROADCASTSS X0, Y1
	XORL         R10, R10

LBB5_8:
	VMOVUPS     -96(AX)(R10*4), Y2
	VMOVUPS     -64(AX)(R10*4), Y3
	VMOVUPS     -32(AX)(R10*4), Y4
	VMOVUPS     (AX)(R10*4), Y5
	VFMADD213PS -96(BX)(R10*4), Y1, Y2
	VFMADD213PS -64(BX)(R10*4), Y1, Y3
	VFMADD213PS -32(BX)(R10*4), Y1, Y4
	VFMADD213PS (BX)(R10*4), Y1, Y5
	VMOVUPS     Y2, -96(BX)(R10*4)
	VMOVUPS     Y3, -64(BX)(R10*4)
	VMOVUPS     Y4, -32(BX)(R10*4)
	VMOVUPS     Y5, (BX)(R10*4)
	ADDQ        $0x20, R10
	CMPQ        R12, R10
	JNE         LBB5_8
	MOVQ        R12, DX
	CMPQ        R12, R9
	JE          LBB5_11

LBB5_10:
	VMOVSS      (R13)(DX*4), X1
	VFMADD213SS (DI)(DX*4), X0, X1
	VMOVSS      X1, (DI)(DX*4)
	ADDQ        $0x01, DX
	CMPQ        R9, DX
	JNE         LBB5_10
	JMP         LBB5_11

LBB5_13:
	POPQ BX
	POPQ R12
	POPQ R13
	POPQ R14
	POPQ R15
	POPQ BP
	VZEROUPPER
	RET

// func MatMulVec_AVX2_F64(x []float64, y []float64, z []float64, a int, b int)
// Requires: AVX, FMA3
TEXT ·MatMulVec_AVX2_F64(SB), $0-88
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  a+72(FP), CX
	MOVQ  b+80(FP), R8
	PUSHQ BX
	TESTQ CX, CX
	JE    LBB6_10
	TESTQ R8, R8
	JE    LBB6_10
	MOVQ  R8, R9
	ANDQ  $-16, R9
	LEAQ  96(SI), AX
	XORQ  R10, R10
	LEAQ  (R10)(R8*8), R10
	XORL  R11, R11
	JMP   LBB6_3

LBB6_9:
	VMOVSD X0, (DI)(R11*8)
	ADDQ   $0x01, R11
	ADDQ   R10, AX
	ADDQ   R10, SI
	CMPQ   R11, CX
	JE     LBB6_10

LBB6_3:
	VMOVQ (DI)(R11*8), X0
	CMPQ  R8, $0x10
	JAE   LBB6_5
	XORL  BX, BX
	JMP   LBB6_8

LBB6_5:
	VMOVQ  X0, X0
	VXORPD X1, X1, X1
	XORL   BX, BX
	VXORPD X2, X2, X2
	VXORPD X3, X3, X3

LBB6_6:
	VMOVUPD      (DX)(BX*8), Y4
	VMOVUPD      32(DX)(BX*8), Y5
	VMOVUPD      64(DX)(BX*8), Y6
	VMOVUPD      96(DX)(BX*8), Y7
	VFMADD231PD  -96(AX)(BX*8), Y4, Y0
	VFMADD231PD  -64(AX)(BX*8), Y5, Y1
	VFMADD231PD  -32(AX)(BX*8), Y6, Y2
	VFMADD231PD  (AX)(BX*8), Y7, Y3
	ADDQ         $0x10, BX
	CMPQ         R9, BX
	JNE          LBB6_6
	VADDPD       Y0, Y1, Y0
	VADDPD       Y0, Y2, Y0
	VADDPD       Y0, Y3, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VADDPD       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VADDSD       X1, X0, X0
	MOVQ         R9, BX
	CMPQ         R9, R8
	JE           LBB6_9

LBB6_8:
	VMOVSD      (DX)(BX*8), X1
	VFMADD231SD (SI)(BX*8), X1, X0
	ADDQ        $0x01, BX
	CMPQ        R8, BX
	JNE         LBB6_8
	JMP         LBB6_9

LBB6_10:
	POPQ BX
	VZEROUPPER
	RET

// func MatMulVec_AVX2_F32(x []float32, y []float32, z []float32, a int, b int)
// Requires: AVX, FMA3
TEXT ·MatMulVec_AVX2_F32(SB), $0-88
	MOVQ   x_base+0(FP), DI
	MOVQ   y_base+24(FP), SI
	MOVQ   z_base+48(FP), DX
	MOVQ   a+72(FP), CX
	MOVQ   b+80(FP), R8
	PUSHQ  BX
	TESTQ  CX, CX
	JE     LBB7_10
	TESTQ  R8, R8
	JE     LBB7_10
	MOVQ   R8, R9
	ANDQ   $-32, R9
	LEAQ   96(SI), AX
	XORQ   R10, R10
	LEAQ   (R10)(R8*4), R10
	XORL   R11, R11
	VXORPS X0, X0, X0
	JMP    LBB7_3

LBB7_9:
	VMOVSS X1, (DI)(R11*4)
	ADDQ   $0x01, R11
	ADDQ   R10, AX
	ADDQ   R10, SI
	CMPQ   R11, CX
	JE     LBB7_10

LBB7_3:
	VMOVSS (DI)(R11*4), X1
	CMPQ   R8, $0x20
	JAE    LBB7_5
	XORL   BX, BX
	JMP    LBB7_8

LBB7_5:
	VBLENDPS $0x01, X1, X0, X1
	VXORPS   X2, X2, X2
	XORL     BX, BX
	VXORPS   X3, X3, X3
	VXORPS   X4, X4, X4

LBB7_6:
	VMOVUPS      (DX)(BX*4), Y5
	VMOVUPS      32(DX)(BX*4), Y6
	VMOVUPS      64(DX)(BX*4), Y7
	VMOVUPS      96(DX)(BX*4), Y8
	VFMADD231PS  -96(AX)(BX*4), Y5, Y1
	VFMADD231PS  -64(AX)(BX*4), Y6, Y2
	VFMADD231PS  -32(AX)(BX*4), Y7, Y3
	VFMADD231PS  (AX)(BX*4), Y8, Y4
	ADDQ         $0x20, BX
	CMPQ         R9, BX
	JNE          LBB7_6
	VADDPS       Y1, Y2, Y1
	VADDPS       Y1, Y3, Y1
	VADDPS       Y1, Y4, Y1
	VEXTRACTF128 $0x01, Y1, X2
	VADDPS       X2, X1, X1
	VPERMILPD    $0x01, X1, X2
	VADDPS       X2, X1, X1
	VMOVSHDUP    X1, X2
	VADDSS       X2, X1, X1
	MOVQ         R9, BX
	CMPQ         R9, R8
	JE           LBB7_9

LBB7_8:
	VMOVSS      (DX)(BX*4), X2
	VFMADD231SS (SI)(BX*4), X2, X1
	ADDQ        $0x01, BX
	CMPQ        R8, BX
	JNE         LBB7_8
	JMP         LBB7_9

LBB7_10:
	POPQ BX
	VZEROUPPER
	RET

// func MatMulTiled_AVX2_F64(x []float64, y []float64, z []float64, a int, b int, c int)
// Requires: AVX, AVX2, CMOV, FMA3
TEXT ·MatMulTiled_AVX2_F64(SB), $8-96
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  a+72(FP), CX
	MOVQ  b+80(FP), R8
	MOVQ  c+88(FP), R9
	PUSHQ BP
	PUSHQ R15
	PUSHQ R14
	PUSHQ R13
	PUSHQ R12
	PUSHQ BX
	SUBQ  $0x48, SP
	MOVQ  R9, -128(SP)
	MOVQ  R8, -104(SP)
	MOVQ  DX, -88(SP)
	MOVQ  DI, -112(SP)
	MOVQ  CX, -64(SP)
	ADDQ  $0x07, CX
	MOVQ  CX, -72(SP)
	JE    LBB8_21
	MOVQ  -104(SP), AX
	ADDQ  $0xff, AX
	MOVQ  AX, 8(SP)
	JE    LBB8_21
	MOVQ  -128(SP), AX
	ADDQ  $0xff, AX
	MOVQ  AX, -40(SP)
	JE    LBB8_21
	MOVQ  -88(SP), AX
	ADDQ  $0x60, AX
	MOVQ  AX, -48(SP)
	MOVQ  -128(SP), AX
	XORQ  R15, R15
	LEAQ  (R15)(AX*8), BX
	MOVQ  -112(SP), CX
	ADDQ  $0x60, CX
	MOVQ  CX, -96(SP)
	SHLQ  $0x06, AX
	MOVQ  AX, -80(SP)
	XORL  DX, DX
	JMP   LBB8_4

LBB8_20:
	MOVQ -80(SP), AX
	ADDQ AX, -96(SP)
	ADDQ AX, -112(SP)
	MOVQ -56(SP), AX
	MOVQ AX, DX
	CMPQ AX, -72(SP)
	JAE  LBB8_21

LBB8_4:
	LEAQ    8(DX), AX
	MOVQ    -64(SP), CX
	CMPQ    AX, CX
	MOVQ    AX, -56(SP)
	CMOVQGT CX, AX
	CDQE
	MOVQ    DX, -16(SP)
	MOVQ    AX, 24(SP)
	CMPQ    DX, AX
	JAE     LBB8_20
	XORL    AX, AX
	MOVQ    AX, -120(SP)
	MOVL    $+256, DX
	XORL    AX, AX
	JMP     LBB8_6

LBB8_19:
	MOVQ -120(SP), AX
	ADDL $0x01, AX
	MOVQ AX, -120(SP)
	MOVQ -24(SP), DX
	ADDQ $+256, DX
	MOVQ -32(SP), AX
	CMPQ AX, -40(SP)
	JAE  LBB8_20

LBB8_6:
	MOVL    AX, DI
	MOVQ    -128(SP), BP
	CMPQ    BP, DX
	MOVQ    DX, -24(SP)
	CMOVQLT BP, DX
	ADDQ    $+256, AX
	CMPQ    BP, AX
	MOVQ    AX, CX
	CMOVQLT BP, CX
	MOVQ    AX, -32(SP)
	CMOVQLT BP, AX
	CMPL    DI, AX
	JGE     LBB8_19
	MOVLQSX DI, R14
	MOVQ    -96(SP), DI
	LEAQ    (DI)(R14*8), DI
	MOVQ    DI, (SP)
	MOVLQSX DX, R11
	SUBQ    R14, R11
	ANDQ    $-16, R11
	MOVLQSX CX, R12
	MOVQ    -120(SP), CX
	SHLL    $0x08, CX
	MOVLQSX CX, CX
	SUBQ    CX, R12
	MOVLQSX AX, DX
	MOVQ    R12, CX
	ANDQ    $-16, CX
	MOVQ    -48(SP), AX
	LEAQ    (AX)(R14*8), AX
	MOVQ    AX, -8(SP)
	MOVQ    R14, R13
	MOVQ    CX, 64(SP)
	ADDQ    CX, R13
	XORL    AX, AX
	JMP     LBB8_8

LBB8_18:
	MOVQ 16(SP), AX
	CMPQ AX, 8(SP)
	JAE  LBB8_19

LBB8_8:
	MOVL    AX, CX
	ADDQ    $+256, AX
	MOVQ    -104(SP), DI
	CMPQ    AX, DI
	MOVQ    AX, 16(SP)
	CMOVQGT DI, AX
	CMPL    CX, AX
	JGE     LBB8_18
	MOVLQSX CX, DI
	MOVQ    -128(SP), CX
	MOVQ    DI, 48(SP)
	IMULQ   DI, CX
	MOVQ    -88(SP), DI
	LEAQ    (DI)(CX*8), DI
	MOVQ    DI, 40(SP)
	MOVQ    -8(SP), DI
	LEAQ    (DI)(CX*8), CX
	MOVQ    CX, 32(SP)
	CDQE
	MOVQ    -112(SP), CX
	MOVQ    (SP), R10
	MOVQ    -16(SP), R8
	JMP     LBB8_10

LBB8_17:
	MOVQ 56(SP), R8
	ADDQ $0x01, R8
	ADDQ BX, R10
	ADDQ BX, CX
	CMPQ R8, 24(SP)
	JAE  LBB8_18

LBB8_10:
	MOVQ  R8, 56(SP)
	IMULQ -104(SP), R8
	MOVQ  40(SP), R15
	MOVQ  32(SP), DI
	MOVQ  48(SP), R9
	JMP   LBB8_11

LBB8_16:
	ADDQ $0x01, R9
	ADDQ BX, DI
	ADDQ BX, R15
	CMPQ R9, AX
	JGE  LBB8_17

LBB8_11:
	LEAQ         (R9)(R8*1), BP
	VMOVSD       (SI)(BP*8), X0
	MOVQ         R14, BP
	CMPQ         R12, $0x10
	JB           LBB8_15
	VBROADCASTSD X0, Y1
	XORL         BP, BP

LBB8_13:
	VMOVUPD     -96(DI)(BP*8), Y2
	VMOVUPD     -64(DI)(BP*8), Y3
	VMOVUPD     -32(DI)(BP*8), Y4
	VMOVUPD     (DI)(BP*8), Y5
	VFMADD213PD -96(R10)(BP*8), Y1, Y2
	VFMADD213PD -64(R10)(BP*8), Y1, Y3
	VFMADD213PD -32(R10)(BP*8), Y1, Y4
	VFMADD213PD (R10)(BP*8), Y1, Y5
	VMOVUPD     Y2, -96(R10)(BP*8)
	VMOVUPD     Y3, -64(R10)(BP*8)
	VMOVUPD     Y4, -32(R10)(BP*8)
	VMOVUPD     Y5, (R10)(BP*8)
	ADDQ        $0x10, BP
	CMPQ        R11, BP
	JNE         LBB8_13
	MOVQ        R13, BP
	CMPQ        R12, 64(SP)
	JE          LBB8_16

LBB8_15:
	VMOVSD      (R15)(BP*8), X1
	VFMADD213SD (CX)(BP*8), X0, X1
	VMOVSD      X1, (CX)(BP*8)
	ADDQ        $0x01, BP
	CMPQ        BP, DX
	JL          LBB8_15
	JMP         LBB8_16

LBB8_21:
	ADDQ $0x48, SP
	POPQ BX
	POPQ R12
	POPQ R13
	POPQ R14
	POPQ R15
	POPQ BP
	VZEROUPPER
	RET

// func MatMulTiled_AVX2_F32(x []float32, y []float32, z []float32, a int, b int, c int)
// Requires: AVX, AVX2, CMOV, FMA3
TEXT ·MatMulTiled_AVX2_F32(SB), $8-96
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  a+72(FP), CX
	MOVQ  b+80(FP), R8
	MOVQ  c+88(FP), R9
	PUSHQ BP
	PUSHQ R15
	PUSHQ R14
	PUSHQ R13
	PUSHQ R12
	PUSHQ BX
	SUBQ  $0x48, SP
	MOVQ  R9, -128(SP)
	MOVQ  R8, -104(SP)
	MOVQ  DX, -88(SP)
	MOVQ  DI, -112(SP)
	MOVQ  CX, -64(SP)
	ADDQ  $0x07, CX
	MOVQ  CX, -72(SP)
	JE    LBB9_21
	MOVQ  -104(SP), AX
	ADDQ  $0xff, AX
	MOVQ  AX, 8(SP)
	JE    LBB9_21
	MOVQ  -128(SP), AX
	ADDQ  $0xff, AX
	MOVQ  AX, -40(SP)
	JE    LBB9_21
	MOVQ  -88(SP), AX
	ADDQ  $0x60, AX
	MOVQ  AX, -48(SP)
	MOVQ  -128(SP), AX
	XORQ  R15, R15
	LEAQ  (R15)(AX*4), BX
	MOVQ  -112(SP), CX
	ADDQ  $0x60, CX
	MOVQ  CX, -96(SP)
	SHLQ  $0x05, AX
	MOVQ  AX, -80(SP)
	XORL  DX, DX
	JMP   LBB9_4

LBB9_20:
	MOVQ -80(SP), AX
	ADDQ AX, -96(SP)
	ADDQ AX, -112(SP)
	MOVQ -56(SP), AX
	MOVQ AX, DX
	CMPQ AX, -72(SP)
	JAE  LBB9_21

LBB9_4:
	LEAQ    8(DX), AX
	MOVQ    -64(SP), CX
	CMPQ    AX, CX
	MOVQ    AX, -56(SP)
	CMOVQGT CX, AX
	CDQE
	MOVQ    DX, -16(SP)
	MOVQ    AX, 24(SP)
	CMPQ    DX, AX
	JAE     LBB9_20
	XORL    AX, AX
	MOVQ    AX, -120(SP)
	MOVL    $+256, DX
	XORL    AX, AX
	JMP     LBB9_6

LBB9_19:
	MOVQ -120(SP), AX
	ADDL $0x01, AX
	MOVQ AX, -120(SP)
	MOVQ -24(SP), DX
	ADDQ $+256, DX
	MOVQ -32(SP), AX
	CMPQ AX, -40(SP)
	JAE  LBB9_20

LBB9_6:
	MOVL    AX, DI
	MOVQ    -128(SP), BP
	CMPQ    BP, DX
	MOVQ    DX, -24(SP)
	CMOVQLT BP, DX
	ADDQ    $+256, AX
	CMPQ    BP, AX
	MOVQ    AX, CX
	CMOVQLT BP, CX
	MOVQ    AX, -32(SP)
	CMOVQLT BP, AX
	CMPL    DI, AX
	JGE     LBB9_19
	MOVLQSX DI, R14
	MOVQ    -96(SP), DI
	LEAQ    (DI)(R14*4), DI
	MOVQ    DI, (SP)
	MOVLQSX DX, R11
	SUBQ    R14, R11
	ANDQ    $-32, R11
	MOVLQSX CX, R12
	MOVQ    -120(SP), CX
	SHLL    $0x08, CX
	MOVLQSX CX, CX
	SUBQ    CX, R12
	MOVLQSX AX, DX
	MOVQ    R12, CX
	ANDQ    $-32, CX
	MOVQ    -48(SP), AX
	LEAQ    (AX)(R14*4), AX
	MOVQ    AX, -8(SP)
	MOVQ    R14, R13
	MOVQ    CX, 64(SP)
	ADDQ    CX, R13
	XORL    AX, AX
	JMP     LBB9_8

LBB9_18:
	MOVQ 16(SP), AX
	CMPQ AX, 8(SP)
	JAE  LBB9_19

LBB9_8:
	MOVL    AX, CX
	ADDQ    $+256, AX
	MOVQ    -104(SP), DI
	CMPQ    AX, DI
	MOVQ    AX, 16(SP)
	CMOVQGT DI, AX
	CMPL    CX, AX
	JGE     LBB9_18
	MOVLQSX CX, DI
	MOVQ    -128(SP), CX
	MOVQ    DI, 48(SP)
	IMULQ   DI, CX
	MOVQ    -88(SP), DI
	LEAQ    (DI)(CX*4), DI
	MOVQ    DI, 40(SP)
	MOVQ    -8(SP), DI
	LEAQ    (DI)(CX*4), CX
	MOVQ    CX, 32(SP)
	CDQE
	MOVQ    -112(SP), CX
	MOVQ    (SP), R10
	MOVQ    -16(SP), R8
	JMP     LBB9_10

LBB9_17:
	MOVQ 56(SP), R8
	ADDQ $0x01, R8
	ADDQ BX, R10
	ADDQ BX, CX
	CMPQ R8, 24(SP)
	JAE  LBB9_18

LBB9_10:
	MOVQ  R8, 56(SP)
	IMULQ -104(SP), R8
	MOVQ  40(SP), R15
	MOVQ  32(SP), DI
	MOVQ  48(SP), R9
	JMP   LBB9_11

LBB9_16:
	ADDQ $0x01, R9
	ADDQ BX, DI
	ADDQ BX, R15
	CMPQ R9, AX
	JGE  LBB9_17

LBB9_11:
	LEAQ         (R9)(R8*1), BP
	VMOVSS       (SI)(BP*4), X0
	MOVQ         R14, BP
	CMPQ         R12, $0x20
	JB           LBB9_15
	VBROADCASTSS X0, Y1
	XORL         BP, BP

LBB9_13:
	VMOVUPS     -96(DI)(BP*4), Y2
	VMOVUPS     -64(DI)(BP*4), Y3
	VMOVUPS     -32(DI)(BP*4), Y4
	VMOVUPS     (DI)(BP*4), Y5
	VFMADD213PS -96(R10)(BP*4), Y1, Y2
	VFMADD213PS -64(R10)(BP*4), Y1, Y3
	VFMADD213PS -32(R10)(BP*4), Y1, Y4
	VFMADD213PS (R10)(BP*4), Y1, Y5
	VMOVUPS     Y2, -96(R10)(BP*4)
	VMOVUPS     Y3, -64(R10)(BP*4)
	VMOVUPS     Y4, -32(R10)(BP*4)
	VMOVUPS     Y5, (R10)(BP*4)
	ADDQ        $0x20, BP
	CMPQ        R11, BP
	JNE         LBB9_13
	MOVQ        R13, BP
	CMPQ        R12, 64(SP)
	JE          LBB9_16

LBB9_15:
	VMOVSS      (R15)(BP*4), X1
	VFMADD213SS (CX)(BP*4), X0, X1
	VMOVSS      X1, (CX)(BP*4)
	ADDQ        $0x01, BP
	CMPQ        BP, DX
	JL          LBB9_15
	JMP         LBB9_16

LBB9_21:
	ADDQ $0x48, SP
	POPQ BX
	POPQ R12
	POPQ R13
	POPQ R14
	POPQ R15
	POPQ BP
	VZEROUPPER
	RET

// func Sqrt_AVX2_F64(x []float64) float64
// Requires: AVX, SSE2
TEXT ·Sqrt_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB0_7
	CMPQ  SI, $0x04
	JAE   LBB0_3
	XORL  AX, AX
	JMP   LBB0_6

LBB0_3:
	MOVQ SI, AX
	ANDQ $-4, AX
	XORL CX, CX

LBB0_4:
	VSQRTPD (DI)(CX*8), Y0
	VMOVUPD Y0, (DI)(CX*8)
	ADDQ    $0x04, CX
	CMPQ    AX, CX
	JNE     LBB0_4
	CMPQ    AX, SI
	JE      LBB0_7

LBB0_6:
	VMOVSD  (DI)(AX*8), X0
	VSQRTSD X0, X0, X0
	VMOVSD  X0, (DI)(AX*8)
	ADDQ    $0x01, AX
	CMPQ    SI, AX
	JNE     LBB0_6

LBB0_7:
	VZEROUPPER
	MOVSD X0, ret+24(FP)
	RET

DATA dataSqrtF32<>+0(SB)/4, $0xc0400000
DATA dataSqrtF32<>+4(SB)/4, $0xbf000000
DATA dataSqrtF32<>+8(SB)/4, $0x7fffffff
DATA dataSqrtF32<>+12(SB)/4, $0x00800000
GLOBL dataSqrtF32<>(SB), RODATA|NOPTR, $16

// func Sqrt_AVX2_F32(x []float32) float32
// Requires: AVX, FMA3, SSE
TEXT ·Sqrt_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB1_8
	CMPQ  SI, $0x20
	JAE   LBB1_3
	XORL  AX, AX
	JMP   LBB1_6

LBB1_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	XORL         CX, CX
	VBROADCASTSS dataSqrtF32<>+0(SB), Y0
	VBROADCASTSS dataSqrtF32<>+4(SB), Y1
	VBROADCASTSS dataSqrtF32<>+8(SB), Y2
	VBROADCASTSS dataSqrtF32<>+12(SB), Y3

LBB1_4:
	VMOVUPS     (DI)(CX*4), Y4
	VMOVUPS     32(DI)(CX*4), Y5
	VMOVUPS     64(DI)(CX*4), Y6
	VRSQRTPS    Y4, Y7
	VMOVUPS     96(DI)(CX*4), Y8
	VMULPS      Y7, Y4, Y9
	VFMADD213PS Y0, Y9, Y7
	VMULPS      Y1, Y9, Y9
	VMULPS      Y7, Y9, Y7
	VANDPS      Y2, Y4, Y4
	VCMPPS      $0x02, Y4, Y3, Y4
	VANDPS      Y7, Y4, Y4
	VRSQRTPS    Y5, Y7
	VMULPS      Y7, Y5, Y9
	VFMADD213PS Y0, Y9, Y7
	VMULPS      Y1, Y9, Y9
	VMULPS      Y7, Y9, Y7
	VANDPS      Y2, Y5, Y5
	VCMPPS      $0x02, Y5, Y3, Y5
	VRSQRTPS    Y6, Y9
	VANDPS      Y7, Y5, Y5
	VMULPS      Y6, Y9, Y7
	VFMADD213PS Y0, Y7, Y9
	VMULPS      Y1, Y7, Y7
	VMULPS      Y7, Y9, Y7
	VANDPS      Y2, Y6, Y6
	VCMPPS      $0x02, Y6, Y3, Y6
	VANDPS      Y7, Y6, Y6
	VRSQRTPS    Y8, Y7
	VMULPS      Y7, Y8, Y9
	VFMADD213PS Y0, Y9, Y7
	VMULPS      Y1, Y9, Y9
	VMULPS      Y7, Y9, Y7
	VANDPS      Y2, Y8, Y8
	VCMPPS      $0x02, Y8, Y3, Y8
	VANDPS      Y7, Y8, Y7
	VMOVUPS     Y4, (DI)(CX*4)
	VMOVUPS     Y5, 32(DI)(CX*4)
	VMOVUPS     Y6, 64(DI)(CX*4)
	VMOVUPS     Y7, 96(DI)(CX*4)
	ADDQ        $0x20, CX
	CMPQ        AX, CX
	JNE         LBB1_4
	CMPQ        AX, SI
	JE          LBB1_8

LBB1_6:
	VMOVSS       dataSqrtF32<>+0(SB), X0
	VMOVSS       dataSqrtF32<>+4(SB), X1
	VBROADCASTSS dataSqrtF32<>+8(SB), X2
	VMOVSS       dataSqrtF32<>+12(SB), X3

LBB1_7:
	VMOVSS      (DI)(AX*4), X4
	VRSQRTSS    X4, X4, X5
	VMULSS      X5, X4, X6
	VFMADD213SS X0, X6, X5
	VMULSS      X1, X6, X6
	VMULSS      X5, X6, X5
	VANDPS      X2, X4, X4
	VCMPSS      $0x01, X3, X4, X4
	VANDNPS     X5, X4, X4
	VMOVSS      X4, (DI)(AX*4)
	ADDQ        $0x01, AX
	CMPQ        SI, AX
	JNE         LBB1_7

LBB1_8:
	VZEROUPPER
	MOVSS X0, ret+24(FP)
	RET

DATA dataRoundF64<>+0(SB)/8, $0x8000000000000000
DATA dataRoundF64<>+8(SB)/8, $0x3fdfffffffffffff
DATA dataRoundF64<>+16(SB)/8, $0x8000000000000000
DATA dataRoundF64<>+24(SB)/8, $0x8000000000000000
GLOBL dataRoundF64<>(SB), RODATA|NOPTR, $32

// func Round_AVX2_F64(x []float64) float64
// Requires: AVX, SSE2
TEXT ·Round_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB2_8
	CMPQ  SI, $0x10
	JAE   LBB2_3
	XORL  AX, AX
	JMP   LBB2_6

LBB2_3:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	XORL         CX, CX
	VBROADCASTSD dataRoundF64<>+0(SB), Y0
	VBROADCASTSD dataRoundF64<>+8(SB), Y1

LBB2_4:
	VMOVUPD  (DI)(CX*8), Y2
	VMOVUPD  32(DI)(CX*8), Y3
	VMOVUPD  64(DI)(CX*8), Y4
	VMOVUPD  96(DI)(CX*8), Y5
	VANDPD   Y0, Y2, Y6
	VORPD    Y1, Y6, Y6
	VADDPD   Y6, Y2, Y2
	VROUNDPD $0x0b, Y2, Y2
	VANDPD   Y0, Y3, Y6
	VORPD    Y1, Y6, Y6
	VADDPD   Y6, Y3, Y3
	VROUNDPD $0x0b, Y3, Y3
	VANDPD   Y0, Y4, Y6
	VORPD    Y1, Y6, Y6
	VADDPD   Y6, Y4, Y4
	VROUNDPD $0x0b, Y4, Y4
	VANDPD   Y0, Y5, Y6
	VORPD    Y1, Y6, Y6
	VADDPD   Y6, Y5, Y5
	VROUNDPD $0x0b, Y5, Y5
	VMOVUPD  Y2, (DI)(CX*8)
	VMOVUPD  Y3, 32(DI)(CX*8)
	VMOVUPD  Y4, 64(DI)(CX*8)
	VMOVUPD  Y5, 96(DI)(CX*8)
	ADDQ     $0x10, CX
	CMPQ     AX, CX
	JNE      LBB2_4
	CMPQ     AX, SI
	JE       LBB2_8

LBB2_6:
	VMOVUPD  dataRoundF64<>+16(SB), X0
	VMOVDDUP dataRoundF64<>+8(SB), X1

LBB2_7:
	VMOVSD   (DI)(AX*8), X2
	VANDPD   X0, X2, X3
	VORPD    X1, X3, X3
	VADDSD   X3, X2, X2
	VROUNDSD $0x0b, X2, X2, X2
	VMOVSD   X2, (DI)(AX*8)
	ADDQ     $0x01, AX
	CMPQ     SI, AX
	JNE      LBB2_7

LBB2_8:
	VZEROUPPER
	MOVSD X0, ret+24(FP)
	RET

DATA dataRoundF32<>+0(SB)/4, $0x80000000
DATA dataRoundF32<>+4(SB)/4, $0x3effffff
GLOBL dataRoundF32<>(SB), RODATA|NOPTR, $8

// func Round_AVX2_F32(x []float32) float32
// Requires: AVX, SSE
TEXT ·Round_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB3_8
	CMPQ  SI, $0x20
	JAE   LBB3_3
	XORL  AX, AX
	JMP   LBB3_6

LBB3_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	XORL         CX, CX
	VBROADCASTSS dataRoundF32<>+0(SB), Y0
	VBROADCASTSS dataRoundF32<>+4(SB), Y1

LBB3_4:
	VMOVUPS  (DI)(CX*4), Y2
	VMOVUPS  32(DI)(CX*4), Y3
	VMOVUPS  64(DI)(CX*4), Y4
	VMOVUPS  96(DI)(CX*4), Y5
	VANDPS   Y0, Y2, Y6
	VORPS    Y1, Y6, Y6
	VADDPS   Y6, Y2, Y2
	VROUNDPS $0x0b, Y2, Y2
	VANDPS   Y0, Y3, Y6
	VORPS    Y1, Y6, Y6
	VADDPS   Y6, Y3, Y3
	VROUNDPS $0x0b, Y3, Y3
	VANDPS   Y0, Y4, Y6
	VORPS    Y1, Y6, Y6
	VADDPS   Y6, Y4, Y4
	VROUNDPS $0x0b, Y4, Y4
	VANDPS   Y0, Y5, Y6
	VORPS    Y1, Y6, Y6
	VADDPS   Y6, Y5, Y5
	VROUNDPS $0x0b, Y5, Y5
	VMOVUPS  Y2, (DI)(CX*4)
	VMOVUPS  Y3, 32(DI)(CX*4)
	VMOVUPS  Y4, 64(DI)(CX*4)
	VMOVUPS  Y5, 96(DI)(CX*4)
	ADDQ     $0x20, CX
	CMPQ     AX, CX
	JNE      LBB3_4
	CMPQ     AX, SI
	JE       LBB3_8

LBB3_6:
	VBROADCASTSS dataRoundF32<>+0(SB), X0
	VBROADCASTSS dataRoundF32<>+4(SB), X1

LBB3_7:
	VMOVSS   (DI)(AX*4), X2
	VANDPS   X0, X2, X3
	VORPS    X1, X3, X3
	VADDSS   X3, X2, X2
	VROUNDSS $0x0b, X2, X2, X2
	VMOVSS   X2, (DI)(AX*4)
	ADDQ     $0x01, AX
	CMPQ     SI, AX
	JNE      LBB3_7

LBB3_8:
	VZEROUPPER
	MOVSS X0, ret+24(FP)
	RET

// func Floor_AVX2_F64(x []float64) float64
// Requires: AVX, SSE2
TEXT ·Floor_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB4_11
	CMPQ  SI, $0x10
	JAE   LBB4_3
	XORL  AX, AX
	JMP   LBB4_10

LBB4_3:
	MOVQ  SI, AX
	ANDQ  $-16, AX
	LEAQ  -16(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x04, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB4_4
	MOVQ  R8, DX
	ANDQ  $-2, DX
	XORL  CX, CX

LBB4_6:
	VROUNDPD $0x09, (DI)(CX*8), Y0
	VROUNDPD $0x09, 32(DI)(CX*8), Y1
	VROUNDPD $0x09, 64(DI)(CX*8), Y2
	VROUNDPD $0x09, 96(DI)(CX*8), Y3
	VMOVUPD  Y0, (DI)(CX*8)
	VMOVUPD  Y1, 32(DI)(CX*8)
	VMOVUPD  Y2, 64(DI)(CX*8)
	VMOVUPD  Y3, 96(DI)(CX*8)
	VROUNDPD $0x09, 128(DI)(CX*8), Y0
	VROUNDPD $0x09, 160(DI)(CX*8), Y1
	VROUNDPD $0x09, 192(DI)(CX*8), Y2
	VROUNDPD $0x09, 224(DI)(CX*8), Y3
	VMOVUPD  Y0, 128(DI)(CX*8)
	VMOVUPD  Y1, 160(DI)(CX*8)
	VMOVUPD  Y2, 192(DI)(CX*8)
	VMOVUPD  Y3, 224(DI)(CX*8)
	ADDQ     $0x20, CX
	ADDQ     $-2, DX
	JNE      LBB4_6
	TESTB    $0x01, R8
	JE       LBB4_9

LBB4_8:
	VROUNDPD $0x09, (DI)(CX*8), Y0
	VROUNDPD $0x09, 32(DI)(CX*8), Y1
	VROUNDPD $0x09, 64(DI)(CX*8), Y2
	VROUNDPD $0x09, 96(DI)(CX*8), Y3
	VMOVUPD  Y0, (DI)(CX*8)
	VMOVUPD  Y1, 32(DI)(CX*8)
	VMOVUPD  Y2, 64(DI)(CX*8)
	VMOVUPD  Y3, 96(DI)(CX*8)

LBB4_9:
	CMPQ AX, SI
	JE   LBB4_11

LBB4_10:
	VMOVSD   (DI)(AX*8), X0
	VROUNDSD $0x09, X0, X0, X0
	VMOVSD   X0, (DI)(AX*8)
	ADDQ     $0x01, AX
	CMPQ     SI, AX
	JNE      LBB4_10

LBB4_11:
	VZEROUPPER
	MOVSD X0, ret+24(FP)
	RET

LBB4_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB4_8
	JMP   LBB4_9

// func Floor_AVX2_F32(x []float32) float32
// Requires: AVX, SSE
TEXT ·Floor_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB5_11
	CMPQ  SI, $0x20
	JAE   LBB5_3
	XORL  AX, AX
	JMP   LBB5_10

LBB5_3:
	MOVQ  SI, AX
	ANDQ  $-32, AX
	LEAQ  -32(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x05, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB5_4
	MOVQ  R8, DX
	ANDQ  $-2, DX
	XORL  CX, CX

LBB5_6:
	VROUNDPS $0x09, (DI)(CX*4), Y0
	VROUNDPS $0x09, 32(DI)(CX*4), Y1
	VROUNDPS $0x09, 64(DI)(CX*4), Y2
	VROUNDPS $0x09, 96(DI)(CX*4), Y3
	VMOVUPS  Y0, (DI)(CX*4)
	VMOVUPS  Y1, 32(DI)(CX*4)
	VMOVUPS  Y2, 64(DI)(CX*4)
	VMOVUPS  Y3, 96(DI)(CX*4)
	VROUNDPS $0x09, 128(DI)(CX*4), Y0
	VROUNDPS $0x09, 160(DI)(CX*4), Y1
	VROUNDPS $0x09, 192(DI)(CX*4), Y2
	VROUNDPS $0x09, 224(DI)(CX*4), Y3
	VMOVUPS  Y0, 128(DI)(CX*4)
	VMOVUPS  Y1, 160(DI)(CX*4)
	VMOVUPS  Y2, 192(DI)(CX*4)
	VMOVUPS  Y3, 224(DI)(CX*4)
	ADDQ     $0x40, CX
	ADDQ     $-2, DX
	JNE      LBB5_6
	TESTB    $0x01, R8
	JE       LBB5_9

LBB5_8:
	VROUNDPS $0x09, (DI)(CX*4), Y0
	VROUNDPS $0x09, 32(DI)(CX*4), Y1
	VROUNDPS $0x09, 64(DI)(CX*4), Y2
	VROUNDPS $0x09, 96(DI)(CX*4), Y3
	VMOVUPS  Y0, (DI)(CX*4)
	VMOVUPS  Y1, 32(DI)(CX*4)
	VMOVUPS  Y2, 64(DI)(CX*4)
	VMOVUPS  Y3, 96(DI)(CX*4)

LBB5_9:
	CMPQ AX, SI
	JE   LBB5_11

LBB5_10:
	VMOVSS   (DI)(AX*4), X0
	VROUNDSS $0x09, X0, X0, X0
	VMOVSS   X0, (DI)(AX*4)
	ADDQ     $0x01, AX
	CMPQ     SI, AX
	JNE      LBB5_10

LBB5_11:
	VZEROUPPER
	MOVSS X0, ret+24(FP)
	RET

LBB5_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB5_8
	JMP   LBB5_9

// func Ceil_AVX2_F64(x []float64) float64
// Requires: AVX, SSE2
TEXT ·Ceil_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB6_11
	CMPQ  SI, $0x10
	JAE   LBB6_3
	XORL  AX, AX
	JMP   LBB6_10

LBB6_3:
	MOVQ  SI, AX
	ANDQ  $-16, AX
	LEAQ  -16(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x04, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB6_4
	MOVQ  R8, DX
	ANDQ  $-2, DX
	XORL  CX, CX

LBB6_6:
	VROUNDPD $0x0a, (DI)(CX*8), Y0
	VROUNDPD $0x0a, 32(DI)(CX*8), Y1
	VROUNDPD $0x0a, 64(DI)(CX*8), Y2
	VROUNDPD $0x0a, 96(DI)(CX*8), Y3
	VMOVUPD  Y0, (DI)(CX*8)
	VMOVUPD  Y1, 32(DI)(CX*8)
	VMOVUPD  Y2, 64(DI)(CX*8)
	VMOVUPD  Y3, 96(DI)(CX*8)
	VROUNDPD $0x0a, 128(DI)(CX*8), Y0
	VROUNDPD $0x0a, 160(DI)(CX*8), Y1
	VROUNDPD $0x0a, 192(DI)(CX*8), Y2
	VROUNDPD $0x0a, 224(DI)(CX*8), Y3
	VMOVUPD  Y0, 128(DI)(CX*8)
	VMOVUPD  Y1, 160(DI)(CX*8)
	VMOVUPD  Y2, 192(DI)(CX*8)
	VMOVUPD  Y3, 224(DI)(CX*8)
	ADDQ     $0x20, CX
	ADDQ     $-2, DX
	JNE      LBB6_6
	TESTB    $0x01, R8
	JE       LBB6_9

LBB6_8:
	VROUNDPD $0x0a, (DI)(CX*8), Y0
	VROUNDPD $0x0a, 32(DI)(CX*8), Y1
	VROUNDPD $0x0a, 64(DI)(CX*8), Y2
	VROUNDPD $0x0a, 96(DI)(CX*8), Y3
	VMOVUPD  Y0, (DI)(CX*8)
	VMOVUPD  Y1, 32(DI)(CX*8)
	VMOVUPD  Y2, 64(DI)(CX*8)
	VMOVUPD  Y3, 96(DI)(CX*8)

LBB6_9:
	CMPQ AX, SI
	JE   LBB6_11

LBB6_10:
	VMOVSD   (DI)(AX*8), X0
	VROUNDSD $0x0a, X0, X0, X0
	VMOVSD   X0, (DI)(AX*8)
	ADDQ     $0x01, AX
	CMPQ     SI, AX
	JNE      LBB6_10

LBB6_11:
	VZEROUPPER
	MOVSD X0, ret+24(FP)
	RET

LBB6_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB6_8
	JMP   LBB6_9

// func Ceil_AVX2_F32(x []float32) float32
// Requires: AVX, SSE
TEXT ·Ceil_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB7_11
	CMPQ  SI, $0x20
	JAE   LBB7_3
	XORL  AX, AX
	JMP   LBB7_10

LBB7_3:
	MOVQ  SI, AX
	ANDQ  $-32, AX
	LEAQ  -32(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x05, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB7_4
	MOVQ  R8, DX
	ANDQ  $-2, DX
	XORL  CX, CX

LBB7_6:
	VROUNDPS $0x0a, (DI)(CX*4), Y0
	VROUNDPS $0x0a, 32(DI)(CX*4), Y1
	VROUNDPS $0x0a, 64(DI)(CX*4), Y2
	VROUNDPS $0x0a, 96(DI)(CX*4), Y3
	VMOVUPS  Y0, (DI)(CX*4)
	VMOVUPS  Y1, 32(DI)(CX*4)
	VMOVUPS  Y2, 64(DI)(CX*4)
	VMOVUPS  Y3, 96(DI)(CX*4)
	VROUNDPS $0x0a, 128(DI)(CX*4), Y0
	VROUNDPS $0x0a, 160(DI)(CX*4), Y1
	VROUNDPS $0x0a, 192(DI)(CX*4), Y2
	VROUNDPS $0x0a, 224(DI)(CX*4), Y3
	VMOVUPS  Y0, 128(DI)(CX*4)
	VMOVUPS  Y1, 160(DI)(CX*4)
	VMOVUPS  Y2, 192(DI)(CX*4)
	VMOVUPS  Y3, 224(DI)(CX*4)
	ADDQ     $0x40, CX
	ADDQ     $-2, DX
	JNE      LBB7_6
	TESTB    $0x01, R8
	JE       LBB7_9

LBB7_8:
	VROUNDPS $0x0a, (DI)(CX*4), Y0
	VROUNDPS $0x0a, 32(DI)(CX*4), Y1
	VROUNDPS $0x0a, 64(DI)(CX*4), Y2
	VROUNDPS $0x0a, 96(DI)(CX*4), Y3
	VMOVUPS  Y0, (DI)(CX*4)
	VMOVUPS  Y1, 32(DI)(CX*4)
	VMOVUPS  Y2, 64(DI)(CX*4)
	VMOVUPS  Y3, 96(DI)(CX*4)

LBB7_9:
	CMPQ AX, SI
	JE   LBB7_11

LBB7_10:
	VMOVSS   (DI)(AX*4), X0
	VROUNDSS $0x0a, X0, X0, X0
	VMOVSS   X0, (DI)(AX*4)
	ADDQ     $0x01, AX
	CMPQ     SI, AX
	JNE      LBB7_10

LBB7_11:
	VZEROUPPER
	MOVSS X0, ret+24(FP)
	RET

LBB7_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB7_8
	JMP   LBB7_9

DATA dataPowF64<>+0(SB)/8, $0x7fffffffffffffff
DATA dataPowF64<>+8(SB)/8, $0x3fe6a09e667f3bcd
DATA dataPowF64<>+16(SB)/8, $0xbff0000000000000
DATA dataPowF64<>+24(SB)/8, $0x401a509f46f4fa53
DATA dataPowF64<>+32(SB)/8, $0x3fdfe818a0fe1a83
DATA dataPowF64<>+40(SB)/8, $0x3f07bc0962b395ca
DATA dataPowF64<>+48(SB)/8, $0x404e798eb86c3351
DATA dataPowF64<>+56(SB)/8, $0x403de9738b8cb9c9
DATA dataPowF64<>+64(SB)/8, $0x40340a202d99830a
DATA dataPowF64<>+72(SB)/8, $0x404c8e7597479a10
DATA dataPowF64<>+80(SB)/8, $0x4054c30b52213498
DATA dataPowF64<>+88(SB)/8, $0x402e20359e903e37
DATA dataPowF64<>+96(SB)/8, $0x407351945dc908a5
DATA dataPowF64<>+104(SB)/8, $0x406bb86590fcfb56
DATA dataPowF64<>+112(SB)/8, $0x404e0f304466448e
DATA dataPowF64<>+120(SB)/8, $0x406b0db13e48e066
DATA dataPowF64<>+128(SB)/8, $0x4330000000000000
DATA dataPowF64<>+136(SB)/8, $0xc3300000000003ff
DATA dataPowF64<>+144(SB)/8, $0x3ff0000000000000
DATA dataPowF64<>+152(SB)/8, $0xbfe0000000000000
DATA dataPowF64<>+160(SB)/8, $0x3fe0000000000000
DATA dataPowF64<>+168(SB)/8, $0x3ff71547652b82fe
DATA dataPowF64<>+176(SB)/8, $0xbfe62e4000000000
DATA dataPowF64<>+184(SB)/8, $0x3eb7f7d1cf79abca
DATA dataPowF64<>+192(SB)/8, $0x3fe62e42fefa39ef
DATA dataPowF64<>+200(SB)/8, $0x3e21eed8eff8d898
DATA dataPowF64<>+208(SB)/8, $0x3de6124613a86d09
DATA dataPowF64<>+216(SB)/8, $0x3e927e4fb7789f5c
DATA dataPowF64<>+224(SB)/8, $0x3e5ae64567f544e4
DATA dataPowF64<>+232(SB)/8, $0x3efa01a01a01a01a
DATA dataPowF64<>+240(SB)/8, $0x3ec71de3a556c734
DATA dataPowF64<>+248(SB)/8, $0x3f56c16c16c16c17
DATA dataPowF64<>+256(SB)/8, $0x3f2a01a01a01a01a
DATA dataPowF64<>+264(SB)/8, $0x3fa5555555555555
DATA dataPowF64<>+272(SB)/8, $0x3f81111111111111
DATA dataPowF64<>+280(SB)/8, $0x3fc5555555555555
DATA dataPowF64<>+288(SB)/8, $0x00000000000007fe
DATA dataPowF64<>+296(SB)/8, $0x40a7700000000000
DATA dataPowF64<>+304(SB)/8, $0x0000000000000001
DATA dataPowF64<>+312(SB)/8, $0xc0a7700000000000
DATA dataPowF64<>+320(SB)/8, $0x7ff0000000000000
DATA dataPowF64<>+328(SB)/8, $0x7ff8002040000000
DATA dataPowF64<>+336(SB)/8, $0x000fffffffffffff
DATA dataPowF64<>+344(SB)/8, $0x000fffffffffffff
DATA dataPowF64<>+352(SB)/8, $0x3fe0000000000000
DATA dataPowF64<>+360(SB)/8, $0x3fe0000000000000
GLOBL dataPowF64<>(SB), RODATA|NOPTR, $368

// func Pow_4x_AVX2_F64(x []float64, y []float64)
// Requires: AVX, AVX2, FMA3
TEXT ·Pow_4x_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), DI
	MOVQ         y_base+24(FP), SI
	MOVQ         x_len+8(FP), DX
	SUBQ         $+1192, SP
	ANDQ         $-4, DX
	JE           LBB9_11
	XORL         R8, R8
	VBROADCASTSD dataPowF64<>+0(SB), Y0
	VMOVUPS      Y0, 512(SP)
	VBROADCASTSD dataPowF64<>+8(SB), Y0
	VMOVUPS      Y0, 1120(SP)
	VPXOR        X6, X6, X6
	VBROADCASTSD dataPowF64<>+16(SB), Y0
	VMOVUPS      Y0, 1088(SP)
	VBROADCASTSD dataPowF64<>+24(SB), Y0
	VMOVUPS      Y0, 1056(SP)
	VBROADCASTSD dataPowF64<>+32(SB), Y0
	VMOVUPS      Y0, 1024(SP)
	VBROADCASTSD dataPowF64<>+40(SB), Y0
	VMOVUPS      Y0, 992(SP)
	VBROADCASTSD dataPowF64<>+48(SB), Y0
	VMOVUPS      Y0, 960(SP)
	VBROADCASTSD dataPowF64<>+56(SB), Y0
	VMOVUPS      Y0, 928(SP)
	VBROADCASTSD dataPowF64<>+64(SB), Y0
	VMOVUPS      Y0, 896(SP)
	VBROADCASTSD dataPowF64<>+72(SB), Y0
	VMOVUPS      Y0, 864(SP)
	VBROADCASTSD dataPowF64<>+80(SB), Y0
	VMOVUPS      Y0, 832(SP)
	VBROADCASTSD dataPowF64<>+88(SB), Y0
	VMOVUPS      Y0, 800(SP)
	VBROADCASTSD dataPowF64<>+96(SB), Y0
	VMOVUPS      Y0, 768(SP)
	VBROADCASTSD dataPowF64<>+104(SB), Y0
	VMOVUPS      Y0, 736(SP)
	VBROADCASTSD dataPowF64<>+112(SB), Y0
	VMOVUPS      Y0, 704(SP)
	VBROADCASTSD dataPowF64<>+120(SB), Y0
	VMOVUPS      Y0, 672(SP)
	VBROADCASTSD dataPowF64<>+128(SB), Y0
	VMOVUPS      Y0, 640(SP)
	VBROADCASTSD dataPowF64<>+136(SB), Y0
	VMOVUPS      Y0, 608(SP)
	VBROADCASTSD dataPowF64<>+144(SB), Y0
	VMOVUPS      Y0, -128(SP)
	VBROADCASTSD dataPowF64<>+152(SB), Y0
	VMOVUPS      Y0, 576(SP)
	VBROADCASTSD dataPowF64<>+160(SB), Y0
	VMOVUPS      Y0, 544(SP)
	VBROADCASTSD dataPowF64<>+168(SB), Y0
	VMOVUPS      Y0, 480(SP)
	VBROADCASTSD dataPowF64<>+176(SB), Y0
	VMOVUPS      Y0, 448(SP)
	VBROADCASTSD dataPowF64<>+184(SB), Y0
	VMOVUPS      Y0, 416(SP)
	VBROADCASTSD dataPowF64<>+192(SB), Y0
	VMOVUPS      Y0, 384(SP)
	VBROADCASTSD dataPowF64<>+200(SB), Y0
	VMOVUPS      Y0, 352(SP)
	VBROADCASTSD dataPowF64<>+208(SB), Y0
	VMOVUPS      Y0, 320(SP)
	VBROADCASTSD dataPowF64<>+216(SB), Y0
	VMOVUPS      Y0, 288(SP)
	VBROADCASTSD dataPowF64<>+224(SB), Y0
	VMOVUPS      Y0, 256(SP)
	VBROADCASTSD dataPowF64<>+232(SB), Y0
	VMOVUPS      Y0, 224(SP)
	VBROADCASTSD dataPowF64<>+240(SB), Y0
	VMOVUPS      Y0, 192(SP)
	VBROADCASTSD dataPowF64<>+248(SB), Y0
	VMOVUPS      Y0, 160(SP)
	VBROADCASTSD dataPowF64<>+256(SB), Y0
	VMOVUPS      Y0, 128(SP)
	VBROADCASTSD dataPowF64<>+264(SB), Y0
	VMOVUPS      Y0, 96(SP)
	VBROADCASTSD dataPowF64<>+272(SB), Y0
	VMOVUPS      Y0, 64(SP)
	VBROADCASTSD dataPowF64<>+280(SB), Y0
	VMOVUPS      Y0, 32(SP)
	VBROADCASTSD dataPowF64<>+288(SB), Y0
	VMOVUPS      Y0, (SP)
	VBROADCASTSD dataPowF64<>+296(SB), Y0
	VMOVUPS      Y0, -32(SP)
	VBROADCASTSD dataPowF64<>+304(SB), Y0
	VMOVUPS      Y0, -64(SP)
	VBROADCASTSD dataPowF64<>+312(SB), Y0
	VMOVUPD      Y0, -96(SP)
	VPBROADCASTQ dataPowF64<>+320(SB), Y5
	VBROADCASTSD dataPowF64<>+320(SB), Y10
	JMP          LBB9_2

LBB9_10:
	VMOVUPD Y2, (DI)(R8*8)
	ADDQ    $0x04, R8
	CMPQ    R8, DX
	JAE     LBB9_11

LBB9_2:
	VMOVAPD      Y10, Y9
	VMOVDQU      (DI)(R8*8), Y13
	VMOVUPD      (SI)(R8*8), Y12
	VPAND        512(SP), Y13, Y10
	VMOVUPD      dataPowF64<>+336(SB), X1
	VANDPD       (DI)(R8*8), X1, X2
	VMOVUPD      dataPowF64<>+352(SB), X0
	VORPD        X0, X2, X2
	VANDPD       16(DI)(R8*8), X1, X3
	VORPD        X0, X3, X3
	VINSERTF128  $0x01, X3, Y2, Y3
	VMOVUPD      1120(SP), Y0
	VCMPPD       $0x01, Y3, Y0, Y2
	VANDNPD      Y3, Y2, Y4
	VADDPD       1088(SP), Y3, Y3
	VADDPD       Y4, Y3, Y4
	VMULPD       Y4, Y4, Y3
	VMULPD       Y3, Y3, Y7
	VMOVUPD      1024(SP), Y8
	VFMADD213PD  1056(SP), Y4, Y8
	VFMADD231PD  992(SP), Y3, Y8
	VMOVUPD      928(SP), Y11
	VFMADD213PD  960(SP), Y4, Y11
	VMOVUPD      864(SP), Y14
	VFMADD213PD  896(SP), Y4, Y14
	VFMADD231PD  Y11, Y3, Y14
	VFMADD231PD  Y8, Y7, Y14
	VMULPD       Y4, Y3, Y8
	VMULPD       Y14, Y8, Y8
	VADDPD       832(SP), Y3, Y11
	VFMADD231PD  800(SP), Y4, Y11
	VMOVUPD      736(SP), Y14
	VFMADD213PD  768(SP), Y4, Y14
	VMOVUPD      672(SP), Y15
	VFMADD213PD  704(SP), Y4, Y15
	VFMADD231PD  Y14, Y3, Y15
	VFMADD231PD  Y11, Y7, Y15
	VDIVPD       Y15, Y8, Y7
	VMOVDQU      Y10, 1152(SP)
	VPSRLQ       $0x34, Y10, Y8
	VPOR         640(SP), Y8, Y8
	VADDPD       608(SP), Y8, Y8
	VMOVUPD      -128(SP), Y0
	VANDPD       Y0, Y2, Y2
	VADDPD       Y2, Y8, Y8
	VMULPD       Y12, Y8, Y2
	VROUNDPD     $0x08, Y2, Y2
	VFNMADD213PD Y2, Y12, Y8
	VMOVUPD      576(SP), Y1
	VMOVAPD      Y1, Y11
	VFMADD213PD  Y4, Y3, Y11
	VADDPD       Y7, Y11, Y11
	VMOVUPD      544(SP), Y10
	VMULPD       Y4, Y10, Y14
	VMULPD       Y1, Y3, Y15
	VFMADD231PD  Y14, Y4, Y15
	VSUBPD       Y4, Y11, Y4
	VFMADD231PD  Y3, Y10, Y4
	VMOVUPD      480(SP), Y1
	VMULPD       Y1, Y12, Y3
	VMULPD       Y3, Y11, Y3
	VROUNDPD     $0x08, Y3, Y3
	VMULPD       448(SP), Y3, Y14
	VFMADD231PD  Y11, Y12, Y14
	VFMSUB231PD  416(SP), Y3, Y14
	VMOVUPD      384(SP), Y11
	VFMADD231PD  Y8, Y11, Y14
	VSUBPD       Y7, Y15, Y7
	VADDPD       Y4, Y7, Y4
	VFNMSUB213PD Y14, Y12, Y4
	VMULPD       Y1, Y4, Y7
	VROUNDPD     $0x08, Y7, Y7
	VFNMADD231PD Y11, Y7, Y4
	VMULPD       Y4, Y4, Y8
	VMOVUPD      320(SP), Y11
	VFMADD213PD  352(SP), Y4, Y11
	VMOVUPD      256(SP), Y14
	VFMADD213PD  288(SP), Y4, Y14
	VMOVUPD      192(SP), Y15
	VFMADD213PD  224(SP), Y4, Y15
	VFMADD231PD  Y14, Y8, Y15
	VMOVUPD      128(SP), Y14
	VFMADD213PD  160(SP), Y4, Y14
	VMOVUPD      64(SP), Y1
	VFMADD213PD  96(SP), Y4, Y1
	VFMADD231PD  Y14, Y8, Y1
	VMOVUPD      32(SP), Y14
	VFMADD213PD  Y10, Y4, Y14
	VFMADD213PD  Y4, Y8, Y14
	VMULPD       Y8, Y8, Y4
	VFMADD231PD  Y11, Y4, Y15
	VFMADD231PD  Y1, Y4, Y14
	VMULPD       Y4, Y4, Y1
	VFMADD231PD  Y15, Y1, Y14
	VADDPD       Y0, Y14, Y1
	VADDPD       Y2, Y3, Y2
	VADDPD       Y7, Y2, Y15
	VROUNDPD     $0x08, Y15, Y2
	VCVTTSD2SIQ  X2, R9
	VPERMILPD    $0x01, X2, X3
	VCVTTSD2SIQ  X3, AX
	VEXTRACTF128 $0x01, Y2, X2
	VCVTTSD2SIQ  X2, CX
	VMOVQ        CX, X3
	VPERMILPD    $0x01, X2, X2
	VCVTTSD2SIQ  X2, CX
	VMOVQ        CX, X2
	VPUNPCKLQDQ  X2, X3, X2
	VMOVQ        R9, X3
	VMOVQ        AX, X4
	VPUNPCKLQDQ  X4, X3, X3
	VINSERTI128  $0x01, X2, Y3, Y2
	VPSRAD       $0x1f, Y1, Y3
	VPSRAD       $0x14, Y1, Y4
	VPSRLQ       $0x20, Y4, Y4
	VPBLENDD     $0xaa, Y3, Y4, Y3
	VPADDQ       Y3, Y2, Y4
	VPCMPGTQ     (SP), Y4, Y3
	VMOVUPD      -32(SP), Y0
	VCMPPD       $0x01, Y15, Y0, Y7
	VPOR         Y7, Y3, Y3
	VMOVDQU      -64(SP), Y0
	VPCMPGTQ     Y4, Y0, Y4
	VCMPPD       $0x01, -96(SP), Y15, Y7
	VPOR         Y7, Y4, Y4
	VPSLLQ       $0x34, Y2, Y2
	VPADDQ       Y1, Y2, Y2
	VPOR         Y3, Y4, Y1
	VPTEST       Y1, Y1
	JNE          LBB9_3
	VMOVAPD      Y9, Y10
	JMP          LBB9_5

LBB9_3:
	VPANDN    Y2, Y4, Y1
	VMOVAPD   Y9, Y10
	VBLENDVPD Y3, Y9, Y1, Y2

LBB9_5:
	VPAND     Y5, Y13, Y11
	VPCMPEQQ  Y6, Y11, Y4
	VPSRAD    $0x1f, Y13, Y1
	VPSHUFD   $0xf5, Y1, Y7
	VCMPPD    $0x01, Y6, Y12, Y14
	VCMPPD    $0x00, Y6, Y12, Y3
	VANDPD    -128(SP), Y3, Y1
	VBLENDVPD Y14, Y10, Y1, Y1
	VBLENDVPD Y4, Y1, Y2, Y2
	VPTEST    Y7, Y7
	JNE       LBB9_7
	VPXOR     X7, X7, X7
	JMP       LBB9_8

LBB9_7:
	VROUNDPD     $0x08, Y12, Y1
	VCMPPD       $0x00, Y1, Y12, Y8
	VCVTTSD2SIQ  X1, R9
	VPERMILPD    $0x01, X1, X10
	VCVTTSD2SIQ  X10, CX
	VEXTRACTF128 $0x01, Y1, X1
	VCVTTSD2SIQ  X1, AX
	VXORPD       X10, X10, X10
	VMOVQ        AX, X6
	VPERMILPD    $0x01, X1, X1
	VCVTTSD2SIQ  X1, AX
	VMOVQ        AX, X1
	VPUNPCKLQDQ  X1, X6, X1
	VMOVQ        R9, X6
	VMOVQ        CX, X0
	VPUNPCKLQDQ  X0, X6, X0
	VINSERTI128  $0x01, X1, Y0, Y0
	VPSLLQ       $0x3f, Y0, Y0
	VPOR         Y2, Y0, Y1
	VCMPPD       $0x00, Y10, Y13, Y6
	VBROADCASTSD dataPowF64<>+328(SB), Y10
	VBLENDVPD    Y6, Y2, Y10, Y6
	VMOVAPD      Y9, Y10
	VBLENDVPD    Y8, Y1, Y6, Y1
	VXORPD       X6, X6, X6
	VBLENDVPD    Y7, Y1, Y2, Y2
	VANDPD       Y0, Y8, Y7

LBB9_8:
	VPCMPEQD  Y9, Y9, Y9
	VANDPD    Y5, Y12, Y0
	VANDPD    Y5, Y15, Y1
	VPCMPEQQ  Y5, Y1, Y15
	VPXOR     Y9, Y15, Y1
	VPCMPEQQ  Y5, Y0, Y8
	VPCMPEQQ  Y5, Y11, Y11
	VPXOR     Y9, Y11, Y0
	VPANDN    Y0, Y8, Y0
	VPOR      Y4, Y1, Y1
	VPAND     Y0, Y1, Y0
	VPTEST    Y9, Y0
	JB        LBB9_10
	VPXOR     Y9, Y8, Y0
	VPANDN    Y0, Y15, Y0
	VMOVUPD   -128(SP), Y8
	VMOVUPD   1152(SP), Y9
	VCMPPD    $0x00, Y8, Y9, Y1
	VCMPPD    $0x01, Y9, Y8, Y4
	VPSRAD    $0x1f, Y12, Y6
	VPXOR     Y4, Y6, Y4
	VPXOR     X6, X6, X6
	VBLENDVPD Y4, Y10, Y6, Y4
	VBLENDVPD Y1, Y8, Y4, Y1
	VBLENDVPD Y0, Y2, Y1, Y0
	VANDPD    Y2, Y7, Y1
	VANDPD    Y7, Y13, Y2
	VORPD     Y2, Y9, Y2
	VBLENDVPD Y14, Y1, Y2, Y1
	VBLENDVPD Y3, Y8, Y1, Y1
	VBLENDVPD Y11, Y1, Y0, Y0
	VCMPPD    $0x03, Y13, Y13, Y1
	VCMPPD    $0x03, Y12, Y12, Y2
	VORPD     Y1, Y2, Y1
	VADDPD    Y13, Y12, Y2
	VBLENDVPD Y1, Y2, Y0, Y2
	JMP       LBB9_10

LBB9_11:
	ADDQ $+1192, SP
	VZEROUPPER
	RET

DATA genPowF32<>+0(SB)/4, $0x7fffffff
DATA genPowF32<>+4(SB)/4, $0x3f3504f3
DATA genPowF32<>+8(SB)/4, $0xbf800000
DATA genPowF32<>+12(SB)/4, $0x3def251a
DATA genPowF32<>+16(SB)/4, $0xbdebd1b8
DATA genPowF32<>+20(SB)/4, $0x3e11e9bf
DATA genPowF32<>+24(SB)/4, $0xbdfe5d4f
DATA genPowF32<>+28(SB)/4, $0x3e4cceac
DATA genPowF32<>+32(SB)/4, $0xbe2aae50
DATA genPowF32<>+36(SB)/4, $0x3eaaaaaa
DATA genPowF32<>+40(SB)/4, $0xbe7ffffc
DATA genPowF32<>+44(SB)/4, $0x3d9021bb
DATA genPowF32<>+48(SB)/4, $0xcb00007f
DATA genPowF32<>+52(SB)/4, $0x3f800000
DATA genPowF32<>+56(SB)/4, $0xbf000000
DATA genPowF32<>+60(SB)/4, $0x3f000000
DATA genPowF32<>+64(SB)/4, $0x3fb8aa3b
DATA genPowF32<>+68(SB)/4, $0xbf318000
DATA genPowF32<>+72(SB)/4, $0xb95e8083
DATA genPowF32<>+76(SB)/4, $0xbf317218
DATA genPowF32<>+80(SB)/4, $0x3d2aaaab
DATA genPowF32<>+84(SB)/4, $0x3c088889
DATA genPowF32<>+88(SB)/4, $0x3ab60b61
DATA genPowF32<>+92(SB)/4, $0x39500d01
DATA genPowF32<>+96(SB)/4, $0x3e2aaaab
DATA genPowF32<>+100(SB)/4, $0x000000fe
DATA genPowF32<>+104(SB)/4, $0x43960000
DATA genPowF32<>+108(SB)/4, $0x00000001
DATA genPowF32<>+112(SB)/4, $0xc3960000
DATA genPowF32<>+116(SB)/4, $0x7f800000
DATA genPowF32<>+120(SB)/4, $0x7fc00102
DATA genPowF32<>+124(SB)/8, $0x007fffff007fffff
DATA genPowF32<>+132(SB)/8, $0x007fffff007fffff
DATA genPowF32<>+140(SB)/8, $0x3f0000003f000000
DATA genPowF32<>+148(SB)/8, $0x3f0000003f000000
DATA genPowF32<>+156(SB)/8, $0x4b0000004b000000
DATA genPowF32<>+164(SB)/1, $0xff
DATA genPowF32<>+165(SB)/1, $0x00
DATA genPowF32<>+166(SB)/1, $0x00
DATA genPowF32<>+167(SB)/1, $0x00
DATA genPowF32<>+168(SB)/1, $0xff
DATA genPowF32<>+169(SB)/1, $0x00
DATA genPowF32<>+170(SB)/1, $0x00
DATA genPowF32<>+171(SB)/1, $0x00
DATA genPowF32<>+172(SB)/1, $0xff
DATA genPowF32<>+173(SB)/1, $0x00
DATA genPowF32<>+174(SB)/1, $0x00
DATA genPowF32<>+175(SB)/1, $0x00
DATA genPowF32<>+176(SB)/1, $0xff
DATA genPowF32<>+177(SB)/1, $0x00
DATA genPowF32<>+178(SB)/1, $0x00
DATA genPowF32<>+179(SB)/1, $0x00
DATA genPowF32<>+180(SB)/1, $0xff
DATA genPowF32<>+181(SB)/1, $0x00
DATA genPowF32<>+182(SB)/1, $0x00
DATA genPowF32<>+183(SB)/1, $0x00
DATA genPowF32<>+184(SB)/1, $0xff
DATA genPowF32<>+185(SB)/1, $0x00
DATA genPowF32<>+186(SB)/1, $0x00
DATA genPowF32<>+187(SB)/1, $0x00
DATA genPowF32<>+188(SB)/1, $0xff
DATA genPowF32<>+189(SB)/1, $0x00
DATA genPowF32<>+190(SB)/1, $0x00
DATA genPowF32<>+191(SB)/1, $0x00
DATA genPowF32<>+192(SB)/1, $0xff
DATA genPowF32<>+193(SB)/1, $0x00
DATA genPowF32<>+194(SB)/1, $0x00
DATA genPowF32<>+195(SB)/1, $0x00
GLOBL genPowF32<>(SB), RODATA|NOPTR, $196

// func Pow_8x_AVX2_F32(x []float32, y []float32)
// Requires: AVX, AVX2, FMA3
TEXT ·Pow_8x_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ         x_base+0(FP), DI
	MOVQ         y_base+24(FP), SI
	MOVQ         x_len+8(FP), DX
	SUBQ         $+872, SP
	ANDQ         $-8, DX
	JE           LBB8_12
	XORL         AX, AX
	VBROADCASTSS genPowF32<>+0(SB), Y0
	VMOVUPS      Y0, 320(SP)
	VBROADCASTSS genPowF32<>+4(SB), Y0
	VMOVUPS      Y0, 800(SP)
	VPXOR        X7, X7, X7
	VBROADCASTSS genPowF32<>+8(SB), Y0
	VMOVUPS      Y0, 768(SP)
	VBROADCASTSS genPowF32<>+12(SB), Y0
	VMOVUPS      Y0, 736(SP)
	VBROADCASTSS genPowF32<>+16(SB), Y0
	VMOVUPS      Y0, 704(SP)
	VBROADCASTSS genPowF32<>+20(SB), Y0
	VMOVUPS      Y0, 672(SP)
	VBROADCASTSS genPowF32<>+24(SB), Y0
	VMOVUPS      Y0, 640(SP)
	VBROADCASTSS genPowF32<>+28(SB), Y0
	VMOVUPS      Y0, 608(SP)
	VBROADCASTSS genPowF32<>+32(SB), Y0
	VMOVUPS      Y0, 576(SP)
	VBROADCASTSS genPowF32<>+36(SB), Y0
	VMOVUPS      Y0, 544(SP)
	VBROADCASTSS genPowF32<>+40(SB), Y0
	VMOVUPS      Y0, 512(SP)
	VBROADCASTSS genPowF32<>+44(SB), Y0
	VMOVUPS      Y0, 480(SP)
	VBROADCASTSD genPowF32<>+156(SB), Y0
	VMOVUPS      Y0, 448(SP)
	VBROADCASTSS genPowF32<>+48(SB), Y0
	VMOVUPS      Y0, 416(SP)
	VBROADCASTSS genPowF32<>+52(SB), Y0
	VMOVUPS      Y0, -128(SP)
	VBROADCASTSS genPowF32<>+56(SB), Y0
	VMOVUPS      Y0, 384(SP)
	VBROADCASTSS genPowF32<>+60(SB), Y0
	VMOVUPS      Y0, 352(SP)
	VBROADCASTSS genPowF32<>+64(SB), Y0
	VMOVUPS      Y0, 288(SP)
	VBROADCASTSS genPowF32<>+68(SB), Y0
	VMOVUPS      Y0, 256(SP)
	VBROADCASTSS genPowF32<>+72(SB), Y0
	VMOVUPS      Y0, 224(SP)
	VBROADCASTSS genPowF32<>+76(SB), Y0
	VMOVUPS      Y0, 192(SP)
	VBROADCASTSS genPowF32<>+80(SB), Y0
	VMOVUPS      Y0, 160(SP)
	VBROADCASTSS genPowF32<>+84(SB), Y0
	VMOVUPS      Y0, 128(SP)
	VBROADCASTSS genPowF32<>+88(SB), Y0
	VMOVUPS      Y0, 96(SP)
	VBROADCASTSS genPowF32<>+92(SB), Y0
	VMOVUPS      Y0, 64(SP)
	VBROADCASTSS genPowF32<>+96(SB), Y0
	VMOVUPS      Y0, 32(SP)
	VBROADCASTSS genPowF32<>+100(SB), Y0
	VMOVUPS      Y0, (SP)
	VBROADCASTSS genPowF32<>+104(SB), Y0
	VMOVUPS      Y0, -32(SP)
	VBROADCASTSS genPowF32<>+108(SB), Y0
	VMOVUPS      Y0, -64(SP)
	VPBROADCASTD genPowF32<>+112(SB), Y0
	VMOVDQU      Y0, -96(SP)
	VPBROADCASTD genPowF32<>+116(SB), Y8
	VBROADCASTSS genPowF32<>+116(SB), Y12
	JMP          LBB8_2

LBB8_10:
	VPXOR     Y0, Y15, Y0
	VPANDN    Y0, Y14, Y0
	VMOVUPS   -128(SP), Y14
	VMOVUPS   832(SP), Y2
	VCMPPS    $0x00, Y2, Y14, Y3
	VCMPPS    $0x01, Y2, Y14, Y4
	VXORPS    Y4, Y11, Y4
	VPXOR     X7, X7, X7
	VBLENDVPS Y4, Y12, Y7, Y4
	VBLENDVPS Y3, Y14, Y4, Y3
	VBLENDVPS Y0, Y6, Y3, Y0
	VANDPS    Y6, Y10, Y3
	VANDPS    Y9, Y10, Y4
	VORPS     Y2, Y4, Y4
	VBLENDVPS Y13, Y3, Y4, Y3
	VBLENDVPS Y1, Y14, Y3, Y1
	VBLENDVPS Y5, Y0, Y1, Y0
	VCMPPS    $0x03, Y9, Y9, Y1
	VCMPPS    $0x03, Y11, Y11, Y3
	VORPS     Y1, Y3, Y1
	VADDPS    Y9, Y11, Y3
	VBLENDVPS Y1, Y3, Y0, Y6
	VMOVUPS   Y6, (DI)(AX*4)
	ADDQ      $0x08, AX
	CMPQ      AX, DX
	JAE       LBB8_12

LBB8_2:
	VMOVAPS      Y12, Y2
	VMOVDQU      (DI)(AX*4), Y9
	VMOVUPS      (SI)(AX*4), Y11
	VPAND        320(SP), Y9, Y12
	VMOVUPS      genPowF32<>+124(SB), X1
	VANDPS       (DI)(AX*4), X1, X0
	VMOVUPS      genPowF32<>+140(SB), X3
	VORPS        X3, X0, X0
	VANDPS       16(DI)(AX*4), X1, X1
	VORPS        X3, X1, X1
	VINSERTF128  $0x01, X1, Y0, Y0
	VMOVUPS      800(SP), Y1
	VCMPPS       $0x01, Y0, Y1, Y1
	VANDNPS      Y0, Y1, Y4
	VADDPS       768(SP), Y0, Y0
	VADDPS       Y4, Y0, Y4
	VMULPS       Y4, Y4, Y6
	VMULPS       Y6, Y6, Y0
	VMOVUPS      704(SP), Y5
	VFMADD213PS  736(SP), Y4, Y5
	VMOVUPS      640(SP), Y10
	VFMADD213PS  672(SP), Y4, Y10
	VFMADD231PS  Y5, Y6, Y10
	VMOVUPS      576(SP), Y5
	VFMADD213PS  608(SP), Y4, Y5
	VMOVUPS      512(SP), Y13
	VFMADD213PS  544(SP), Y4, Y13
	VMULPS       Y0, Y0, Y14
	VFMADD132PS  480(SP), Y13, Y14
	VFMADD231PS  Y5, Y6, Y14
	VFMADD231PS  Y10, Y0, Y14
	VMULPS       Y4, Y6, Y0
	VMULPS       Y0, Y14, Y0
	VMOVDQU      Y12, 832(SP)
	VPSRLD       $0x17, Y12, Y5
	VPOR         448(SP), Y5, Y5
	VADDPS       416(SP), Y5, Y5
	VMOVUPS      -128(SP), Y3
	VANDPS       Y3, Y1, Y1
	VADDPS       Y1, Y5, Y5
	VMULPS       Y5, Y11, Y1
	VROUNDPS     $0x08, Y1, Y1
	VFNMADD213PS Y1, Y11, Y5
	VMOVUPS      384(SP), Y14
	VMOVAPS      Y14, Y10
	VFMADD213PS  Y4, Y6, Y10
	VADDPS       Y0, Y10, Y10
	VMOVUPS      352(SP), Y12
	VMULPS       Y4, Y12, Y13
	VMULPS       Y6, Y14, Y14
	VFMADD231PS  Y13, Y4, Y14
	VSUBPS       Y4, Y10, Y4
	VFMADD231PS  Y6, Y12, Y4
	VMOVUPS      288(SP), Y15
	VMULPS       Y15, Y11, Y6
	VMULPS       Y6, Y10, Y6
	VROUNDPS     $0x08, Y6, Y6
	VMULPS       256(SP), Y6, Y13
	VFMADD231PS  Y10, Y11, Y13
	VFNMADD231PS 224(SP), Y6, Y13
	VSUBPS       Y0, Y14, Y0
	VADDPS       Y4, Y0, Y0
	VMOVUPS      192(SP), Y10
	VMULPS       Y5, Y10, Y4
	VFNMADD231PS Y0, Y11, Y4
	VADDPS       Y4, Y13, Y0
	VMULPS       Y0, Y15, Y4
	VROUNDPS     $0x08, Y4, Y4
	VFMADD231PS  Y10, Y4, Y0
	VMULPS       Y0, Y0, Y5
	VMULPS       Y5, Y5, Y10
	VMOVUPS      64(SP), Y13
	VFMADD213PS  96(SP), Y0, Y13
	VMOVUPS      32(SP), Y14
	VFMADD213PS  Y12, Y0, Y14
	VFMADD231PS  Y13, Y10, Y14
	VMOVUPS      128(SP), Y10
	VFMADD213PS  160(SP), Y0, Y10
	VFMADD231PS  Y10, Y5, Y14
	VADDPS       Y3, Y0, Y10
	VFMADD231PS  Y14, Y5, Y10
	VADDPS       Y1, Y6, Y0
	VADDPS       Y4, Y0, Y14
	VCVTPS2DQ    Y14, Y4
	VPSRLD       $0x17, Y10, Y0
	VPAND        genPowF32<>+164(SB), Y0, Y0
	VPADDD       Y4, Y0, Y0
	VPCMPGTD     (SP), Y0, Y1
	VMOVUPS      -32(SP), Y3
	VCMPPS       $0x01, Y14, Y3, Y5
	VPOR         Y5, Y1, Y1
	VMOVDQU      -64(SP), Y3
	VPCMPGTD     Y0, Y3, Y0
	VCMPPS       $0x01, -96(SP), Y14, Y5
	VPOR         Y5, Y0, Y0
	VPSLLD       $0x17, Y4, Y4
	VPADDD       Y4, Y10, Y6
	VPOR         Y1, Y0, Y4
	VTESTPS      Y4, Y4
	JNE          LBB8_3
	VPCMPEQD     Y15, Y15, Y15
	VMOVAPS      Y2, Y12
	JMP          LBB8_5

LBB8_3:
	VPANDN    Y6, Y0, Y0
	VMOVAPS   Y2, Y12
	VBLENDVPS Y1, Y2, Y0, Y6
	VPCMPEQD  Y15, Y15, Y15

LBB8_5:
	VPAND     Y8, Y9, Y5
	VPCMPEQD  Y7, Y5, Y4
	VCMPPS    $0x01, Y7, Y11, Y13
	VCMPPS    $0x00, Y7, Y11, Y1
	VANDPS    -128(SP), Y1, Y0
	VBLENDVPS Y13, Y12, Y0, Y0
	VBLENDVPS Y4, Y0, Y6, Y6
	VMOVMSKPS Y9, CX
	TESTL     CX, CX
	JNE       LBB8_7
	VXORPS    X10, X10, X10
	JMP       LBB8_8

LBB8_7:
	VROUNDPS     $0x08, Y11, Y0
	VCMPPS       $0x00, Y0, Y11, Y0
	VCVTPS2DQ    Y11, Y10
	VPSLLD       $0x1f, Y10, Y10
	VPOR         Y6, Y10, Y12
	VPXOR        X3, X3, X3
	VCMPPS       $0x00, Y3, Y9, Y7
	VBROADCASTSS genPowF32<>+120(SB), Y3
	VBLENDVPS    Y7, Y6, Y3, Y3
	VBLENDVPS    Y0, Y12, Y3, Y3
	VMOVAPS      Y2, Y12
	VPSRAD       $0x1f, Y9, Y7
	VBLENDVPS    Y7, Y3, Y6, Y6
	VANDPS       Y0, Y10, Y10

LBB8_8:
	VPCMPEQD Y5, Y8, Y0
	VPXOR    Y0, Y15, Y5
	VANDPS   Y8, Y11, Y0
	VANDPS   Y8, Y14, Y3
	VPCMPEQD Y3, Y8, Y14
	VPXOR    Y15, Y14, Y3
	VPCMPEQD Y0, Y8, Y0
	VPANDN   Y5, Y0, Y7
	VPOR     Y4, Y3, Y3
	VPAND    Y7, Y3, Y3
	VTESTPS  Y15, Y3
	JAE      LBB8_10
	VPXOR    X7, X7, X7
	VMOVUPS  Y6, (DI)(AX*4)
	ADDQ     $0x08, AX
	CMPQ     AX, DX
	JB       LBB8_2

LBB8_12:
	ADDQ $+872, SP
	VZEROUPPER
	RET

DATA dataSinF32<>+0(SB)/4, $0x7fffffff
DATA dataSinF32<>+4(SB)/4, $0x3fa2f983
DATA dataSinF32<>+8(SB)/4, $0xfffffffe
DATA dataSinF32<>+12(SB)/4, $0x00000002
DATA dataSinF32<>+16(SB)/4, $0xbf490fdb
DATA dataSinF32<>+20(SB)/4, $0x80000000
DATA dataSinF32<>+24(SB)/4, $0x37ccf5ce
DATA dataSinF32<>+28(SB)/4, $0xbab6061a
DATA dataSinF32<>+32(SB)/4, $0x3d2aaaa5
DATA dataSinF32<>+36(SB)/4, $0xbf000000
DATA dataSinF32<>+40(SB)/4, $0x3f800000
DATA dataSinF32<>+44(SB)/4, $0xb94ca1f9
DATA dataSinF32<>+48(SB)/4, $0x3c08839e
DATA dataSinF32<>+52(SB)/4, $0xbe2aaaa3
DATA dataSinF32<>+56(SB)/4, $0x4b7fffff
DATA dataSinF32<>+60(SB)/8, $0xffffffffffffffff
DATA dataSinF32<>+68(SB)/8, $0xffffffffffffffff
DATA dataSinF32<>+76(SB)/8, $0xffffffffffffffff
DATA dataSinF32<>+84(SB)/8, $0xffffffffffffffff
DATA dataSinF32<>+92(SB)/8, $0x0000000000000000
DATA dataSinF32<>+100(SB)/8, $0x0000000000000000
DATA dataSinF32<>+108(SB)/8, $0x0000000000000000
DATA dataSinF32<>+116(SB)/8, $0x0000000000000000
GLOBL dataSinF32<>(SB), RODATA|NOPTR, $124

// func Sin_AVX2_F32(x []float32)
// Requires: AVX, AVX2, CMOV, FMA3
TEXT ·Sin_AVX2_F32(SB), $0-24
	MOVQ         x_base+0(FP), DI
	MOVQ         x_len+8(FP), SI
	PUSHQ        AX
	MOVQ         SI, AX
	ANDQ         $-8, AX
	JE           LBB12_3
	XORL         CX, CX
	VBROADCASTSS dataSinF32<>+0(SB), Y0
	VMOVUPS      Y0, -32(SP)
	VBROADCASTSS dataSinF32<>+4(SB), Y0
	VMOVUPS      Y0, -64(SP)
	VBROADCASTSS dataSinF32<>+8(SB), Y0
	VMOVUPS      Y0, -96(SP)
	VPBROADCASTD dataSinF32<>+12(SB), Y4
	VPBROADCASTD dataSinF32<>+16(SB), Y0
	VMOVDQU      Y0, -128(SP)
	VPBROADCASTD dataSinF32<>+20(SB), Y7
	VBROADCASTSS dataSinF32<>+24(SB), Y8
	VBROADCASTSS dataSinF32<>+28(SB), Y9
	VBROADCASTSS dataSinF32<>+32(SB), Y10
	VBROADCASTSS dataSinF32<>+36(SB), Y11
	VBROADCASTSS dataSinF32<>+40(SB), Y12
	VBROADCASTSS dataSinF32<>+44(SB), Y3
	VBROADCASTSS dataSinF32<>+48(SB), Y14
	VBROADCASTSS dataSinF32<>+52(SB), Y15

LBB12_2:
	VMOVUPS     (DI)(CX*4), Y2
	VANDPS      -32(SP), Y2, Y5
	VMULPS      -64(SP), Y5, Y0
	VCVTTPS2DQ  Y0, Y0
	VPSUBD      dataSinF32<>+60(SB), Y0, Y0
	VPAND       -96(SP), Y0, Y1
	VCVTDQ2PS   Y1, Y1
	VFMADD132PS -128(SP), Y5, Y1
	VMULPS      Y1, Y1, Y5
	VMOVAPS     Y3, Y13
	VFMADD213PS Y14, Y5, Y13
	VFMADD213PS Y15, Y5, Y13
	VMULPS      Y1, Y5, Y6
	VFMADD213PS Y1, Y13, Y6
	VPSLLD      $0x1d, Y0, Y1
	VPAND       Y4, Y0, Y0
	VPXOR       Y2, Y1, Y1
	VMOVAPS     Y8, Y2
	VFMADD213PS Y9, Y5, Y2
	VFMADD213PS Y10, Y5, Y2
	VFMADD213PS Y11, Y5, Y2
	VFMADD213PS Y12, Y5, Y2
	VPCMPEQD    Y4, Y0, Y5
	VANDPS      Y5, Y2, Y2
	VPCMPEQD    dataSinF32<>+92(SB), Y0, Y0
	VANDPS      Y0, Y6, Y0
	VADDPS      Y2, Y0, Y0
	VPAND       Y7, Y1, Y1
	VPXOR       Y0, Y1, Y0
	VMOVDQU     Y0, (DI)(CX*4)
	ADDQ        $0x08, CX
	CMPQ        CX, AX
	JB          LBB12_2

LBB12_3:
	CMPQ         AX, SI
	JAE          LBB12_14
	VBROADCASTSS dataSinF32<>+20(SB), X0
	VPXOR        X1, X1, X1
	VMOVSS       dataSinF32<>+56(SB), X2
	VMOVSS       dataSinF32<>+40(SB), X9
	VMOVSS       dataSinF32<>+16(SB), X10
	VMOVSS       dataSinF32<>+24(SB), X12
	VMOVSS       dataSinF32<>+28(SB), X11
	VMOVSS       dataSinF32<>+32(SB), X13
	VMOVSS       dataSinF32<>+36(SB), X14
	VMOVSS       dataSinF32<>+44(SB), X8
	VMOVSS       dataSinF32<>+48(SB), X15
	VMOVSS       dataSinF32<>+52(SB), X6
	JMP          LBB12_5

LBB12_13:
	ADDQ $0x01, AX
	CMPQ AX, SI
	JAE  LBB12_14

LBB12_5:
	VMOVSS     (DI)(AX*4), X4
	VXORPS     X0, X4, X3
	VCMPSS     $0x01, X1, X4, X5
	VBLENDVPS  X5, X3, X4, X3
	VUCOMISS   X2, X3
	JA         LBB12_13
	VUCOMISS   X1, X4
	SETCS      R8
	VMULSS     dataSinF32<>+4(SB), X3, X4
	VCVTTSS2SI X4, DX
	VROUNDSS   $0x0b, X4, X4, X4
	MOVL       DX, CX
	ANDL       $0x01, CX
	JE         LBB12_8
	VADDSS     X4, X9, X4

LBB12_8:
	ADDL        DX, CX
	ANDL        $0x07, CX
	LEAL        -4(CX), DX
	CMPL        CX, $0x04
	SETCC       R9
	CMOVLLT     CX, DX
	VFMADD231SS X10, X4, X3
	VMULSS      X3, X3, X4
	VMOVAPS     X12, X7
	VFMADD213SS X11, X4, X7
	VFMADD213SS X13, X4, X7
	VFMADD213SS X14, X4, X7
	VMOVAPS     X8, X5
	VFMADD213SS X15, X4, X5
	VFMADD213SS X6, X4, X5
	ADDL        $-1, DX
	CMPL        DX, $0x02
	JB          LBB12_9
	VMULSS      X3, X4, X4
	VFMADD213SS X3, X4, X5
	VMOVAPS     X5, X4
	VMOVSS      X4, (DI)(AX*4)
	CMPB        R8, R9
	JE          LBB12_13
	JMP         LBB12_12

LBB12_9:
	VFMADD213SS X9, X7, X4
	VMOVSS      X4, (DI)(AX*4)
	CMPB        R8, R9
	JE          LBB12_13

LBB12_12:
	VXORPS X0, X4, X3
	VMOVSS X3, (DI)(AX*4)
	JMP    LBB12_13

LBB12_14:
	POPQ AX
	VZEROUPPER
	RET

DATA dataCosF32<>+0(SB)/4, $0x7fffffff
DATA dataCosF32<>+4(SB)/4, $0x3fa2f983
DATA dataCosF32<>+8(SB)/4, $0xfffffffe
DATA dataCosF32<>+12(SB)/4, $0x00000002
DATA dataCosF32<>+16(SB)/4, $0xbf490fdb
DATA dataCosF32<>+20(SB)/4, $0xc0000000
DATA dataCosF32<>+24(SB)/4, $0x37ccf5ce
DATA dataCosF32<>+28(SB)/4, $0xbab6061a
DATA dataCosF32<>+32(SB)/4, $0x3d2aaaa5
DATA dataCosF32<>+36(SB)/4, $0xbf000000
DATA dataCosF32<>+40(SB)/4, $0x3f800000
DATA dataCosF32<>+44(SB)/4, $0xb94ca1f9
DATA dataCosF32<>+48(SB)/4, $0x3c08839e
DATA dataCosF32<>+52(SB)/4, $0xbe2aaaa3
DATA dataCosF32<>+56(SB)/4, $0x80000000
DATA dataCosF32<>+60(SB)/4, $0x4b7fffff
DATA dataCosF32<>+64(SB)/8, $0xffffffffffffffff
DATA dataCosF32<>+72(SB)/8, $0xffffffffffffffff
DATA dataCosF32<>+80(SB)/8, $0xffffffffffffffff
DATA dataCosF32<>+88(SB)/8, $0xffffffffffffffff
DATA dataCosF32<>+96(SB)/8, $0x0000000000000000
DATA dataCosF32<>+104(SB)/8, $0x0000000000000000
DATA dataCosF32<>+112(SB)/8, $0x0000000000000000
DATA dataCosF32<>+120(SB)/8, $0x0000000000000000
GLOBL dataCosF32<>(SB), RODATA|NOPTR, $128

// func Cos_AVX2_F32(x []float32)
// Requires: AVX, AVX2, CMOV, FMA3
TEXT ·Cos_AVX2_F32(SB), NOSPLIT, $0-24
	MOVQ         x_base+0(FP), DI
	MOVQ         x_len+8(FP), SI
	SUBQ         $0x48, SP
	MOVQ         SI, AX
	ANDQ         $-8, AX
	JE           LBB13_3
	XORL         CX, CX
	VBROADCASTSS dataCosF32<>+0(SB), Y0
	VMOVUPS      Y0, 32(SP)
	VBROADCASTSS dataCosF32<>+4(SB), Y0
	VMOVUPS      Y0, (SP)
	VBROADCASTSS dataCosF32<>+8(SB), Y0
	VMOVUPS      Y0, -32(SP)
	VPBROADCASTD dataCosF32<>+12(SB), Y4
	VBROADCASTSS dataCosF32<>+16(SB), Y0
	VMOVUPS      Y0, -64(SP)
	VBROADCASTSS dataCosF32<>+20(SB), Y0
	VMOVUPS      Y0, -96(SP)
	VBROADCASTSS dataCosF32<>+24(SB), Y0
	VMOVUPS      Y0, -128(SP)
	VBROADCASTSS dataCosF32<>+28(SB), Y9
	VBROADCASTSS dataCosF32<>+32(SB), Y10
	VBROADCASTSS dataCosF32<>+36(SB), Y6
	VBROADCASTSS dataCosF32<>+40(SB), Y12
	VBROADCASTSS dataCosF32<>+44(SB), Y13
	VBROADCASTSS dataCosF32<>+48(SB), Y14
	VBROADCASTSS dataCosF32<>+52(SB), Y15
	VPBROADCASTD dataCosF32<>+56(SB), Y2

LBB13_2:
	VMOVUPS     32(SP), Y0
	VANDPS      (DI)(CX*4), Y0, Y5
	VMULPS      (SP), Y5, Y0
	VCVTTPS2DQ  Y0, Y0
	VPSUBD      dataCosF32<>+64(SB), Y0, Y0
	VPAND       -32(SP), Y0, Y1
	VCVTDQ2PS   Y1, Y3
	VFMADD132PS -64(SP), Y5, Y3
	VMULPS      Y3, Y3, Y5
	VMOVUPS     -128(SP), Y8
	VFMADD213PS Y9, Y5, Y8
	VFMADD213PS Y10, Y5, Y8
	VMULPS      Y5, Y5, Y7
	VMOVAPS     Y6, Y11
	VFMADD213PS Y12, Y5, Y11
	VFMADD231PS Y7, Y8, Y11
	VMOVAPS     Y13, Y7
	VFMADD213PS Y14, Y5, Y7
	VFMADD213PS Y15, Y5, Y7
	VMULPS      Y3, Y5, Y5
	VFMADD213PS Y3, Y7, Y5
	VPAND       Y4, Y0, Y0
	VPCMPEQD    Y4, Y0, Y3
	VPCMPEQD    dataCosF32<>+96(SB), Y0, Y0
	VANDPS      Y0, Y5, Y0
	VANDPS      Y3, Y11, Y3
	VADDPS      Y3, Y0, Y0
	VADDPS      Y5, Y11, Y3
	VSUBPS      Y0, Y3, Y0
	VPSLLD      $0x1d, Y1, Y1
	VPADDD      -96(SP), Y1, Y1
	VPAND       Y2, Y1, Y1
	VPXOR       Y2, Y1, Y1
	VXORPS      Y1, Y0, Y0
	VMOVUPS     Y0, (DI)(CX*4)
	ADDQ        $0x08, CX
	CMPQ        CX, AX
	JB          LBB13_2

LBB13_3:
	CMPQ         AX, SI
	JAE          LBB13_14
	VBROADCASTSS dataCosF32<>+56(SB), X0
	VXORPS       X1, X1, X1
	VMOVSS       dataCosF32<>+60(SB), X2
	VMOVSS       dataCosF32<>+40(SB), X9
	VMOVSS       dataCosF32<>+16(SB), X10
	VMOVSS       dataCosF32<>+24(SB), X8
	VMOVSS       dataCosF32<>+28(SB), X11
	VMOVSS       dataCosF32<>+32(SB), X13
	VMOVSS       dataCosF32<>+36(SB), X14
	VMOVSS       dataCosF32<>+44(SB), X7
	VMOVSS       dataCosF32<>+48(SB), X15
	VMOVSS       dataCosF32<>+52(SB), X6
	JMP          LBB13_5

LBB13_13:
	ADDQ $0x01, AX
	CMPQ AX, SI
	JAE  LBB13_14

LBB13_5:
	VMOVSS     (DI)(AX*4), X3
	VXORPS     X0, X3, X4
	VCMPSS     $0x01, X1, X3, X5
	VBLENDVPS  X5, X4, X3, X3
	VUCOMISS   X2, X3
	JA         LBB13_13
	VMULSS     dataCosF32<>+4(SB), X3, X4
	VCVTTSS2SI X4, DX
	VROUNDSS   $0x0b, X4, X4, X4
	MOVL       DX, CX
	ANDL       $0x01, CX
	JE         LBB13_8
	VADDSS     X4, X9, X4

LBB13_8:
	ADDL        DX, CX
	ANDL        $0x07, CX
	LEAL        -4(CX), DX
	CMPL        CX, $0x04
	CMOVLLT     CX, DX
	SETCC       R8
	CMPL        DX, $0x02
	SETCC       CL
	VFMADD231SS X10, X4, X3
	VMULSS      X3, X3, X4
	VMOVAPS     X8, X12
	VFMADD213SS X11, X4, X12
	VFMADD213SS X13, X4, X12
	VFMADD213SS X14, X4, X12
	VMOVAPS     X7, X5
	VFMADD213SS X15, X4, X5
	VFMADD213SS X6, X4, X5
	ADDL        $-1, DX
	CMPL        DX, $0x02
	JB          LBB13_9
	VFMADD213SS X9, X12, X4
	VMOVAPS     X4, X5
	VMOVSS      X5, (DI)(AX*4)
	CMPB        R8, CL
	JE          LBB13_13
	JMP         LBB13_12

LBB13_9:
	VMULSS      X3, X4, X4
	VFMADD213SS X3, X4, X5
	VMOVSS      X5, (DI)(AX*4)
	CMPB        R8, CL
	JE          LBB13_13

LBB13_12:
	VXORPS X0, X5, X3
	VMOVSS X3, (DI)(AX*4)
	JMP    LBB13_13

LBB13_14:
	ADDQ $0x48, SP
	VZEROUPPER
	RET

DATA dataSinCosF32<>+0(SB)/4, $0x7fffffff
DATA dataSinCosF32<>+4(SB)/4, $0x3fa2f983
DATA dataSinCosF32<>+8(SB)/4, $0xfffffffe
DATA dataSinCosF32<>+12(SB)/4, $0x00000002
DATA dataSinCosF32<>+16(SB)/4, $0xbf490fdb
DATA dataSinCosF32<>+20(SB)/4, $0xc0000000
DATA dataSinCosF32<>+24(SB)/4, $0x80000000
DATA dataSinCosF32<>+28(SB)/4, $0x37ccf5ce
DATA dataSinCosF32<>+32(SB)/4, $0xbab6061a
DATA dataSinCosF32<>+36(SB)/4, $0x3d2aaaa5
DATA dataSinCosF32<>+40(SB)/4, $0xbf000000
DATA dataSinCosF32<>+44(SB)/4, $0x3f800000
DATA dataSinCosF32<>+48(SB)/4, $0xb94ca1f9
DATA dataSinCosF32<>+52(SB)/4, $0x3c08839e
DATA dataSinCosF32<>+56(SB)/4, $0xbe2aaaa3
DATA dataSinCosF32<>+60(SB)/4, $0x4b7fffff
DATA dataSinCosF32<>+64(SB)/8, $0xffffffffffffffff
DATA dataSinCosF32<>+72(SB)/8, $0xffffffffffffffff
DATA dataSinCosF32<>+80(SB)/8, $0xffffffffffffffff
DATA dataSinCosF32<>+88(SB)/8, $0xffffffffffffffff
DATA dataSinCosF32<>+96(SB)/8, $0x0000000000000000
DATA dataSinCosF32<>+104(SB)/8, $0x0000000000000000
DATA dataSinCosF32<>+112(SB)/8, $0x0000000000000000
DATA dataSinCosF32<>+120(SB)/8, $0x0000000000000000
GLOBL dataSinCosF32<>(SB), RODATA|NOPTR, $128

// func SinCos_AVX2_F32(x []float32, y []float32, z []float32)
// Requires: AVX, AVX2, CMOV, FMA3
TEXT ·SinCos_AVX2_F32(SB), $0-72
	MOVQ         x_base+0(FP), DI
	MOVQ         y_base+24(FP), SI
	MOVQ         z_base+48(FP), DX
	MOVQ         x_len+8(FP), CX
	PUSHQ        BX
	SUBQ         $0x60, SP
	MOVQ         CX, R8
	ANDQ         $-8, R8
	JE           LBB14_3
	XORL         AX, AX
	VBROADCASTSS dataSinCosF32<>+0(SB), Y0
	VMOVUPS      Y0, 64(SP)
	VBROADCASTSS dataSinCosF32<>+4(SB), Y0
	VMOVUPS      Y0, 32(SP)
	VBROADCASTSS dataSinCosF32<>+8(SB), Y0
	VMOVUPS      Y0, (SP)
	VPBROADCASTD dataSinCosF32<>+12(SB), Y4
	VBROADCASTSS dataSinCosF32<>+16(SB), Y0
	VMOVUPS      Y0, -32(SP)
	VBROADCASTSS dataSinCosF32<>+20(SB), Y0
	VMOVUPS      Y0, -64(SP)
	VPBROADCASTD dataSinCosF32<>+24(SB), Y8
	VBROADCASTSS dataSinCosF32<>+28(SB), Y0
	VMOVUPS      Y0, -96(SP)
	VBROADCASTSS dataSinCosF32<>+32(SB), Y0
	VMOVUPS      Y0, -128(SP)
	VBROADCASTSS dataSinCosF32<>+36(SB), Y11
	VBROADCASTSS dataSinCosF32<>+40(SB), Y10
	VBROADCASTSS dataSinCosF32<>+44(SB), Y13
	VBROADCASTSS dataSinCosF32<>+48(SB), Y14
	VBROADCASTSS dataSinCosF32<>+52(SB), Y15
	VBROADCASTSS dataSinCosF32<>+56(SB), Y2

LBB14_2:
	VMOVUPS     (DX)(AX*4), Y5
	VANDPS      64(SP), Y5, Y1
	VMULPS      32(SP), Y1, Y0
	VCVTTPS2DQ  Y0, Y0
	VPSUBD      dataSinCosF32<>+64(SB), Y0, Y3
	VPAND       (SP), Y3, Y0
	VCVTDQ2PS   Y0, Y6
	VFMADD132PS -32(SP), Y1, Y6
	VMULPS      Y6, Y6, Y1
	VMOVUPS     -96(SP), Y9
	VFMADD213PS -128(SP), Y1, Y9
	VFMADD213PS Y11, Y1, Y9
	VMULPS      Y1, Y1, Y7
	VMOVAPS     Y10, Y12
	VFMADD213PS Y13, Y1, Y12
	VFMADD231PS Y7, Y9, Y12
	VMOVAPS     Y14, Y7
	VFMADD213PS Y15, Y1, Y7
	VFMADD213PS Y2, Y1, Y7
	VMULPS      Y6, Y1, Y1
	VFMADD213PS Y6, Y7, Y1
	VPSLLD      $0x1d, Y3, Y6
	VPAND       Y4, Y3, Y3
	VPXOR       Y5, Y6, Y5
	VPCMPEQD    Y4, Y3, Y6
	VPCMPEQD    dataSinCosF32<>+96(SB), Y3, Y3
	VANDPS      Y3, Y1, Y3
	VANDPS      Y6, Y12, Y6
	VADDPS      Y3, Y6, Y3
	VADDPS      Y1, Y12, Y1
	VPAND       Y5, Y8, Y5
	VSUBPS      Y3, Y1, Y1
	VPXOR       Y3, Y5, Y3
	VPSLLD      $0x1d, Y0, Y0
	VPADDD      -64(SP), Y0, Y0
	VPAND       Y0, Y8, Y0
	VPXOR       Y0, Y8, Y0
	VXORPS      Y0, Y1, Y0
	VMOVDQU     Y3, (DI)(AX*4)
	VMOVUPS     Y0, (SI)(AX*4)
	ADDQ        $0x08, AX
	CMPQ        AX, R8
	JB          LBB14_2

LBB14_3:
	CMPQ         R8, CX
	JAE          LBB14_16
	VBROADCASTSS dataSinCosF32<>+24(SB), X0
	VXORPS       X1, X1, X1
	VMOVSS       dataSinCosF32<>+60(SB), X2
	VMOVSS       dataSinCosF32<>+44(SB), X6
	VMOVSS       dataSinCosF32<>+28(SB), X8
	VMOVSS       dataSinCosF32<>+36(SB), X12
	VMOVSS       dataSinCosF32<>+40(SB), X13
	VMOVSS       dataSinCosF32<>+48(SB), X15
	VMOVSS       dataSinCosF32<>+52(SB), X14
	VMOVSS       dataSinCosF32<>+56(SB), X10
	JMP          LBB14_5

LBB14_15:
	ADDQ $0x01, R8
	CMPQ R8, CX
	JAE  LBB14_16

LBB14_5:
	VMOVSS     (DX)(R8*4), X4
	VXORPS     X0, X4, X5
	VCMPSS     $0x01, X1, X4, X7
	VBLENDVPS  X7, X5, X4, X5
	VUCOMISS   X2, X5
	JA         LBB14_15
	VUCOMISS   X1, X4
	SETCS      R9
	VMULSS     dataSinCosF32<>+4(SB), X5, X4
	VCVTTSS2SI X4, R10
	VROUNDSS   $0x0b, X4, X4, X4
	MOVL       R10, AX
	ANDL       $0x01, AX
	JE         LBB14_8
	VADDSS     X6, X4, X4

LBB14_8:
	ADDL        R10, AX
	ANDL        $0x07, AX
	LEAL        -4(AX), R10
	CMPL        AX, $0x04
	SETCC       R11
	CMOVLLT     AX, R10
	VFMADD231SS dataSinCosF32<>+16(SB), X4, X5
	VMULSS      X5, X5, X7
	VMOVAPS     X8, X11
	VFMADD213SS dataSinCosF32<>+32(SB), X7, X11
	VFMADD213SS X12, X7, X11
	VMULSS      X7, X7, X9
	VMOVAPS     X6, X4
	VFMADD231SS X13, X7, X4
	VFMADD231SS X9, X11, X4
	VMOVAPS     X15, X3
	VFMADD213SS X14, X7, X3
	VFMADD213SS X10, X7, X3
	VMULSS      X5, X7, X7
	VFMADD213SS X5, X3, X7
	LEAL        -1(R10), BX
	CMPL        BX, $0x02
	JB          LBB14_9
	VMOVAPS     X7, X5
	VMOVSS      X5, (DI)(R8*4)
	VMOVSS      X4, (SI)(R8*4)
	CMPB        R9, R11
	JNE         LBB14_12
	JMP         LBB14_13

LBB14_9:
	VMOVAPS X4, X5
	VMOVAPS X7, X4
	VMOVSS  X5, (DI)(R8*4)
	VMOVSS  X4, (SI)(R8*4)
	CMPB    R9, R11
	JE      LBB14_13

LBB14_12:
	VMOVSS (DI)(R8*4), X3
	VXORPS X0, X3, X3
	VMOVSS X3, (DI)(R8*4)

LBB14_13:
	CMPL   R10, $0x02
	SETCC  BL
	CMPL   AX, $0x04
	SETCC  AL
	CMPB   AL, BL
	JE     LBB14_15
	VMOVSS (SI)(R8*4), X3
	VXORPS X0, X3, X3
	VMOVSS X3, (SI)(R8*4)
	JMP    LBB14_15

LBB14_16:
	ADDQ $0x60, SP
	POPQ BX
	VZEROUPPER
	RET

DATA dataExpLen8xF32<>+0(SB)/4, $0x42b17218
DATA dataExpLen8xF32<>+4(SB)/4, $0xc2ce8ed0
DATA dataExpLen8xF32<>+8(SB)/4, $0x3f000000
DATA dataExpLen8xF32<>+12(SB)/4, $0x3fb8aa3b
DATA dataExpLen8xF32<>+16(SB)/4, $0xbf318000
DATA dataExpLen8xF32<>+20(SB)/4, $0x395e8083
DATA dataExpLen8xF32<>+24(SB)/4, $0x3f800000
DATA dataExpLen8xF32<>+28(SB)/4, $0x3ab743ce
DATA dataExpLen8xF32<>+32(SB)/4, $0x39506967
DATA dataExpLen8xF32<>+36(SB)/4, $0x3c088908
DATA dataExpLen8xF32<>+40(SB)/4, $0x3d2aa9c1
DATA dataExpLen8xF32<>+44(SB)/4, $0x3e2aaaaa
DATA dataExpLen8xF32<>+48(SB)/4, $0x7f7fffff
GLOBL dataExpLen8xF32<>(SB), RODATA|NOPTR, $52

// func Exp_Len8x_AVX2_F32(x []float32)
// Requires: AVX, AVX2, FMA3
TEXT ·Exp_Len8x_AVX2_F32(SB), NOSPLIT, $0-24
	MOVQ         x_base+0(FP), DI
	MOVQ         x_len+8(FP), SI
	TESTQ        SI, SI
	JE           LBB11_3
	XORL         AX, AX
	VBROADCASTSS dataExpLen8xF32<>+0(SB), Y0
	VMOVUPS      Y0, -40(SP)
	VBROADCASTSS dataExpLen8xF32<>+4(SB), Y0
	VMOVUPS      Y0, -72(SP)
	VBROADCASTSS dataExpLen8xF32<>+8(SB), Y2
	VBROADCASTSS dataExpLen8xF32<>+12(SB), Y3
	VBROADCASTSS dataExpLen8xF32<>+16(SB), Y4
	VBROADCASTSS dataExpLen8xF32<>+20(SB), Y5
	VPBROADCASTD dataExpLen8xF32<>+24(SB), Y6
	VBROADCASTSS dataExpLen8xF32<>+28(SB), Y7
	VBROADCASTSS dataExpLen8xF32<>+32(SB), Y1
	VBROADCASTSS dataExpLen8xF32<>+36(SB), Y9
	VBROADCASTSS dataExpLen8xF32<>+40(SB), Y10
	VBROADCASTSS dataExpLen8xF32<>+44(SB), Y11
	VBROADCASTSS dataExpLen8xF32<>+48(SB), Y12

LBB11_2:
	VMOVUPS     (DI)(AX*4), Y13
	VMOVAPS     Y3, Y14
	VFMADD213PS Y2, Y13, Y14
	VROUNDPS    $0x01, Y14, Y14
	VMOVAPS     Y4, Y15
	VFMADD213PS Y13, Y14, Y15
	VFMADD231PS Y5, Y14, Y15
	VMULPS      Y15, Y15, Y0
	VMOVAPS     Y1, Y8
	VFMADD213PS Y7, Y15, Y8
	VFMADD213PS Y9, Y15, Y8
	VFMADD213PS Y10, Y15, Y8
	VFMADD213PS Y11, Y15, Y8
	VFMADD213PS Y2, Y15, Y8
	VFMADD213PS Y15, Y0, Y8
	VCVTTPS2DQ  Y14, Y0
	VPSLLD      $0x17, Y0, Y0
	VPADDD      Y6, Y0, Y0
	VFMADD213PS Y0, Y0, Y8
	VMOVUPS     -40(SP), Y0
	VCMPPS      $0x01, Y13, Y0, Y0
	VBLENDVPS   Y0, Y12, Y8, Y0
	VMOVUPS     -72(SP), Y8
	VCMPPS      $0x02, Y13, Y8, Y8
	VANDPS      Y0, Y8, Y0
	VMOVUPS     Y0, (DI)(AX*4)
	ADDQ        $0x08, AX
	CMPQ        AX, SI
	JB          LBB11_2

LBB11_3:
	VZEROUPPER
	RET

DATA dataLogLen8xF32<>+0(SB)/4, $0x00800000
DATA dataLogLen8xF32<>+4(SB)/4, $0x807fffff
DATA dataLogLen8xF32<>+8(SB)/4, $0x3f000000
DATA dataLogLen8xF32<>+12(SB)/4, $0xffffff81
DATA dataLogLen8xF32<>+16(SB)/4, $0x3f800000
DATA dataLogLen8xF32<>+20(SB)/4, $0x3f3504f3
DATA dataLogLen8xF32<>+24(SB)/4, $0xbf800000
DATA dataLogLen8xF32<>+28(SB)/4, $0x3d9021bb
DATA dataLogLen8xF32<>+32(SB)/4, $0xbdebd1b8
DATA dataLogLen8xF32<>+36(SB)/4, $0x3def251a
DATA dataLogLen8xF32<>+40(SB)/4, $0xbdfe5d4f
DATA dataLogLen8xF32<>+44(SB)/4, $0x3e11e9bf
DATA dataLogLen8xF32<>+48(SB)/4, $0xbe2aae50
DATA dataLogLen8xF32<>+52(SB)/4, $0x3e4cceac
DATA dataLogLen8xF32<>+56(SB)/4, $0xbe7ffffc
DATA dataLogLen8xF32<>+60(SB)/4, $0x3eaaaaaa
DATA dataLogLen8xF32<>+64(SB)/4, $0x3f317218
DATA dataLogLen8xF32<>+68(SB)/4, $0xbf000000
DATA dataLogLen8xF32<>+72(SB)/8, $0x0000000000000000
DATA dataLogLen8xF32<>+80(SB)/8, $0x0000000000000000
DATA dataLogLen8xF32<>+88(SB)/8, $0x0000000000000000
DATA dataLogLen8xF32<>+96(SB)/8, $0x0000000000000000
GLOBL dataLogLen8xF32<>(SB), RODATA|NOPTR, $104

// func Log_Len8x_AVX2_F32(x []float32)
// Requires: AVX, AVX2, FMA3
TEXT ·Log_Len8x_AVX2_F32(SB), NOSPLIT, $0-24
	MOVQ         x_base+0(FP), DI
	MOVQ         x_len+8(FP), SI
	SUBQ         $0x68, SP
	TESTQ        SI, SI
	JE           LBB10_3
	XORL         AX, AX
	VBROADCASTSS dataLogLen8xF32<>+0(SB), Y0
	VMOVUPS      Y0, 64(SP)
	VBROADCASTSS dataLogLen8xF32<>+4(SB), Y0
	VMOVUPS      Y0, 32(SP)
	VBROADCASTSS dataLogLen8xF32<>+8(SB), Y0
	VMOVUPS      Y0, (SP)
	VBROADCASTSS dataLogLen8xF32<>+12(SB), Y0
	VMOVUPS      Y0, -32(SP)
	VBROADCASTSS dataLogLen8xF32<>+16(SB), Y0
	VMOVUPS      Y0, -64(SP)
	VBROADCASTSS dataLogLen8xF32<>+20(SB), Y0
	VMOVUPS      Y0, -96(SP)
	VBROADCASTSS dataLogLen8xF32<>+24(SB), Y0
	VMOVUPS      Y0, -128(SP)
	VBROADCASTSS dataLogLen8xF32<>+28(SB), Y8
	VBROADCASTSS dataLogLen8xF32<>+32(SB), Y9
	VBROADCASTSS dataLogLen8xF32<>+36(SB), Y10
	VBROADCASTSS dataLogLen8xF32<>+40(SB), Y11
	VBROADCASTSS dataLogLen8xF32<>+44(SB), Y12
	VBROADCASTSS dataLogLen8xF32<>+48(SB), Y13
	VBROADCASTSS dataLogLen8xF32<>+52(SB), Y14
	VBROADCASTSS dataLogLen8xF32<>+56(SB), Y15
	VBROADCASTSS dataLogLen8xF32<>+60(SB), Y0
	VBROADCASTSS dataLogLen8xF32<>+64(SB), Y1
	VBROADCASTSS dataLogLen8xF32<>+68(SB), Y2

LBB10_2:
	VMOVUPS     (DI)(AX*4), Y3
	VMAXPS      64(SP), Y3, Y4
	VPSRLD      $0x17, Y4, Y5
	VPADDD      -32(SP), Y5, Y5
	VANDPS      32(SP), Y4, Y4
	VORPS       (SP), Y4, Y4
	VCVTDQ2PS   Y5, Y5
	VADDPS      -64(SP), Y5, Y6
	VCMPPS      $0x01, -96(SP), Y4, Y7
	VBLENDVPS   Y7, Y5, Y6, Y5
	VANDPS      Y4, Y7, Y6
	VADDPS      -128(SP), Y4, Y4
	VADDPS      Y6, Y4, Y4
	VMOVAPS     Y8, Y6
	VFMADD213PS Y9, Y4, Y6
	VFMADD213PS Y10, Y4, Y6
	VFMADD213PS Y11, Y4, Y6
	VFMADD213PS Y12, Y4, Y6
	VFMADD213PS Y13, Y4, Y6
	VFMADD213PS Y14, Y4, Y6
	VFMADD213PS Y15, Y4, Y6
	VFMADD213PS Y0, Y4, Y6
	VFMADD213PS Y2, Y4, Y6
	VFMADD213PS Y4, Y1, Y5
	VMULPS      Y4, Y4, Y4
	VFMADD231PS Y6, Y4, Y5
	VCMPPS      $0x02, dataLogLen8xF32<>+72(SB), Y3, Y3
	VORPS       Y5, Y3, Y3
	VMOVUPS     Y3, (DI)(AX*4)
	ADDQ        $0x08, AX
	CMPQ        AX, SI
	JB          LBB10_2

LBB10_3:
	ADDQ $0x68, SP
	VZEROUPPER
	RET

DATA dataLog2Len8xF32<>+0(SB)/4, $0x00800000
DATA dataLog2Len8xF32<>+4(SB)/4, $0x807fffff
DATA dataLog2Len8xF32<>+8(SB)/4, $0x3f000000
DATA dataLog2Len8xF32<>+12(SB)/4, $0xffffff81
DATA dataLog2Len8xF32<>+16(SB)/4, $0x3f800000
DATA dataLog2Len8xF32<>+20(SB)/4, $0x3f3504f3
DATA dataLog2Len8xF32<>+24(SB)/4, $0xbf800000
DATA dataLog2Len8xF32<>+28(SB)/4, $0x3d9021bb
DATA dataLog2Len8xF32<>+32(SB)/4, $0xbdebd1b8
DATA dataLog2Len8xF32<>+36(SB)/4, $0x3def251a
DATA dataLog2Len8xF32<>+40(SB)/4, $0xbdfe5d4f
DATA dataLog2Len8xF32<>+44(SB)/4, $0x3e11e9bf
DATA dataLog2Len8xF32<>+48(SB)/4, $0xbe2aae50
DATA dataLog2Len8xF32<>+52(SB)/4, $0x3e4cceac
DATA dataLog2Len8xF32<>+56(SB)/4, $0xbe7ffffc
DATA dataLog2Len8xF32<>+60(SB)/4, $0x3eaaaaaa
DATA dataLog2Len8xF32<>+64(SB)/4, $0x3f317218
DATA dataLog2Len8xF32<>+68(SB)/4, $0xbf000000
DATA dataLog2Len8xF32<>+72(SB)/4, $0x3fb8aa3b
DATA dataLog2Len8xF32<>+76(SB)/8, $0x0000000000000000
DATA dataLog2Len8xF32<>+84(SB)/8, $0x0000000000000000
DATA dataLog2Len8xF32<>+92(SB)/8, $0x0000000000000000
DATA dataLog2Len8xF32<>+100(SB)/8, $0x0000000000000000
GLOBL dataLog2Len8xF32<>(SB), RODATA|NOPTR, $108

// func Log2_Len8x_AVX2_F32(x []float32)
// Requires: AVX, AVX2, FMA3
TEXT ·Log2_Len8x_AVX2_F32(SB), NOSPLIT, $0-24
	MOVQ         x_base+0(FP), DI
	MOVQ         x_len+8(FP), SI
	SUBQ         $0x88, SP
	TESTQ        SI, SI
	JE           LBB9_3
	XORL         AX, AX
	VBROADCASTSS dataLog2Len8xF32<>+4(SB), Y0
	VMOVUPS      Y0, 96(SP)
	VBROADCASTSS dataLog2Len8xF32<>+8(SB), Y0
	VMOVUPS      Y0, 64(SP)
	VBROADCASTSS dataLog2Len8xF32<>+12(SB), Y0
	VMOVUPS      Y0, 32(SP)
	VBROADCASTSS dataLog2Len8xF32<>+0(SB), Y0
	VMOVUPS      Y0, (SP)
	VBROADCASTSS dataLog2Len8xF32<>+16(SB), Y0
	VMOVUPS      Y0, -32(SP)
	VBROADCASTSS dataLog2Len8xF32<>+20(SB), Y0
	VMOVUPS      Y0, -64(SP)
	VBROADCASTSS dataLog2Len8xF32<>+24(SB), Y0
	VMOVUPS      Y0, -96(SP)
	VBROADCASTSS dataLog2Len8xF32<>+28(SB), Y0
	VMOVUPS      Y0, -128(SP)
	VBROADCASTSS dataLog2Len8xF32<>+32(SB), Y9
	VBROADCASTSS dataLog2Len8xF32<>+36(SB), Y10
	VBROADCASTSS dataLog2Len8xF32<>+40(SB), Y11
	VBROADCASTSS dataLog2Len8xF32<>+44(SB), Y12
	VBROADCASTSS dataLog2Len8xF32<>+48(SB), Y13
	VBROADCASTSS dataLog2Len8xF32<>+52(SB), Y14
	VBROADCASTSS dataLog2Len8xF32<>+56(SB), Y15
	VBROADCASTSS dataLog2Len8xF32<>+60(SB), Y0
	VBROADCASTSS dataLog2Len8xF32<>+64(SB), Y1
	VBROADCASTSS dataLog2Len8xF32<>+68(SB), Y2
	VBROADCASTSS dataLog2Len8xF32<>+72(SB), Y3

LBB9_2:
	VMOVUPS     (DI)(AX*4), Y4
	VMAXPS      (SP), Y4, Y5
	VPSRLD      $0x17, Y5, Y6
	VPADDD      32(SP), Y6, Y6
	VANDPS      96(SP), Y5, Y5
	VORPS       64(SP), Y5, Y5
	VCVTDQ2PS   Y6, Y6
	VADDPS      -32(SP), Y6, Y7
	VCMPPS      $0x01, -64(SP), Y5, Y8
	VBLENDVPS   Y8, Y6, Y7, Y6
	VANDPS      Y5, Y8, Y7
	VADDPS      -96(SP), Y5, Y5
	VADDPS      Y7, Y5, Y5
	VMOVUPS     -128(SP), Y7
	VFMADD213PS Y9, Y5, Y7
	VFMADD213PS Y10, Y5, Y7
	VFMADD213PS Y11, Y5, Y7
	VFMADD213PS Y12, Y5, Y7
	VFMADD213PS Y13, Y5, Y7
	VFMADD213PS Y14, Y5, Y7
	VFMADD213PS Y15, Y5, Y7
	VFMADD213PS Y0, Y5, Y7
	VFMADD213PS Y2, Y5, Y7
	VFMADD213PS Y5, Y1, Y6
	VMULPS      Y5, Y5, Y5
	VFMADD231PS Y7, Y5, Y6
	VCMPPS      $0x02, dataLog2Len8xF32<>+76(SB), Y4, Y4
	VMULPS      Y3, Y6, Y5
	VORPS       Y5, Y4, Y4
	VMOVUPS     Y4, (DI)(AX*4)
	ADDQ        $0x08, AX
	CMPQ        AX, SI
	JB          LBB9_2

LBB9_3:
	ADDQ $0x88, SP
	VZEROUPPER
	RET

DATA dataLog10Len8xF32<>+0(SB)/4, $0x00800000
DATA dataLog10Len8xF32<>+4(SB)/4, $0x807fffff
DATA dataLog10Len8xF32<>+8(SB)/4, $0x3f000000
DATA dataLog10Len8xF32<>+12(SB)/4, $0xffffff81
DATA dataLog10Len8xF32<>+16(SB)/4, $0x3f800000
DATA dataLog10Len8xF32<>+20(SB)/4, $0x3f3504f3
DATA dataLog10Len8xF32<>+24(SB)/4, $0xbf800000
DATA dataLog10Len8xF32<>+28(SB)/4, $0x3d9021bb
DATA dataLog10Len8xF32<>+32(SB)/4, $0xbdebd1b8
DATA dataLog10Len8xF32<>+36(SB)/4, $0x3def251a
DATA dataLog10Len8xF32<>+40(SB)/4, $0xbdfe5d4f
DATA dataLog10Len8xF32<>+44(SB)/4, $0x3e11e9bf
DATA dataLog10Len8xF32<>+48(SB)/4, $0xbe2aae50
DATA dataLog10Len8xF32<>+52(SB)/4, $0x3e4cceac
DATA dataLog10Len8xF32<>+56(SB)/4, $0xbe7ffffc
DATA dataLog10Len8xF32<>+60(SB)/4, $0x3eaaaaaa
DATA dataLog10Len8xF32<>+64(SB)/4, $0x3f317218
DATA dataLog10Len8xF32<>+68(SB)/4, $0xbf000000
DATA dataLog10Len8xF32<>+72(SB)/4, $0x3ede5bd9
DATA dataLog10Len8xF32<>+76(SB)/8, $0x0000000000000000
DATA dataLog10Len8xF32<>+84(SB)/8, $0x0000000000000000
DATA dataLog10Len8xF32<>+92(SB)/8, $0x0000000000000000
DATA dataLog10Len8xF32<>+100(SB)/8, $0x0000000000000000
GLOBL dataLog10Len8xF32<>(SB), RODATA|NOPTR, $108

// func Log10_Len8x_AVX2_F32(x []float32)
// Requires: AVX, AVX2, FMA3
TEXT ·Log10_Len8x_AVX2_F32(SB), NOSPLIT, $0-24
	MOVQ         x_base+0(FP), DI
	MOVQ         x_len+8(FP), SI
	SUBQ         $0x88, SP
	TESTQ        SI, SI
	JE           LBB8_3
	XORL         AX, AX
	VBROADCASTSS dataLog10Len8xF32<>+4(SB), Y0
	VMOVUPS      Y0, 96(SP)
	VBROADCASTSS dataLog10Len8xF32<>+8(SB), Y0
	VMOVUPS      Y0, 64(SP)
	VBROADCASTSS dataLog10Len8xF32<>+12(SB), Y0
	VMOVUPS      Y0, 32(SP)
	VBROADCASTSS dataLog10Len8xF32<>+0(SB), Y0
	VMOVUPS      Y0, (SP)
	VBROADCASTSS dataLog10Len8xF32<>+16(SB), Y0
	VMOVUPS      Y0, -32(SP)
	VBROADCASTSS dataLog10Len8xF32<>+20(SB), Y0
	VMOVUPS      Y0, -64(SP)
	VBROADCASTSS dataLog10Len8xF32<>+24(SB), Y0
	VMOVUPS      Y0, -96(SP)
	VBROADCASTSS dataLog10Len8xF32<>+28(SB), Y0
	VMOVUPS      Y0, -128(SP)
	VBROADCASTSS dataLog10Len8xF32<>+32(SB), Y9
	VBROADCASTSS dataLog10Len8xF32<>+36(SB), Y10
	VBROADCASTSS dataLog10Len8xF32<>+40(SB), Y11
	VBROADCASTSS dataLog10Len8xF32<>+44(SB), Y12
	VBROADCASTSS dataLog10Len8xF32<>+48(SB), Y13
	VBROADCASTSS dataLog10Len8xF32<>+52(SB), Y14
	VBROADCASTSS dataLog10Len8xF32<>+56(SB), Y15
	VBROADCASTSS dataLog10Len8xF32<>+60(SB), Y0
	VBROADCASTSS dataLog10Len8xF32<>+64(SB), Y1
	VBROADCASTSS dataLog10Len8xF32<>+68(SB), Y2
	VBROADCASTSS dataLog10Len8xF32<>+72(SB), Y3

LBB8_2:
	VMOVUPS     (DI)(AX*4), Y4
	VMAXPS      (SP), Y4, Y5
	VPSRLD      $0x17, Y5, Y6
	VPADDD      32(SP), Y6, Y6
	VANDPS      96(SP), Y5, Y5
	VORPS       64(SP), Y5, Y5
	VCVTDQ2PS   Y6, Y6
	VADDPS      -32(SP), Y6, Y7
	VCMPPS      $0x01, -64(SP), Y5, Y8
	VBLENDVPS   Y8, Y6, Y7, Y6
	VANDPS      Y5, Y8, Y7
	VADDPS      -96(SP), Y5, Y5
	VADDPS      Y7, Y5, Y5
	VMOVUPS     -128(SP), Y7
	VFMADD213PS Y9, Y5, Y7
	VFMADD213PS Y10, Y5, Y7
	VFMADD213PS Y11, Y5, Y7
	VFMADD213PS Y12, Y5, Y7
	VFMADD213PS Y13, Y5, Y7
	VFMADD213PS Y14, Y5, Y7
	VFMADD213PS Y15, Y5, Y7
	VFMADD213PS Y0, Y5, Y7
	VFMADD213PS Y2, Y5, Y7
	VFMADD213PS Y5, Y1, Y6
	VMULPS      Y5, Y5, Y5
	VFMADD231PS Y7, Y5, Y6
	VCMPPS      $0x02, dataLog10Len8xF32<>+76(SB), Y4, Y4
	VMULPS      Y3, Y6, Y5
	VORPS       Y5, Y4, Y4
	VMOVUPS     Y4, (DI)(AX*4)
	ADDQ        $0x08, AX
	CMPQ        AX, SI
	JB          LBB8_2

LBB8_3:
	ADDQ $0x88, SP
	VZEROUPPER
	RET

DATA dataMinF64<>+0(SB)/8, $0x7fefffffffffffff
GLOBL dataMinF64<>(SB), RODATA|NOPTR, $8

// func Min_AVX2_F64(x []float64) float64
// Requires: AVX, SSE2
TEXT ·Min_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB0_1
	CMPQ   SI, $0x10
	JAE    LBB0_4
	VMOVSD dataMinF64<>+0(SB), X0
	XORL   AX, AX
	JMP    LBB0_11

LBB0_1:
	VMOVSD dataMinF64<>+0(SB), X0
	MOVSD  X0, ret+24(FP)
	RET

LBB0_4:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	LEAQ         -16(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x04, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB0_5
	MOVQ         R8, CX
	ANDQ         $-2, CX
	VBROADCASTSD dataMinF64<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPD      Y0, Y1
	VMOVAPD      Y0, Y2
	VMOVAPD      Y0, Y3

LBB0_7:
	VMINPD (DI)(DX*8), Y0, Y0
	VMINPD 32(DI)(DX*8), Y1, Y1
	VMINPD 64(DI)(DX*8), Y2, Y2
	VMINPD 96(DI)(DX*8), Y3, Y3
	VMINPD 128(DI)(DX*8), Y0, Y0
	VMINPD 160(DI)(DX*8), Y1, Y1
	VMINPD 192(DI)(DX*8), Y2, Y2
	VMINPD 224(DI)(DX*8), Y3, Y3
	ADDQ   $0x20, DX
	ADDQ   $-2, CX
	JNE    LBB0_7
	TESTB  $0x01, R8
	JE     LBB0_10

LBB0_9:
	VMINPD (DI)(DX*8), Y0, Y0
	VMINPD 32(DI)(DX*8), Y1, Y1
	VMINPD 64(DI)(DX*8), Y2, Y2
	VMINPD 96(DI)(DX*8), Y3, Y3

LBB0_10:
	VMINPD       Y3, Y0, Y0
	VMINPD       Y2, Y1, Y1
	VMINPD       Y0, Y1, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VMINPD       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VMINSD       X1, X0, X0
	CMPQ         AX, SI
	JE           LBB0_12

LBB0_11:
	VMINSD (DI)(AX*8), X0, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB0_11

LBB0_12:
	VZEROUPPER
	MOVSD X0, ret+24(FP)
	RET

LBB0_5:
	VBROADCASTSD dataMinF64<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPD      Y0, Y1
	VMOVAPD      Y0, Y2
	VMOVAPD      Y0, Y3
	TESTB        $0x01, R8
	JNE          LBB0_9
	JMP          LBB0_10

DATA dataMinF32<>+0(SB)/4, $0x7f7fffff
GLOBL dataMinF32<>(SB), RODATA|NOPTR, $4

// func Min_AVX2_F32(x []float32) float32
// Requires: AVX, SSE
TEXT ·Min_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     LBB1_1
	CMPQ   SI, $0x20
	JAE    LBB1_4
	VMOVSS dataMinF32<>+0(SB), X0
	XORL   AX, AX
	JMP    LBB1_11

LBB1_1:
	VMOVSS dataMinF32<>+0(SB), X0
	MOVSS  X0, ret+24(FP)
	RET

LBB1_4:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	LEAQ         -32(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x05, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB1_5
	MOVQ         R8, CX
	ANDQ         $-2, CX
	VBROADCASTSS dataMinF32<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPS      Y0, Y1
	VMOVAPS      Y0, Y2
	VMOVAPS      Y0, Y3

LBB1_7:
	VMINPS (DI)(DX*4), Y0, Y0
	VMINPS 32(DI)(DX*4), Y1, Y1
	VMINPS 64(DI)(DX*4), Y2, Y2
	VMINPS 96(DI)(DX*4), Y3, Y3
	VMINPS 128(DI)(DX*4), Y0, Y0
	VMINPS 160(DI)(DX*4), Y1, Y1
	VMINPS 192(DI)(DX*4), Y2, Y2
	VMINPS 224(DI)(DX*4), Y3, Y3
	ADDQ   $0x40, DX
	ADDQ   $-2, CX
	JNE    LBB1_7
	TESTB  $0x01, R8
	JE     LBB1_10

LBB1_9:
	VMINPS (DI)(DX*4), Y0, Y0
	VMINPS 32(DI)(DX*4), Y1, Y1
	VMINPS 64(DI)(DX*4), Y2, Y2
	VMINPS 96(DI)(DX*4), Y3, Y3

LBB1_10:
	VMINPS       Y3, Y0, Y0
	VMINPS       Y2, Y1, Y1
	VMINPS       Y0, Y1, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VMINPS       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VMINPS       X1, X0, X0
	VMOVSHDUP    X0, X1
	VMINSS       X1, X0, X0
	CMPQ         AX, SI
	JE           LBB1_12

LBB1_11:
	VMINSS (DI)(AX*4), X0, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB1_11

LBB1_12:
	VZEROUPPER
	MOVSS X0, ret+24(FP)
	RET

LBB1_5:
	VBROADCASTSS dataMinF32<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPS      Y0, Y1
	VMOVAPS      Y0, Y2
	VMOVAPS      Y0, Y3
	TESTB        $0x01, R8
	JNE          LBB1_9
	JMP          LBB1_10

// func Minimum_AVX2_F64(x []float64, y []float64)
// Requires: AVX
TEXT ·Minimum_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB2_9
	CMPQ  DX, $0x10
	JAE   LBB2_3
	XORL  AX, AX
	JMP   LBB2_6

LBB2_3:
	MOVQ DX, AX
	ANDQ $-16, AX
	LEAQ 96(DI), R8
	XORL CX, CX

LBB2_4:
	VMOVUPD    (SI)(CX*8), Y0
	VMOVUPD    32(SI)(CX*8), Y1
	VMOVUPD    64(SI)(CX*8), Y2
	VMOVUPD    96(SI)(CX*8), Y3
	VCMPPD     $0x01, -96(R8)(CX*8), Y0, Y4
	VCMPPD     $0x01, -64(R8)(CX*8), Y1, Y5
	VCMPPD     $0x01, -32(R8)(CX*8), Y2, Y6
	VCMPPD     $0x01, (R8)(CX*8), Y3, Y7
	VMASKMOVPD Y0, Y4, -96(R8)(CX*8)
	VMASKMOVPD Y1, Y5, -64(R8)(CX*8)
	VMASKMOVPD Y2, Y6, -32(R8)(CX*8)
	VMASKMOVPD Y3, Y7, (R8)(CX*8)
	ADDQ       $0x10, CX
	CMPQ       AX, CX
	JNE        LBB2_4
	CMPQ       AX, DX
	JNE        LBB2_6

LBB2_9:
	VZEROUPPER
	RET

LBB2_8:
	ADDQ $0x01, AX
	CMPQ DX, AX
	JE   LBB2_9

LBB2_6:
	VMOVSD   (SI)(AX*8), X0
	VUCOMISD (DI)(AX*8), X0
	JAE      LBB2_8
	VMOVSD   X0, (DI)(AX*8)
	JMP      LBB2_8

// func Minimum_AVX2_F32(x []float32, y []float32)
// Requires: AVX
TEXT ·Minimum_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB3_9
	CMPQ  DX, $0x20
	JAE   LBB3_3
	XORL  AX, AX
	JMP   LBB3_6

LBB3_3:
	MOVQ DX, AX
	ANDQ $-32, AX
	LEAQ 96(DI), R8
	XORL CX, CX

LBB3_4:
	VMOVUPS    (SI)(CX*4), Y0
	VMOVUPS    32(SI)(CX*4), Y1
	VMOVUPS    64(SI)(CX*4), Y2
	VMOVUPS    96(SI)(CX*4), Y3
	VCMPPS     $0x01, -96(R8)(CX*4), Y0, Y4
	VCMPPS     $0x01, -64(R8)(CX*4), Y1, Y5
	VCMPPS     $0x01, -32(R8)(CX*4), Y2, Y6
	VCMPPS     $0x01, (R8)(CX*4), Y3, Y7
	VMASKMOVPS Y0, Y4, -96(R8)(CX*4)
	VMASKMOVPS Y1, Y5, -64(R8)(CX*4)
	VMASKMOVPS Y2, Y6, -32(R8)(CX*4)
	VMASKMOVPS Y3, Y7, (R8)(CX*4)
	ADDQ       $0x20, CX
	CMPQ       AX, CX
	JNE        LBB3_4
	CMPQ       AX, DX
	JNE        LBB3_6

LBB3_9:
	VZEROUPPER
	RET

LBB3_8:
	ADDQ $0x01, AX
	CMPQ DX, AX
	JE   LBB3_9

LBB3_6:
	VMOVSS   (SI)(AX*4), X0
	VUCOMISS (DI)(AX*4), X0
	JAE      LBB3_8
	VMOVSS   X0, (DI)(AX*4)
	JMP      LBB3_8

// func MinimumNumber_AVX2_F64(x []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·MinimumNumber_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVSD a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB4_9
	CMPQ  SI, $0x10
	JAE   LBB4_3
	XORL  AX, AX
	JMP   LBB4_6

LBB4_3:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	LEAQ         96(DI), CX
	XORL         DX, DX

LBB4_4:
	VCMPPD     $0x01, -96(CX)(DX*8), Y1, Y2
	VCMPPD     $0x01, -64(CX)(DX*8), Y1, Y3
	VCMPPD     $0x01, -32(CX)(DX*8), Y1, Y4
	VCMPPD     $0x01, (CX)(DX*8), Y1, Y5
	VMASKMOVPD Y1, Y2, -96(CX)(DX*8)
	VMASKMOVPD Y1, Y3, -64(CX)(DX*8)
	VMASKMOVPD Y1, Y4, -32(CX)(DX*8)
	VMASKMOVPD Y1, Y5, (CX)(DX*8)
	ADDQ       $0x10, DX
	CMPQ       AX, DX
	JNE        LBB4_4
	CMPQ       AX, SI
	JNE        LBB4_6

LBB4_9:
	VZEROUPPER
	RET

LBB4_8:
	ADDQ $0x01, AX
	CMPQ SI, AX
	JE   LBB4_9

LBB4_6:
	VUCOMISD (DI)(AX*8), X0
	JAE      LBB4_8
	VMOVSD   X0, (DI)(AX*8)
	JMP      LBB4_8

// func MinimumNumber_AVX2_F32(x []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·MinimumNumber_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), DI
	MOVSS a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB5_9
	CMPQ  SI, $0x20
	JAE   LBB5_3
	XORL  AX, AX
	JMP   LBB5_6

LBB5_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	LEAQ         96(DI), CX
	XORL         DX, DX

LBB5_4:
	VCMPPS     $0x01, -96(CX)(DX*4), Y1, Y2
	VCMPPS     $0x01, -64(CX)(DX*4), Y1, Y3
	VCMPPS     $0x01, -32(CX)(DX*4), Y1, Y4
	VCMPPS     $0x01, (CX)(DX*4), Y1, Y5
	VMASKMOVPS Y1, Y2, -96(CX)(DX*4)
	VMASKMOVPS Y1, Y3, -64(CX)(DX*4)
	VMASKMOVPS Y1, Y4, -32(CX)(DX*4)
	VMASKMOVPS Y1, Y5, (CX)(DX*4)
	ADDQ       $0x20, DX
	CMPQ       AX, DX
	JNE        LBB5_4
	CMPQ       AX, SI
	JNE        LBB5_6

LBB5_9:
	VZEROUPPER
	RET

LBB5_8:
	ADDQ $0x01, AX
	CMPQ SI, AX
	JE   LBB5_9

LBB5_6:
	VUCOMISS (DI)(AX*4), X0
	JAE      LBB5_8
	VMOVSS   X0, (DI)(AX*4)
	JMP      LBB5_8

DATA dataMaxF64<>+0(SB)/8, $0xffefffffffffffff
GLOBL dataMaxF64<>(SB), RODATA|NOPTR, $8

// func Max_AVX2_F64(x []float64) float64
// Requires: AVX, SSE2
TEXT ·Max_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     empty
	CMPQ   SI, $0x10
	JAE    loop
	VMOVSD dataMaxF64<>+0(SB), X0
	XORL   AX, AX
	JMP    collect

empty:
	VMOVSD dataMaxF64<>+0(SB), X0
	MOVSD  X0, ret+24(FP)
	RET

loop:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	LEAQ         -16(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x04, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           setmin
	MOVQ         R8, CX
	ANDQ         $-2, CX
	VBROADCASTSD dataMaxF64<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPD      Y0, Y1
	VMOVAPD      Y0, Y2
	VMOVAPD      Y0, Y3

body:
	VMAXPD (DI)(DX*8), Y0, Y0
	VMAXPD 32(DI)(DX*8), Y1, Y1
	VMAXPD 64(DI)(DX*8), Y2, Y2
	VMAXPD 96(DI)(DX*8), Y3, Y3
	VMAXPD 128(DI)(DX*8), Y0, Y0
	VMAXPD 160(DI)(DX*8), Y1, Y1
	VMAXPD 192(DI)(DX*8), Y2, Y2
	VMAXPD 224(DI)(DX*8), Y3, Y3
	ADDQ   $+32, DX
	ADDQ   $-2, CX
	JNE    body
	TESTB  $0x01, R8
	JE     combinevectors

tail:
	VMAXPD (DI)(DX*8), Y0, Y0
	VMAXPD 32(DI)(DX*8), Y1, Y1
	VMAXPD 64(DI)(DX*8), Y1, Y1
	VMAXPD 96(DI)(DX*8), Y1, Y1

combinevectors:
	VMAXPD       Y3, Y0, Y0
	VMAXPD       Y2, Y1, Y1
	VMAXPD       Y0, Y1, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VMAXPD       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VMAXSD       X1, X0, X0
	CMPQ         AX, SI
	JE           return

collect:
	VMAXSD (DI)(AX*8), X0, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    collect

return:
	VZEROUPPER
	MOVSD X0, ret+24(FP)
	RET

setmin:
	VBROADCASTSD dataMaxF64<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPD      Y0, Y1
	VMOVAPD      Y0, Y2
	VMOVAPD      Y0, Y3
	TESTB        $0x01, R8
	JNE          tail
	JMP          combinevectors

DATA dataMaxF32<>+0(SB)/4, $0xff7fffff
GLOBL dataMaxF32<>(SB), RODATA|NOPTR, $4

// func Max_AVX2_F32(x []float32) float32
// Requires: AVX, SSE
TEXT ·Max_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ   x_base+0(FP), DI
	MOVQ   x_len+8(FP), SI
	TESTQ  SI, SI
	JE     empty
	CMPQ   SI, $0x20
	JAE    loop
	VMOVSS dataMaxF32<>+0(SB), X0
	XORL   AX, AX
	JMP    collect

empty:
	VMOVSS dataMaxF32<>+0(SB), X0
	MOVSS  X0, ret+24(FP)
	RET

loop:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	LEAQ         -32(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x05, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           setmin
	MOVQ         R8, CX
	ANDQ         $-2, CX
	VBROADCASTSS dataMaxF32<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPD      Y0, Y1
	VMOVAPD      Y0, Y2
	VMOVAPD      Y0, Y3

body:
	VMAXPS (DI)(DX*4), Y0, Y0
	VMAXPS 32(DI)(DX*4), Y1, Y1
	VMAXPS 64(DI)(DX*4), Y2, Y2
	VMAXPS 96(DI)(DX*4), Y3, Y3
	VMAXPS 128(DI)(DX*4), Y0, Y0
	VMAXPS 160(DI)(DX*4), Y1, Y1
	VMAXPS 192(DI)(DX*4), Y2, Y2
	VMAXPS 224(DI)(DX*4), Y3, Y3
	ADDQ   $+64, DX
	ADDQ   $-2, CX
	JNE    body
	TESTB  $0x01, R8
	JE     combinevectors

tail:
	VMAXPS (DI)(DX*4), Y0, Y0
	VMAXPS 32(DI)(DX*4), Y1, Y1
	VMAXPS 64(DI)(DX*4), Y1, Y1
	VMAXPS 96(DI)(DX*4), Y1, Y1

combinevectors:
	VMAXPS       Y3, Y0, Y0
	VMAXPS       Y2, Y1, Y1
	VMAXPS       Y0, Y1, Y0
	VEXTRACTF128 $0x01, Y0, X1
	VMAXPS       X1, X0, X0
	VPERMILPD    $0x01, X0, X1
	VMAXPS       X1, X0, X0
	VMOVSHDUP    X0, X1
	VMAXSS       X1, X0, X0
	CMPQ         AX, SI
	JE           return

collect:
	VMAXSS (DI)(AX*4), X0, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    collect

return:
	VZEROUPPER
	MOVSS X0, ret+24(FP)
	RET

setmin:
	VBROADCASTSS dataMaxF32<>+0(SB), Y0
	XORL         DX, DX
	VMOVAPS      Y0, Y1
	VMOVAPS      Y0, Y2
	VMOVAPS      Y0, Y3
	TESTB        $0x01, R8
	JNE          tail
	JMP          combinevectors

// func Maximum_AVX2_F64(x []float64, y []float64)
// Requires: AVX
TEXT ·Maximum_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    return
	CMPQ  DX, $0x10
	JAE   loop
	XORL  AX, AX
	JMP   tailbody

loop:
	MOVQ DX, AX
	ANDQ $-16, AX
	LEAQ 96(DI), R8
	XORL CX, CX

body:
	VMOVUPD    (SI)(CX*8), Y0
	VMOVUPD    32(SI)(CX*8), Y1
	VMOVUPD    64(SI)(CX*8), Y2
	VMOVUPD    96(SI)(CX*8), Y3
	VMOVUPD    -96(R8)(CX*8), Y4
	VMOVUPD    -64(R8)(CX*8), Y5
	VMOVUPD    -32(R8)(CX*8), Y6
	VMOVUPD    (R8)(CX*8), Y7
	VCMPPD     $0x01, Y0, Y4, Y4
	VMASKMOVPD Y0, Y4, -96(R8)(CX*8)
	VCMPPD     $0x01, Y1, Y5, Y0
	VMASKMOVPD Y1, Y0, -64(R8)(CX*8)
	VCMPPD     $0x01, Y2, Y6, Y0
	VMASKMOVPD Y2, Y0, -32(R8)(CX*8)
	VCMPPD     $0x01, Y3, Y7, Y0
	VMASKMOVPD Y3, Y0, (R8)(CX*8)
	ADDQ       $0x10, CX
	CMPQ       CX, AX
	JNE        body
	CMPQ       DX, AX
	JNE        tailbody

return:
	VZEROUPPER
	RET

tail:
	ADDQ $0x01, AX
	CMPQ AX, DX
	JE   return

tailbody:
	VMOVSD   (SI)(AX*8), X0
	VUCOMISD (DI)(AX*8), X0
	JBE      tail
	VMOVSD   X0, (DI)(AX*8)
	JMP      tail

// func Maximum_AVX2_F32(x []float32, y []float32)
// Requires: AVX
TEXT ·Maximum_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    return
	CMPQ  DX, $0x20
	JAE   loop
	XORL  AX, AX
	JMP   tailbody

loop:
	MOVQ DX, AX
	ANDQ $-32, AX
	LEAQ 96(DI), R8
	XORL CX, CX

body:
	VMOVUPS    (SI)(CX*4), Y0
	VMOVUPS    32(SI)(CX*4), Y1
	VMOVUPS    64(SI)(CX*4), Y2
	VMOVUPS    96(SI)(CX*4), Y3
	VMOVUPS    -96(R8)(CX*4), Y4
	VMOVUPS    -64(R8)(CX*4), Y5
	VMOVUPS    -32(R8)(CX*4), Y6
	VMOVUPS    (R8)(CX*4), Y7
	VCMPPS     $0x01, Y0, Y4, Y4
	VMASKMOVPS Y0, Y4, -96(R8)(CX*4)
	VCMPPS     $0x01, Y1, Y5, Y0
	VMASKMOVPS Y1, Y0, -64(R8)(CX*4)
	VCMPPS     $0x01, Y2, Y6, Y0
	VMASKMOVPS Y2, Y0, -32(R8)(CX*4)
	VCMPPS     $0x01, Y3, Y7, Y0
	VMASKMOVPS Y3, Y0, (R8)(CX*4)
	ADDQ       $0x20, CX
	CMPQ       CX, AX
	JNE        body
	CMPQ       DX, AX
	JNE        tailbody

return:
	VZEROUPPER
	RET

tail:
	ADDQ $0x01, AX
	CMPQ AX, DX
	JE   return

tailbody:
	VMOVSS   (SI)(AX*4), X0
	VUCOMISS (DI)(AX*4), X0
	JBE      tail
	VMOVSS   X0, (DI)(AX*4)
	JMP      tail

// func MaximumNumber_AVX2_F64(x []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·MaximumNumber_AVX2_F64(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVSD a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    return
	CMPQ  SI, $0x10
	JAE   loop
	XORL  AX, AX
	JMP   tailbody

loop:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	LEAQ         96(DI), CX
	XORL         DX, DX

body:
	VMOVUPD    -96(CX)(DX*8), Y2
	VMOVUPD    -64(CX)(DX*8), Y3
	VMOVUPD    -32(CX)(DX*8), Y4
	VMOVUPD    (CX)(DX*8), Y5
	VCMPPD     $0x01, Y1, Y2, Y2
	VMASKMOVPD Y1, Y2, -96(CX)(DX*8)
	VCMPPD     $0x01, Y1, Y3, Y2
	VMASKMOVPD Y1, Y2, -64(CX)(DX*8)
	VCMPPD     $0x01, Y1, Y4, Y2
	VMASKMOVPD Y1, Y2, -32(CX)(DX*8)
	VCMPPD     $0x01, Y1, Y5, Y2
	VMASKMOVPD Y1, Y2, (CX)(DX*8)
	ADDQ       $0x10, DX
	CMPQ       AX, DX
	JNE        body
	CMPQ       AX, SI
	JNE        tailbody

return:
	VZEROUPPER
	RET

tail:
	ADDQ $0x01, AX
	CMPQ SI, AX
	JE   return

tailbody:
	VUCOMISD (DI)(AX*8), X0
	JBE      tail
	VMOVSD   X0, (DI)(AX*8)
	JMP      tail

// func MaximumNumber_AVX2_F32(x []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·MaximumNumber_AVX2_F32(SB), NOSPLIT, $0-28
	MOVQ  x_base+0(FP), DI
	MOVSS a+24(FP), X0
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    return
	CMPQ  SI, $0x20
	JAE   loop
	XORL  AX, AX
	JMP   tailbody

loop:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	LEAQ         96(DI), CX
	XORL         DX, DX

body:
	VMOVUPS    -96(CX)(DX*4), Y2
	VMOVUPS    -64(CX)(DX*4), Y3
	VMOVUPS    -32(CX)(DX*4), Y4
	VMOVUPS    (CX)(DX*4), Y5
	VCMPPS     $0x01, Y1, Y2, Y2
	VMASKMOVPS Y1, Y2, -96(CX)(DX*4)
	VCMPPS     $0x01, Y1, Y3, Y2
	VMASKMOVPS Y1, Y2, -64(CX)(DX*4)
	VCMPPS     $0x01, Y1, Y4, Y2
	VMASKMOVPS Y1, Y2, -32(CX)(DX*4)
	VCMPPS     $0x01, Y1, Y5, Y2
	VMASKMOVPS Y1, Y2, (CX)(DX*4)
	ADDQ       $0x20, DX
	CMPQ       AX, DX
	JNE        body
	CMPQ       AX, SI
	JNE        tailbody

return:
	VZEROUPPER
	RET

tail:
	ADDQ $0x01, AX
	CMPQ SI, AX
	JE   return

tailbody:
	VUCOMISS (DI)(AX*4), X0
	JBE      tail
	VMOVSS   X0, (DI)(AX*4)
	JMP      tail

// func Find_AVX2_F64(x []float64, a float64) int
// Requires: AVX, AVX2, SSE2
TEXT ·Find_AVX2_F64(SB), NOSPLIT, $0-40
	MOVQ         x_base+0(FP), DI
	MOVSD        a+24(FP), X0
	MOVQ         x_len+8(FP), SI
	MOVQ         SI, CX
	ANDQ         $-8, CX
	JE           tail
	VPBROADCASTQ X0, Y1
	XORL         AX, AX

loop:
	VPCMPEQQ (DI)(AX*8), Y1, Y2
	VPCMPEQQ 32(DI)(AX*8), Y1, Y3
	VPOR     Y2, Y3, Y4
	VPTEST   Y4, Y4
	JNE      mask
	ADDQ     $0x08, AX
	CMPQ     AX, CX
	JB       loop
	CMPQ     AX, SI
	JB       tailbody

return:
	VZEROUPPER
	MOVQ AX, ret+32(FP)
	RET

tail:
	XORL AX, AX
	CMPQ AX, SI
	JAE  return

tailbody:
	VUCOMISD (DI)(AX*8), X0
	JE       return
	ADDQ     $0x01, AX
	CMPQ     SI, AX
	JNE      tailbody
	MOVQ     SI, AX
	VZEROUPPER
	MOVQ     AX, ret+32(FP)
	RET

mask:
	VMOVMSKPD Y3, CX
	SHLL      $0x04, CX
	VMOVMSKPD Y2, DX
	ORL       CX, DX
	BSFL      DX, CX
	ADDQ      CX, AX
	VZEROUPPER
	MOVQ      AX, ret+32(FP)
	RET

// func Find_AVX2_F32(x []float32, a float32) int
// Requires: AVX, AVX2, SSE
TEXT ·Find_AVX2_F32(SB), NOSPLIT, $0-40
	MOVQ         x_base+0(FP), DI
	MOVSS        a+24(FP), X0
	MOVQ         x_len+8(FP), SI
	MOVQ         SI, CX
	ANDQ         $-16, CX
	JE           tail
	VPBROADCASTD X0, Y1
	XORL         AX, AX

loop:
	VPCMPEQD (DI)(AX*4), Y1, Y2
	VPCMPEQD 32(DI)(AX*4), Y1, Y3
	VPOR     Y2, Y3, Y4
	VPTEST   Y4, Y4
	JNE      mask
	ADDQ     $0x10, AX
	CMPQ     AX, CX
	JB       loop
	CMPQ     AX, SI
	JB       tailbody

return:
	VZEROUPPER
	MOVQ AX, ret+32(FP)
	RET

tail:
	XORL AX, AX
	CMPQ AX, SI
	JAE  return

tailbody:
	VUCOMISS (DI)(AX*4), X0
	JE       return
	ADDQ     $+1, AX
	CMPQ     SI, AX
	JNE      tailbody
	MOVQ     SI, AX
	VZEROUPPER
	MOVQ     AX, ret+32(FP)
	RET

mask:
	VMOVMSKPS Y3, CX
	SHLL      $0x08, CX
	VMOVMSKPS Y2, DX
	ORL       CX, DX
	BSFL      DX, CX
	ADDQ      CX, AX
	VZEROUPPER
	MOVQ      AX, ret+32(FP)
	RET

DATA dataLtF64<>+0(SB)/1, $0x01
DATA dataLtF64<>+1(SB)/1, $0x01
DATA dataLtF64<>+2(SB)/1, $0x01
DATA dataLtF64<>+3(SB)/1, $0x01
DATA dataLtF64<>+4(SB)/1, $0x00
DATA dataLtF64<>+5(SB)/1, $0x00
DATA dataLtF64<>+6(SB)/1, $0x00
DATA dataLtF64<>+7(SB)/1, $0x00
DATA dataLtF64<>+8(SB)/1, $0x00
DATA dataLtF64<>+9(SB)/1, $0x00
DATA dataLtF64<>+10(SB)/1, $0x00
DATA dataLtF64<>+11(SB)/1, $0x00
DATA dataLtF64<>+12(SB)/1, $0x00
DATA dataLtF64<>+13(SB)/1, $0x00
DATA dataLtF64<>+14(SB)/1, $0x00
DATA dataLtF64<>+15(SB)/1, $0x00
GLOBL dataLtF64<>(SB), RODATA|NOPTR, $16

// func Lt_AVX2_F64(x []bool, y []float64, z []float64)
// Requires: AVX, AVX2
TEXT ·Lt_AVX2_F64(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB0_7
	CMPQ  CX, $0x10
	JAE   LBB0_3
	XORL  R8, R8
	JMP   LBB0_6

LBB0_3:
	MOVQ    CX, R8
	ANDQ    $-16, R8
	XORL    AX, AX
	VMOVDQU dataLtF64<>+0(SB), X0

LBB0_4:
	VMOVUPD      (SI)(AX*8), Y1
	VMOVUPD      32(SI)(AX*8), Y2
	VMOVUPD      64(SI)(AX*8), Y3
	VMOVUPD      96(SI)(AX*8), Y4
	VCMPPD       $0x01, (DX)(AX*8), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSDW    X1, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPD       $0x01, 32(DX)(AX*8), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x01, 64(DX)(AX*8), Y3, Y3
	VPUNPCKLDQ   X2, X1, X1
	VEXTRACTF128 $0x01, Y3, X2
	VPACKSSDW    X2, X3, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x01, 96(DX)(AX*8), Y4, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X0, X3, X3
	VPBROADCASTD X3, X3
	VPBROADCASTD X2, X2
	VPUNPCKLDQ   X3, X2, X2
	VPBLENDD     $0x0c, X2, X1, X1
	VMOVDQU      X1, (DI)(AX*1)
	ADDQ         $0x10, AX
	CMPQ         R8, AX
	JNE          LBB0_4
	CMPQ         R8, CX
	JE           LBB0_7

LBB0_6:
	VMOVSD   (SI)(R8*8), X0
	VUCOMISD (DX)(R8*8), X0
	SETCS    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB0_6

LBB0_7:
	VZEROUPPER
	RET

DATA dataLtF32<>+0(SB)/1, $0x01
DATA dataLtF32<>+1(SB)/1, $0x01
DATA dataLtF32<>+2(SB)/1, $0x01
DATA dataLtF32<>+3(SB)/1, $0x01
DATA dataLtF32<>+4(SB)/1, $0x01
DATA dataLtF32<>+5(SB)/1, $0x01
DATA dataLtF32<>+6(SB)/1, $0x01
DATA dataLtF32<>+7(SB)/1, $0x01
DATA dataLtF32<>+8(SB)/1, $0x00
DATA dataLtF32<>+9(SB)/1, $0x00
DATA dataLtF32<>+10(SB)/1, $0x00
DATA dataLtF32<>+11(SB)/1, $0x00
DATA dataLtF32<>+12(SB)/1, $0x00
DATA dataLtF32<>+13(SB)/1, $0x00
DATA dataLtF32<>+14(SB)/1, $0x00
DATA dataLtF32<>+15(SB)/1, $0x00
GLOBL dataLtF32<>(SB), RODATA|NOPTR, $16

// func Lt_AVX2_F32(x []bool, y []float32, z []float32)
// Requires: AVX, AVX2
TEXT ·Lt_AVX2_F32(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB1_7
	CMPQ  CX, $0x20
	JAE   LBB1_3
	XORL  R8, R8
	JMP   LBB1_6

LBB1_3:
	MOVQ    CX, R8
	ANDQ    $-32, R8
	XORL    AX, AX
	VMOVDQU dataLtF32<>+0(SB), X0

LBB1_4:
	VMOVUPS      (SI)(AX*4), Y1
	VMOVUPS      32(SI)(AX*4), Y2
	VMOVUPS      64(SI)(AX*4), Y3
	VMOVUPS      96(SI)(AX*4), Y4
	VCMPPS       $0x01, (DX)(AX*4), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPS       $0x01, 32(DX)(AX*4), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPS       $0x01, 64(DX)(AX*4), Y3, Y3
	VEXTRACTF128 $0x01, Y3, X5
	VPACKSSDW    X5, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPS       $0x01, 96(DX)(AX*4), Y4, Y4
	VPAND        X0, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X0, X4, X4
	VINSERTI128  $0x01, X4, Y3, Y3
	VINSERTI128  $0x01, X2, Y1, Y1
	VPUNPCKLQDQ  Y3, Y1, Y1
	VPERMQ       $0xd8, Y1, Y1
	VMOVDQU      Y1, (DI)(AX*1)
	ADDQ         $0x20, AX
	CMPQ         R8, AX
	JNE          LBB1_4
	CMPQ         R8, CX
	JE           LBB1_7

LBB1_6:
	VMOVSS   (SI)(R8*4), X0
	VUCOMISS (DX)(R8*4), X0
	SETCS    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB1_6

LBB1_7:
	VZEROUPPER
	RET

DATA dataLteF64<>+0(SB)/1, $0x01
DATA dataLteF64<>+1(SB)/1, $0x01
DATA dataLteF64<>+2(SB)/1, $0x01
DATA dataLteF64<>+3(SB)/1, $0x01
DATA dataLteF64<>+4(SB)/1, $0x00
DATA dataLteF64<>+5(SB)/1, $0x00
DATA dataLteF64<>+6(SB)/1, $0x00
DATA dataLteF64<>+7(SB)/1, $0x00
DATA dataLteF64<>+8(SB)/1, $0x00
DATA dataLteF64<>+9(SB)/1, $0x00
DATA dataLteF64<>+10(SB)/1, $0x00
DATA dataLteF64<>+11(SB)/1, $0x00
DATA dataLteF64<>+12(SB)/1, $0x00
DATA dataLteF64<>+13(SB)/1, $0x00
DATA dataLteF64<>+14(SB)/1, $0x00
DATA dataLteF64<>+15(SB)/1, $0x00
GLOBL dataLteF64<>(SB), RODATA|NOPTR, $16

// func Lte_AVX2_F64(x []bool, y []float64, z []float64)
// Requires: AVX, AVX2
TEXT ·Lte_AVX2_F64(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB2_7
	CMPQ  CX, $0x10
	JAE   LBB2_3
	XORL  R8, R8
	JMP   LBB2_6

LBB2_3:
	MOVQ    CX, R8
	ANDQ    $-16, R8
	XORL    AX, AX
	VMOVDQU dataLteF64<>+0(SB), X0

LBB2_4:
	VMOVUPD      (SI)(AX*8), Y1
	VMOVUPD      32(SI)(AX*8), Y2
	VMOVUPD      64(SI)(AX*8), Y3
	VMOVUPD      96(SI)(AX*8), Y4
	VCMPPD       $0x02, (DX)(AX*8), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSDW    X1, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPD       $0x02, 32(DX)(AX*8), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x02, 64(DX)(AX*8), Y3, Y3
	VPUNPCKLDQ   X2, X1, X1
	VEXTRACTF128 $0x01, Y3, X2
	VPACKSSDW    X2, X3, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x02, 96(DX)(AX*8), Y4, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X0, X3, X3
	VPBROADCASTD X3, X3
	VPBROADCASTD X2, X2
	VPUNPCKLDQ   X3, X2, X2
	VPBLENDD     $0x0c, X2, X1, X1
	VMOVDQU      X1, (DI)(AX*1)
	ADDQ         $0x10, AX
	CMPQ         R8, AX
	JNE          LBB2_4
	CMPQ         R8, CX
	JE           LBB2_7

LBB2_6:
	VMOVSD   (SI)(R8*8), X0
	VUCOMISD (DX)(R8*8), X0
	SETLS    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB2_6

LBB2_7:
	VZEROUPPER
	RET

DATA dataLteF32<>+0(SB)/1, $0x01
DATA dataLteF32<>+1(SB)/1, $0x01
DATA dataLteF32<>+2(SB)/1, $0x01
DATA dataLteF32<>+3(SB)/1, $0x01
DATA dataLteF32<>+4(SB)/1, $0x01
DATA dataLteF32<>+5(SB)/1, $0x01
DATA dataLteF32<>+6(SB)/1, $0x01
DATA dataLteF32<>+7(SB)/1, $0x01
DATA dataLteF32<>+8(SB)/1, $0x00
DATA dataLteF32<>+9(SB)/1, $0x00
DATA dataLteF32<>+10(SB)/1, $0x00
DATA dataLteF32<>+11(SB)/1, $0x00
DATA dataLteF32<>+12(SB)/1, $0x00
DATA dataLteF32<>+13(SB)/1, $0x00
DATA dataLteF32<>+14(SB)/1, $0x00
DATA dataLteF32<>+15(SB)/1, $0x00
GLOBL dataLteF32<>(SB), RODATA|NOPTR, $16

// func Lte_AVX2_F32(x []bool, y []float32, z []float32)
// Requires: AVX, AVX2
TEXT ·Lte_AVX2_F32(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB3_7
	CMPQ  CX, $0x20
	JAE   LBB3_3
	XORL  R8, R8
	JMP   LBB3_6

LBB3_3:
	MOVQ    CX, R8
	ANDQ    $-32, R8
	XORL    AX, AX
	VMOVDQU dataLteF32<>+0(SB), X0

LBB3_4:
	VMOVUPS      (SI)(AX*4), Y1
	VMOVUPS      32(SI)(AX*4), Y2
	VMOVUPS      64(SI)(AX*4), Y3
	VMOVUPS      96(SI)(AX*4), Y4
	VCMPPS       $0x02, (DX)(AX*4), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPS       $0x02, 32(DX)(AX*4), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPS       $0x02, 64(DX)(AX*4), Y3, Y3
	VEXTRACTF128 $0x01, Y3, X5
	VPACKSSDW    X5, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPS       $0x02, 96(DX)(AX*4), Y4, Y4
	VPAND        X0, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X0, X4, X4
	VINSERTI128  $0x01, X4, Y3, Y3
	VINSERTI128  $0x01, X2, Y1, Y1
	VPUNPCKLQDQ  Y3, Y1, Y1
	VPERMQ       $0xd8, Y1, Y1
	VMOVDQU      Y1, (DI)(AX*1)
	ADDQ         $0x20, AX
	CMPQ         R8, AX
	JNE          LBB3_4
	CMPQ         R8, CX
	JE           LBB3_7

LBB3_6:
	VMOVSS   (SI)(R8*4), X0
	VUCOMISS (DX)(R8*4), X0
	SETLS    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB3_6

LBB3_7:
	VZEROUPPER
	RET

DATA dataGtF64<>+0(SB)/1, $0x01
DATA dataGtF64<>+1(SB)/1, $0x01
DATA dataGtF64<>+2(SB)/1, $0x01
DATA dataGtF64<>+3(SB)/1, $0x01
DATA dataGtF64<>+4(SB)/1, $0x00
DATA dataGtF64<>+5(SB)/1, $0x00
DATA dataGtF64<>+6(SB)/1, $0x00
DATA dataGtF64<>+7(SB)/1, $0x00
DATA dataGtF64<>+8(SB)/1, $0x00
DATA dataGtF64<>+9(SB)/1, $0x00
DATA dataGtF64<>+10(SB)/1, $0x00
DATA dataGtF64<>+11(SB)/1, $0x00
DATA dataGtF64<>+12(SB)/1, $0x00
DATA dataGtF64<>+13(SB)/1, $0x00
DATA dataGtF64<>+14(SB)/1, $0x00
DATA dataGtF64<>+15(SB)/1, $0x00
GLOBL dataGtF64<>(SB), RODATA|NOPTR, $16

// func Gt_AVX2_F64(x []bool, y []float64, z []float64)
// Requires: AVX, AVX2
TEXT ·Gt_AVX2_F64(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB4_7
	CMPQ  CX, $0x10
	JAE   LBB4_3
	XORL  R8, R8
	JMP   LBB4_6

LBB4_3:
	MOVQ    CX, R8
	ANDQ    $-16, R8
	XORL    AX, AX
	VMOVDQU dataGtF64<>+0(SB), X0

LBB4_4:
	VMOVUPD      (DX)(AX*8), Y1
	VMOVUPD      32(DX)(AX*8), Y2
	VMOVUPD      64(DX)(AX*8), Y3
	VMOVUPD      96(DX)(AX*8), Y4
	VCMPPD       $0x01, (SI)(AX*8), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSDW    X1, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPD       $0x01, 32(SI)(AX*8), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x01, 64(SI)(AX*8), Y3, Y3
	VPUNPCKLDQ   X2, X1, X1
	VEXTRACTF128 $0x01, Y3, X2
	VPACKSSDW    X2, X3, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x01, 96(SI)(AX*8), Y4, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X0, X3, X3
	VPBROADCASTD X3, X3
	VPBROADCASTD X2, X2
	VPUNPCKLDQ   X3, X2, X2
	VPBLENDD     $0x0c, X2, X1, X1
	VMOVDQU      X1, (DI)(AX*1)
	ADDQ         $0x10, AX
	CMPQ         R8, AX
	JNE          LBB4_4
	CMPQ         R8, CX
	JE           LBB4_7

LBB4_6:
	VMOVSD   (SI)(R8*8), X0
	VUCOMISD (DX)(R8*8), X0
	SETHI    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB4_6

LBB4_7:
	VZEROUPPER
	RET

DATA dataGtF32<>+0(SB)/1, $0x01
DATA dataGtF32<>+1(SB)/1, $0x01
DATA dataGtF32<>+2(SB)/1, $0x01
DATA dataGtF32<>+3(SB)/1, $0x01
DATA dataGtF32<>+4(SB)/1, $0x01
DATA dataGtF32<>+5(SB)/1, $0x01
DATA dataGtF32<>+6(SB)/1, $0x01
DATA dataGtF32<>+7(SB)/1, $0x01
DATA dataGtF32<>+8(SB)/1, $0x00
DATA dataGtF32<>+9(SB)/1, $0x00
DATA dataGtF32<>+10(SB)/1, $0x00
DATA dataGtF32<>+11(SB)/1, $0x00
DATA dataGtF32<>+12(SB)/1, $0x00
DATA dataGtF32<>+13(SB)/1, $0x00
DATA dataGtF32<>+14(SB)/1, $0x00
DATA dataGtF32<>+15(SB)/1, $0x00
GLOBL dataGtF32<>(SB), RODATA|NOPTR, $16

// func Gt_AVX2_F32(x []bool, y []float32, z []float32)
// Requires: AVX, AVX2
TEXT ·Gt_AVX2_F32(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB5_7
	CMPQ  CX, $0x20
	JAE   LBB5_3
	XORL  R8, R8
	JMP   LBB5_6

LBB5_3:
	MOVQ    CX, R8
	ANDQ    $-32, R8
	XORL    AX, AX
	VMOVDQU dataGtF32<>+0(SB), X0

LBB5_4:
	VMOVUPS      (DX)(AX*4), Y1
	VMOVUPS      32(DX)(AX*4), Y2
	VMOVUPS      64(DX)(AX*4), Y3
	VMOVUPS      96(DX)(AX*4), Y4
	VCMPPS       $0x01, (SI)(AX*4), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPS       $0x01, 32(SI)(AX*4), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPS       $0x01, 64(SI)(AX*4), Y3, Y3
	VEXTRACTF128 $0x01, Y3, X5
	VPACKSSDW    X5, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPS       $0x01, 96(SI)(AX*4), Y4, Y4
	VPAND        X0, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X0, X4, X4
	VINSERTI128  $0x01, X4, Y3, Y3
	VINSERTI128  $0x01, X2, Y1, Y1
	VPUNPCKLQDQ  Y3, Y1, Y1
	VPERMQ       $0xd8, Y1, Y1
	VMOVDQU      Y1, (DI)(AX*1)
	ADDQ         $0x20, AX
	CMPQ         R8, AX
	JNE          LBB5_4
	CMPQ         R8, CX
	JE           LBB5_7

LBB5_6:
	VMOVSS   (SI)(R8*4), X0
	VUCOMISS (DX)(R8*4), X0
	SETHI    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB5_6

LBB5_7:
	VZEROUPPER
	RET

DATA dataGteF64<>+0(SB)/1, $0x01
DATA dataGteF64<>+1(SB)/1, $0x01
DATA dataGteF64<>+2(SB)/1, $0x01
DATA dataGteF64<>+3(SB)/1, $0x01
DATA dataGteF64<>+4(SB)/1, $0x00
DATA dataGteF64<>+5(SB)/1, $0x00
DATA dataGteF64<>+6(SB)/1, $0x00
DATA dataGteF64<>+7(SB)/1, $0x00
DATA dataGteF64<>+8(SB)/1, $0x00
DATA dataGteF64<>+9(SB)/1, $0x00
DATA dataGteF64<>+10(SB)/1, $0x00
DATA dataGteF64<>+11(SB)/1, $0x00
DATA dataGteF64<>+12(SB)/1, $0x00
DATA dataGteF64<>+13(SB)/1, $0x00
DATA dataGteF64<>+14(SB)/1, $0x00
DATA dataGteF64<>+15(SB)/1, $0x00
GLOBL dataGteF64<>(SB), RODATA|NOPTR, $16

// func Gte_AVX2_F64(x []bool, y []float64, z []float64)
// Requires: AVX, AVX2
TEXT ·Gte_AVX2_F64(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB6_7
	CMPQ  CX, $0x10
	JAE   LBB6_3
	XORL  R8, R8
	JMP   LBB6_6

LBB6_3:
	MOVQ    CX, R8
	ANDQ    $-16, R8
	XORL    AX, AX
	VMOVDQU dataGteF64<>+0(SB), X0

LBB6_4:
	VMOVUPD      (DX)(AX*8), Y1
	VMOVUPD      32(DX)(AX*8), Y2
	VMOVUPD      64(DX)(AX*8), Y3
	VMOVUPD      96(DX)(AX*8), Y4
	VCMPPD       $0x02, (SI)(AX*8), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSDW    X1, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPD       $0x02, 32(SI)(AX*8), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x02, 64(SI)(AX*8), Y3, Y3
	VPUNPCKLDQ   X2, X1, X1
	VEXTRACTF128 $0x01, Y3, X2
	VPACKSSDW    X2, X3, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x02, 96(SI)(AX*8), Y4, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X0, X3, X3
	VPBROADCASTD X3, X3
	VPBROADCASTD X2, X2
	VPUNPCKLDQ   X3, X2, X2
	VPBLENDD     $0x0c, X2, X1, X1
	VMOVDQU      X1, (DI)(AX*1)
	ADDQ         $0x10, AX
	CMPQ         R8, AX
	JNE          LBB6_4
	CMPQ         R8, CX
	JE           LBB6_7

LBB6_6:
	VMOVSD   (SI)(R8*8), X0
	VUCOMISD (DX)(R8*8), X0
	SETCC    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB6_6

LBB6_7:
	VZEROUPPER
	RET

DATA dataGteF32<>+0(SB)/1, $0x01
DATA dataGteF32<>+1(SB)/1, $0x01
DATA dataGteF32<>+2(SB)/1, $0x01
DATA dataGteF32<>+3(SB)/1, $0x01
DATA dataGteF32<>+4(SB)/1, $0x01
DATA dataGteF32<>+5(SB)/1, $0x01
DATA dataGteF32<>+6(SB)/1, $0x01
DATA dataGteF32<>+7(SB)/1, $0x01
DATA dataGteF32<>+8(SB)/1, $0x00
DATA dataGteF32<>+9(SB)/1, $0x00
DATA dataGteF32<>+10(SB)/1, $0x00
DATA dataGteF32<>+11(SB)/1, $0x00
DATA dataGteF32<>+12(SB)/1, $0x00
DATA dataGteF32<>+13(SB)/1, $0x00
DATA dataGteF32<>+14(SB)/1, $0x00
DATA dataGteF32<>+15(SB)/1, $0x00
GLOBL dataGteF32<>(SB), RODATA|NOPTR, $16

// func Gte_AVX2_F32(x []bool, y []float32, z []float32)
// Requires: AVX, AVX2
TEXT ·Gte_AVX2_F32(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB7_7
	CMPQ  CX, $0x20
	JAE   LBB7_3
	XORL  R8, R8
	JMP   LBB7_6

LBB7_3:
	MOVQ    CX, R8
	ANDQ    $-32, R8
	XORL    AX, AX
	VMOVDQU dataGteF32<>+0(SB), X0

LBB7_4:
	VMOVUPS      (DX)(AX*4), Y1
	VMOVUPS      32(DX)(AX*4), Y2
	VMOVUPS      64(DX)(AX*4), Y3
	VMOVUPS      96(DX)(AX*4), Y4
	VCMPPS       $0x02, (SI)(AX*4), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPS       $0x02, 32(SI)(AX*4), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPS       $0x02, 64(SI)(AX*4), Y3, Y3
	VEXTRACTF128 $0x01, Y3, X5
	VPACKSSDW    X5, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPS       $0x02, 96(SI)(AX*4), Y4, Y4
	VPAND        X0, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X0, X4, X4
	VINSERTI128  $0x01, X4, Y3, Y3
	VINSERTI128  $0x01, X2, Y1, Y1
	VPUNPCKLQDQ  Y3, Y1, Y1
	VPERMQ       $0xd8, Y1, Y1
	VMOVDQU      Y1, (DI)(AX*1)
	ADDQ         $0x20, AX
	CMPQ         R8, AX
	JNE          LBB7_4
	CMPQ         R8, CX
	JE           LBB7_7

LBB7_6:
	VMOVSS   (SI)(R8*4), X0
	VUCOMISS (DX)(R8*4), X0
	SETCC    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB7_6

LBB7_7:
	VZEROUPPER
	RET

DATA dataEqF64<>+0(SB)/1, $0x01
DATA dataEqF64<>+1(SB)/1, $0x01
DATA dataEqF64<>+2(SB)/1, $0x01
DATA dataEqF64<>+3(SB)/1, $0x01
DATA dataEqF64<>+4(SB)/1, $0x00
DATA dataEqF64<>+5(SB)/1, $0x00
DATA dataEqF64<>+6(SB)/1, $0x00
DATA dataEqF64<>+7(SB)/1, $0x00
DATA dataEqF64<>+8(SB)/1, $0x00
DATA dataEqF64<>+9(SB)/1, $0x00
DATA dataEqF64<>+10(SB)/1, $0x00
DATA dataEqF64<>+11(SB)/1, $0x00
DATA dataEqF64<>+12(SB)/1, $0x00
DATA dataEqF64<>+13(SB)/1, $0x00
DATA dataEqF64<>+14(SB)/1, $0x00
DATA dataEqF64<>+15(SB)/1, $0x00
GLOBL dataEqF64<>(SB), RODATA|NOPTR, $16

// func Eq_AVX2_F64(x []bool, y []float64, z []float64)
// Requires: AVX, AVX2
TEXT ·Eq_AVX2_F64(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB8_7
	CMPQ  CX, $0x10
	JAE   LBB8_3
	XORL  R8, R8
	JMP   LBB8_6

LBB8_3:
	MOVQ    CX, R8
	ANDQ    $-16, R8
	XORL    AX, AX
	VMOVDQU dataEqF64<>+0(SB), X0

LBB8_4:
	VMOVUPD      (DX)(AX*8), Y1
	VMOVUPD      32(DX)(AX*8), Y2
	VMOVUPD      64(DX)(AX*8), Y3
	VMOVUPD      96(DX)(AX*8), Y4
	VCMPPD       $0x00, (SI)(AX*8), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSDW    X1, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPD       $0x00, 32(SI)(AX*8), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x00, 64(SI)(AX*8), Y3, Y3
	VPUNPCKLDQ   X2, X1, X1
	VEXTRACTF128 $0x01, Y3, X2
	VPACKSSDW    X2, X3, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x00, 96(SI)(AX*8), Y4, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X0, X3, X3
	VPBROADCASTD X3, X3
	VPBROADCASTD X2, X2
	VPUNPCKLDQ   X3, X2, X2
	VPBLENDD     $0x0c, X2, X1, X1
	VMOVDQU      X1, (DI)(AX*1)
	ADDQ         $0x10, AX
	CMPQ         R8, AX
	JNE          LBB8_4
	CMPQ         R8, CX
	JE           LBB8_7

LBB8_6:
	VMOVSD   (SI)(R8*8), X0
	VUCOMISD (DX)(R8*8), X0
	SETEQ    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB8_6

LBB8_7:
	VZEROUPPER
	RET

DATA dataEqF32<>+0(SB)/1, $0x01
DATA dataEqF32<>+1(SB)/1, $0x01
DATA dataEqF32<>+2(SB)/1, $0x01
DATA dataEqF32<>+3(SB)/1, $0x01
DATA dataEqF32<>+4(SB)/1, $0x01
DATA dataEqF32<>+5(SB)/1, $0x01
DATA dataEqF32<>+6(SB)/1, $0x01
DATA dataEqF32<>+7(SB)/1, $0x01
DATA dataEqF32<>+8(SB)/1, $0x00
DATA dataEqF32<>+9(SB)/1, $0x00
DATA dataEqF32<>+10(SB)/1, $0x00
DATA dataEqF32<>+11(SB)/1, $0x00
DATA dataEqF32<>+12(SB)/1, $0x00
DATA dataEqF32<>+13(SB)/1, $0x00
DATA dataEqF32<>+14(SB)/1, $0x00
DATA dataEqF32<>+15(SB)/1, $0x00
GLOBL dataEqF32<>(SB), RODATA|NOPTR, $16

// func Eq_AVX2_F32(x []bool, y []float32, z []float32)
// Requires: AVX, AVX2
TEXT ·Eq_AVX2_F32(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB9_7
	CMPQ  CX, $0x20
	JAE   LBB9_3
	XORL  R8, R8
	JMP   LBB9_6

LBB9_3:
	MOVQ    CX, R8
	ANDQ    $-32, R8
	XORL    AX, AX
	VMOVDQU dataEqF32<>+0(SB), X0

LBB9_4:
	VMOVUPS      (DX)(AX*4), Y1
	VMOVUPS      32(DX)(AX*4), Y2
	VMOVUPS      64(DX)(AX*4), Y3
	VMOVUPS      96(DX)(AX*4), Y4
	VCMPPS       $0x00, (SI)(AX*4), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPS       $0x00, 32(SI)(AX*4), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPS       $0x00, 64(SI)(AX*4), Y3, Y3
	VEXTRACTF128 $0x01, Y3, X5
	VPACKSSDW    X5, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPS       $0x00, 96(SI)(AX*4), Y4, Y4
	VPAND        X0, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X0, X4, X4
	VINSERTI128  $0x01, X4, Y3, Y3
	VINSERTI128  $0x01, X2, Y1, Y1
	VPUNPCKLQDQ  Y3, Y1, Y1
	VPERMQ       $0xd8, Y1, Y1
	VMOVDQU      Y1, (DI)(AX*1)
	ADDQ         $0x20, AX
	CMPQ         R8, AX
	JNE          LBB9_4
	CMPQ         R8, CX
	JE           LBB9_7

LBB9_6:
	VMOVSS   (SI)(R8*4), X0
	VUCOMISS (DX)(R8*4), X0
	SETEQ    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB9_6

LBB9_7:
	VZEROUPPER
	RET

DATA dataNeqF64<>+0(SB)/1, $0x01
DATA dataNeqF64<>+1(SB)/1, $0x01
DATA dataNeqF64<>+2(SB)/1, $0x01
DATA dataNeqF64<>+3(SB)/1, $0x01
DATA dataNeqF64<>+4(SB)/1, $0x00
DATA dataNeqF64<>+5(SB)/1, $0x00
DATA dataNeqF64<>+6(SB)/1, $0x00
DATA dataNeqF64<>+7(SB)/1, $0x00
DATA dataNeqF64<>+8(SB)/1, $0x00
DATA dataNeqF64<>+9(SB)/1, $0x00
DATA dataNeqF64<>+10(SB)/1, $0x00
DATA dataNeqF64<>+11(SB)/1, $0x00
DATA dataNeqF64<>+12(SB)/1, $0x00
DATA dataNeqF64<>+13(SB)/1, $0x00
DATA dataNeqF64<>+14(SB)/1, $0x00
DATA dataNeqF64<>+15(SB)/1, $0x00
GLOBL dataNeqF64<>(SB), RODATA|NOPTR, $16

// func Neq_AVX2_F64(x []bool, y []float64, z []float64)
// Requires: AVX, AVX2
TEXT ·Neq_AVX2_F64(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB10_7
	CMPQ  CX, $0x10
	JAE   LBB10_3
	XORL  R8, R8
	JMP   LBB10_6

LBB10_3:
	MOVQ    CX, R8
	ANDQ    $-16, R8
	XORL    AX, AX
	VMOVDQU dataNeqF64<>+0(SB), X0

LBB10_4:
	VMOVUPD      (DX)(AX*8), Y1
	VMOVUPD      32(DX)(AX*8), Y2
	VMOVUPD      64(DX)(AX*8), Y3
	VMOVUPD      96(DX)(AX*8), Y4
	VCMPPD       $0x04, (SI)(AX*8), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSDW    X1, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPD       $0x04, 32(SI)(AX*8), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x04, 64(SI)(AX*8), Y3, Y3
	VPUNPCKLDQ   X2, X1, X1
	VEXTRACTF128 $0x01, Y3, X2
	VPACKSSDW    X2, X3, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPD       $0x04, 96(SI)(AX*8), Y4, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X0, X3, X3
	VPBROADCASTD X3, X3
	VPBROADCASTD X2, X2
	VPUNPCKLDQ   X3, X2, X2
	VPBLENDD     $0x0c, X2, X1, X1
	VMOVDQU      X1, (DI)(AX*1)
	ADDQ         $0x10, AX
	CMPQ         R8, AX
	JNE          LBB10_4
	CMPQ         R8, CX
	JE           LBB10_7

LBB10_6:
	VMOVSD   (SI)(R8*8), X0
	VUCOMISD (DX)(R8*8), X0
	SETNE    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB10_6

LBB10_7:
	VZEROUPPER
	RET

DATA dataNeqF32<>+0(SB)/1, $0x01
DATA dataNeqF32<>+1(SB)/1, $0x01
DATA dataNeqF32<>+2(SB)/1, $0x01
DATA dataNeqF32<>+3(SB)/1, $0x01
DATA dataNeqF32<>+4(SB)/1, $0x01
DATA dataNeqF32<>+5(SB)/1, $0x01
DATA dataNeqF32<>+6(SB)/1, $0x01
DATA dataNeqF32<>+7(SB)/1, $0x01
DATA dataNeqF32<>+8(SB)/1, $0x00
DATA dataNeqF32<>+9(SB)/1, $0x00
DATA dataNeqF32<>+10(SB)/1, $0x00
DATA dataNeqF32<>+11(SB)/1, $0x00
DATA dataNeqF32<>+12(SB)/1, $0x00
DATA dataNeqF32<>+13(SB)/1, $0x00
DATA dataNeqF32<>+14(SB)/1, $0x00
DATA dataNeqF32<>+15(SB)/1, $0x00
GLOBL dataNeqF32<>(SB), RODATA|NOPTR, $16

// func Neq_AVX2_F32(x []bool, y []float32, z []float32)
// Requires: AVX, AVX2
TEXT ·Neq_AVX2_F32(SB), NOSPLIT, $0-72
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  z_base+48(FP), DX
	MOVQ  x_len+8(FP), CX
	TESTQ CX, CX
	JE    LBB11_7
	CMPQ  CX, $0x20
	JAE   LBB11_3
	XORL  R8, R8
	JMP   LBB11_6

LBB11_3:
	MOVQ    CX, R8
	ANDQ    $-32, R8
	XORL    AX, AX
	VMOVDQU dataNeqF32<>+0(SB), X0

LBB11_4:
	VMOVUPS      (DX)(AX*4), Y1
	VMOVUPS      32(DX)(AX*4), Y2
	VMOVUPS      64(DX)(AX*4), Y3
	VMOVUPS      96(DX)(AX*4), Y4
	VCMPPS       $0x04, (SI)(AX*4), Y1, Y1
	VEXTRACTF128 $0x01, Y1, X5
	VPACKSSDW    X5, X1, X1
	VPACKSSWB    X1, X1, X1
	VCMPPS       $0x04, 32(SI)(AX*4), Y2, Y2
	VPAND        X0, X1, X1
	VEXTRACTF128 $0x01, Y2, X5
	VPACKSSDW    X5, X2, X2
	VPACKSSWB    X2, X2, X2
	VPAND        X0, X2, X2
	VCMPPS       $0x04, 64(SI)(AX*4), Y3, Y3
	VEXTRACTF128 $0x01, Y3, X5
	VPACKSSDW    X5, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPS       $0x04, 96(SI)(AX*4), Y4, Y4
	VPAND        X0, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X0, X4, X4
	VINSERTI128  $0x01, X4, Y3, Y3
	VINSERTI128  $0x01, X2, Y1, Y1
	VPUNPCKLQDQ  Y3, Y1, Y1
	VPERMQ       $0xd8, Y1, Y1
	VMOVDQU      Y1, (DI)(AX*1)
	ADDQ         $0x20, AX
	CMPQ         R8, AX
	JNE          LBB11_4
	CMPQ         R8, CX
	JE           LBB11_7

LBB11_6:
	VMOVSS   (SI)(R8*4), X0
	VUCOMISS (DX)(R8*4), X0
	SETNE    (DI)(R8*1)
	ADDQ     $0x01, R8
	CMPQ     CX, R8
	JNE      LBB11_6

LBB11_7:
	VZEROUPPER
	RET

DATA dataLtNumberF64<>+0(SB)/1, $0x01
DATA dataLtNumberF64<>+1(SB)/1, $0x01
DATA dataLtNumberF64<>+2(SB)/1, $0x01
DATA dataLtNumberF64<>+3(SB)/1, $0x01
DATA dataLtNumberF64<>+4(SB)/1, $0x00
DATA dataLtNumberF64<>+5(SB)/1, $0x00
DATA dataLtNumberF64<>+6(SB)/1, $0x00
DATA dataLtNumberF64<>+7(SB)/1, $0x00
DATA dataLtNumberF64<>+8(SB)/1, $0x00
DATA dataLtNumberF64<>+9(SB)/1, $0x00
DATA dataLtNumberF64<>+10(SB)/1, $0x00
DATA dataLtNumberF64<>+11(SB)/1, $0x00
DATA dataLtNumberF64<>+12(SB)/1, $0x00
DATA dataLtNumberF64<>+13(SB)/1, $0x00
DATA dataLtNumberF64<>+14(SB)/1, $0x00
DATA dataLtNumberF64<>+15(SB)/1, $0x00
GLOBL dataLtNumberF64<>(SB), RODATA|NOPTR, $16

// func LtNumber_AVX2_F64(x []bool, y []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·LtNumber_AVX2_F64(SB), NOSPLIT, $0-56
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSD a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB12_7
	CMPQ  DX, $0x10
	JAE   LBB12_3
	XORL  AX, AX
	JMP   LBB12_6

LBB12_3:
	MOVQ         DX, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	XORL         CX, CX
	VMOVDQU      dataLtNumberF64<>+0(SB), X2

LBB12_4:
	VMOVUPD      (SI)(CX*8), Y3
	VMOVUPD      32(SI)(CX*8), Y4
	VMOVUPD      64(SI)(CX*8), Y5
	VMOVUPD      96(SI)(CX*8), Y6
	VCMPPD       $0x01, Y1, Y3, Y3
	VEXTRACTF128 $0x01, Y3, X7
	VPACKSSDW    X7, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X2, X3, X3
	VCMPPD       $0x01, Y1, Y4, Y4
	VEXTRACTF128 $0x01, Y4, X7
	VPACKSSDW    X7, X4, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VPUNPCKLDQ   X4, X3, X3
	VCMPPD       $0x01, Y1, Y5, Y4
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPD       $0x01, Y1, Y6, Y5
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSDW    X5, X5, X5
	VPACKSSWB    X5, X5, X5
	VPAND        X2, X5, X5
	VPBROADCASTD X5, X5
	VPBROADCASTD X4, X4
	VPUNPCKLDQ   X5, X4, X4
	VPBLENDD     $0x0c, X4, X3, X3
	VMOVDQU      X3, (DI)(CX*1)
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB12_4
	CMPQ         AX, DX
	JE           LBB12_7

LBB12_6:
	VUCOMISD (SI)(AX*8), X0
	SETHI    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB12_6

LBB12_7:
	VZEROUPPER
	RET

DATA dataLtNumberF32<>+0(SB)/1, $0x01
DATA dataLtNumberF32<>+1(SB)/1, $0x01
DATA dataLtNumberF32<>+2(SB)/1, $0x01
DATA dataLtNumberF32<>+3(SB)/1, $0x01
DATA dataLtNumberF32<>+4(SB)/1, $0x01
DATA dataLtNumberF32<>+5(SB)/1, $0x01
DATA dataLtNumberF32<>+6(SB)/1, $0x01
DATA dataLtNumberF32<>+7(SB)/1, $0x01
DATA dataLtNumberF32<>+8(SB)/1, $0x00
DATA dataLtNumberF32<>+9(SB)/1, $0x00
DATA dataLtNumberF32<>+10(SB)/1, $0x00
DATA dataLtNumberF32<>+11(SB)/1, $0x00
DATA dataLtNumberF32<>+12(SB)/1, $0x00
DATA dataLtNumberF32<>+13(SB)/1, $0x00
DATA dataLtNumberF32<>+14(SB)/1, $0x00
DATA dataLtNumberF32<>+15(SB)/1, $0x00
GLOBL dataLtNumberF32<>(SB), RODATA|NOPTR, $16

// func LtNumber_AVX2_F32(x []bool, y []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·LtNumber_AVX2_F32(SB), NOSPLIT, $0-52
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSS a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB13_7
	CMPQ  DX, $0x20
	JAE   LBB13_3
	XORL  AX, AX
	JMP   LBB13_6

LBB13_3:
	MOVQ         DX, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	XORL         CX, CX
	VMOVDQU      dataLtNumberF32<>+0(SB), X2

LBB13_4:
	VMOVUPS      (SI)(CX*4), Y3
	VMOVUPS      32(SI)(CX*4), Y4
	VMOVUPS      64(SI)(CX*4), Y5
	VMOVUPS      96(SI)(CX*4), Y6
	VCMPPS       $0x01, Y1, Y3, Y3
	VEXTRACTF128 $0x01, Y3, X7
	VPACKSSDW    X7, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X2, X3, X3
	VCMPPS       $0x01, Y1, Y4, Y4
	VEXTRACTF128 $0x01, Y4, X7
	VPACKSSDW    X7, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPS       $0x01, Y1, Y5, Y5
	VEXTRACTF128 $0x01, Y5, X7
	VPACKSSDW    X7, X5, X5
	VPACKSSWB    X5, X5, X5
	VPAND        X2, X5, X5
	VCMPPS       $0x01, Y1, Y6, Y6
	VEXTRACTF128 $0x01, Y6, X7
	VPACKSSDW    X7, X6, X6
	VPACKSSWB    X6, X6, X6
	VPAND        X2, X6, X6
	VINSERTI128  $0x01, X6, Y5, Y5
	VINSERTI128  $0x01, X4, Y3, Y3
	VPUNPCKLQDQ  Y5, Y3, Y3
	VPERMQ       $0xd8, Y3, Y3
	VMOVDQU      Y3, (DI)(CX*1)
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB13_4
	CMPQ         AX, DX
	JE           LBB13_7

LBB13_6:
	VUCOMISS (SI)(AX*4), X0
	SETHI    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB13_6

LBB13_7:
	VZEROUPPER
	RET

DATA dataLteNumberF64<>+0(SB)/1, $0x01
DATA dataLteNumberF64<>+1(SB)/1, $0x01
DATA dataLteNumberF64<>+2(SB)/1, $0x01
DATA dataLteNumberF64<>+3(SB)/1, $0x01
DATA dataLteNumberF64<>+4(SB)/1, $0x00
DATA dataLteNumberF64<>+5(SB)/1, $0x00
DATA dataLteNumberF64<>+6(SB)/1, $0x00
DATA dataLteNumberF64<>+7(SB)/1, $0x00
DATA dataLteNumberF64<>+8(SB)/1, $0x00
DATA dataLteNumberF64<>+9(SB)/1, $0x00
DATA dataLteNumberF64<>+10(SB)/1, $0x00
DATA dataLteNumberF64<>+11(SB)/1, $0x00
DATA dataLteNumberF64<>+12(SB)/1, $0x00
DATA dataLteNumberF64<>+13(SB)/1, $0x00
DATA dataLteNumberF64<>+14(SB)/1, $0x00
DATA dataLteNumberF64<>+15(SB)/1, $0x00
GLOBL dataLteNumberF64<>(SB), RODATA|NOPTR, $16

// func LteNumber_AVX2_F64(x []bool, y []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·LteNumber_AVX2_F64(SB), NOSPLIT, $0-56
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSD a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB14_7
	CMPQ  DX, $0x10
	JAE   LBB14_3
	XORL  AX, AX
	JMP   LBB14_6

LBB14_3:
	MOVQ         DX, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	XORL         CX, CX
	VMOVDQU      dataLteNumberF64<>+0(SB), X2

LBB14_4:
	VMOVUPD      (SI)(CX*8), Y3
	VMOVUPD      32(SI)(CX*8), Y4
	VMOVUPD      64(SI)(CX*8), Y5
	VMOVUPD      96(SI)(CX*8), Y6
	VCMPPD       $0x02, Y1, Y3, Y3
	VEXTRACTF128 $0x01, Y3, X7
	VPACKSSDW    X7, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X2, X3, X3
	VCMPPD       $0x02, Y1, Y4, Y4
	VEXTRACTF128 $0x01, Y4, X7
	VPACKSSDW    X7, X4, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VPUNPCKLDQ   X4, X3, X3
	VCMPPD       $0x02, Y1, Y5, Y4
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPD       $0x02, Y1, Y6, Y5
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSDW    X5, X5, X5
	VPACKSSWB    X5, X5, X5
	VPAND        X2, X5, X5
	VPBROADCASTD X5, X5
	VPBROADCASTD X4, X4
	VPUNPCKLDQ   X5, X4, X4
	VPBLENDD     $0x0c, X4, X3, X3
	VMOVDQU      X3, (DI)(CX*1)
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB14_4
	CMPQ         AX, DX
	JE           LBB14_7

LBB14_6:
	VUCOMISD (SI)(AX*8), X0
	SETCC    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB14_6

LBB14_7:
	VZEROUPPER
	RET

DATA dataLteNumberF32<>+0(SB)/1, $0x01
DATA dataLteNumberF32<>+1(SB)/1, $0x01
DATA dataLteNumberF32<>+2(SB)/1, $0x01
DATA dataLteNumberF32<>+3(SB)/1, $0x01
DATA dataLteNumberF32<>+4(SB)/1, $0x01
DATA dataLteNumberF32<>+5(SB)/1, $0x01
DATA dataLteNumberF32<>+6(SB)/1, $0x01
DATA dataLteNumberF32<>+7(SB)/1, $0x01
DATA dataLteNumberF32<>+8(SB)/1, $0x00
DATA dataLteNumberF32<>+9(SB)/1, $0x00
DATA dataLteNumberF32<>+10(SB)/1, $0x00
DATA dataLteNumberF32<>+11(SB)/1, $0x00
DATA dataLteNumberF32<>+12(SB)/1, $0x00
DATA dataLteNumberF32<>+13(SB)/1, $0x00
DATA dataLteNumberF32<>+14(SB)/1, $0x00
DATA dataLteNumberF32<>+15(SB)/1, $0x00
GLOBL dataLteNumberF32<>(SB), RODATA|NOPTR, $16

// func LteNumber_AVX2_F32(x []bool, y []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·LteNumber_AVX2_F32(SB), NOSPLIT, $0-52
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSS a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB15_7
	CMPQ  DX, $0x20
	JAE   LBB15_3
	XORL  AX, AX
	JMP   LBB15_6

LBB15_3:
	MOVQ         DX, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	XORL         CX, CX
	VMOVDQU      dataLteNumberF32<>+0(SB), X2

LBB15_4:
	VMOVUPS      (SI)(CX*4), Y3
	VMOVUPS      32(SI)(CX*4), Y4
	VMOVUPS      64(SI)(CX*4), Y5
	VMOVUPS      96(SI)(CX*4), Y6
	VCMPPS       $0x02, Y1, Y3, Y3
	VEXTRACTF128 $0x01, Y3, X7
	VPACKSSDW    X7, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X2, X3, X3
	VCMPPS       $0x02, Y1, Y4, Y4
	VEXTRACTF128 $0x01, Y4, X7
	VPACKSSDW    X7, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPS       $0x02, Y1, Y5, Y5
	VEXTRACTF128 $0x01, Y5, X7
	VPACKSSDW    X7, X5, X5
	VPACKSSWB    X5, X5, X5
	VPAND        X2, X5, X5
	VCMPPS       $0x02, Y1, Y6, Y6
	VEXTRACTF128 $0x01, Y6, X7
	VPACKSSDW    X7, X6, X6
	VPACKSSWB    X6, X6, X6
	VPAND        X2, X6, X6
	VINSERTI128  $0x01, X6, Y5, Y5
	VINSERTI128  $0x01, X4, Y3, Y3
	VPUNPCKLQDQ  Y5, Y3, Y3
	VPERMQ       $0xd8, Y3, Y3
	VMOVDQU      Y3, (DI)(CX*1)
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB15_4
	CMPQ         AX, DX
	JE           LBB15_7

LBB15_6:
	VUCOMISS (SI)(AX*4), X0
	SETCC    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB15_6

LBB15_7:
	VZEROUPPER
	RET

DATA dataGtNumberF64<>+0(SB)/1, $0x01
DATA dataGtNumberF64<>+1(SB)/1, $0x01
DATA dataGtNumberF64<>+2(SB)/1, $0x01
DATA dataGtNumberF64<>+3(SB)/1, $0x01
DATA dataGtNumberF64<>+4(SB)/1, $0x00
DATA dataGtNumberF64<>+5(SB)/1, $0x00
DATA dataGtNumberF64<>+6(SB)/1, $0x00
DATA dataGtNumberF64<>+7(SB)/1, $0x00
DATA dataGtNumberF64<>+8(SB)/1, $0x00
DATA dataGtNumberF64<>+9(SB)/1, $0x00
DATA dataGtNumberF64<>+10(SB)/1, $0x00
DATA dataGtNumberF64<>+11(SB)/1, $0x00
DATA dataGtNumberF64<>+12(SB)/1, $0x00
DATA dataGtNumberF64<>+13(SB)/1, $0x00
DATA dataGtNumberF64<>+14(SB)/1, $0x00
DATA dataGtNumberF64<>+15(SB)/1, $0x00
GLOBL dataGtNumberF64<>(SB), RODATA|NOPTR, $16

// func GtNumber_AVX2_F64(x []bool, y []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·GtNumber_AVX2_F64(SB), NOSPLIT, $0-56
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSD a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB16_7
	CMPQ  DX, $0x10
	JAE   LBB16_3
	XORL  AX, AX
	JMP   LBB16_6

LBB16_3:
	MOVQ         DX, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	XORL         CX, CX
	VMOVDQU      dataGtNumberF64<>+0(SB), X2

LBB16_4:
	VCMPPD       $0x01, (SI)(CX*8), Y1, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPD       $0x01, 32(SI)(CX*8), Y1, Y4
	VPAND        X2, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPD       $0x01, 64(SI)(CX*8), Y1, Y5
	VPUNPCKLDQ   X4, X3, X3
	VEXTRACTF128 $0x01, Y5, X4
	VPACKSSDW    X4, X5, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPD       $0x01, 96(SI)(CX*8), Y1, Y5
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSDW    X5, X5, X5
	VPACKSSWB    X5, X5, X5
	VPAND        X2, X5, X5
	VPBROADCASTD X5, X5
	VPBROADCASTD X4, X4
	VPUNPCKLDQ   X5, X4, X4
	VPBLENDD     $0x0c, X4, X3, X3
	VMOVDQU      X3, (DI)(CX*1)
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB16_4
	CMPQ         AX, DX
	JE           LBB16_7

LBB16_6:
	VUCOMISD (SI)(AX*8), X0
	SETCS    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB16_6

LBB16_7:
	VZEROUPPER
	RET

DATA dataGtNumberF32<>+0(SB)/1, $0x01
DATA dataGtNumberF32<>+1(SB)/1, $0x01
DATA dataGtNumberF32<>+2(SB)/1, $0x01
DATA dataGtNumberF32<>+3(SB)/1, $0x01
DATA dataGtNumberF32<>+4(SB)/1, $0x01
DATA dataGtNumberF32<>+5(SB)/1, $0x01
DATA dataGtNumberF32<>+6(SB)/1, $0x01
DATA dataGtNumberF32<>+7(SB)/1, $0x01
DATA dataGtNumberF32<>+8(SB)/1, $0x00
DATA dataGtNumberF32<>+9(SB)/1, $0x00
DATA dataGtNumberF32<>+10(SB)/1, $0x00
DATA dataGtNumberF32<>+11(SB)/1, $0x00
DATA dataGtNumberF32<>+12(SB)/1, $0x00
DATA dataGtNumberF32<>+13(SB)/1, $0x00
DATA dataGtNumberF32<>+14(SB)/1, $0x00
DATA dataGtNumberF32<>+15(SB)/1, $0x00
GLOBL dataGtNumberF32<>(SB), RODATA|NOPTR, $16

// func GtNumber_AVX2_F32(x []bool, y []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·GtNumber_AVX2_F32(SB), NOSPLIT, $0-52
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSS a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB17_7
	CMPQ  DX, $0x20
	JAE   LBB17_3
	XORL  AX, AX
	JMP   LBB17_6

LBB17_3:
	MOVQ         DX, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	XORL         CX, CX
	VMOVDQU      dataGtNumberF32<>+0(SB), X2

LBB17_4:
	VCMPPS       $0x01, (SI)(CX*4), Y1, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPS       $0x01, 32(SI)(CX*4), Y1, Y4
	VPAND        X2, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPS       $0x01, 64(SI)(CX*4), Y1, Y5
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSWB    X5, X5, X5
	VCMPPS       $0x01, 96(SI)(CX*4), Y1, Y6
	VPAND        X2, X5, X5
	VEXTRACTF128 $0x01, Y6, X7
	VPACKSSDW    X7, X6, X6
	VPACKSSWB    X6, X6, X6
	VPAND        X2, X6, X6
	VINSERTI128  $0x01, X6, Y5, Y5
	VINSERTI128  $0x01, X4, Y3, Y3
	VPUNPCKLQDQ  Y5, Y3, Y3
	VPERMQ       $0xd8, Y3, Y3
	VMOVDQU      Y3, (DI)(CX*1)
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB17_4
	CMPQ         AX, DX
	JE           LBB17_7

LBB17_6:
	VUCOMISS (SI)(AX*4), X0
	SETCS    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB17_6

LBB17_7:
	VZEROUPPER
	RET

DATA dataGteNumberF64<>+0(SB)/1, $0x01
DATA dataGteNumberF64<>+1(SB)/1, $0x01
DATA dataGteNumberF64<>+2(SB)/1, $0x01
DATA dataGteNumberF64<>+3(SB)/1, $0x01
DATA dataGteNumberF64<>+4(SB)/1, $0x00
DATA dataGteNumberF64<>+5(SB)/1, $0x00
DATA dataGteNumberF64<>+6(SB)/1, $0x00
DATA dataGteNumberF64<>+7(SB)/1, $0x00
DATA dataGteNumberF64<>+8(SB)/1, $0x00
DATA dataGteNumberF64<>+9(SB)/1, $0x00
DATA dataGteNumberF64<>+10(SB)/1, $0x00
DATA dataGteNumberF64<>+11(SB)/1, $0x00
DATA dataGteNumberF64<>+12(SB)/1, $0x00
DATA dataGteNumberF64<>+13(SB)/1, $0x00
DATA dataGteNumberF64<>+14(SB)/1, $0x00
DATA dataGteNumberF64<>+15(SB)/1, $0x00
GLOBL dataGteNumberF64<>(SB), RODATA|NOPTR, $16

// func GteNumber_AVX2_F64(x []bool, y []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·GteNumber_AVX2_F64(SB), NOSPLIT, $0-56
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSD a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB18_7
	CMPQ  DX, $0x10
	JAE   LBB18_3
	XORL  AX, AX
	JMP   LBB18_6

LBB18_3:
	MOVQ         DX, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	XORL         CX, CX
	VMOVDQU      dataGteNumberF64<>+0(SB), X2

LBB18_4:
	VCMPPD       $0x02, (SI)(CX*8), Y1, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPD       $0x02, 32(SI)(CX*8), Y1, Y4
	VPAND        X2, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPD       $0x02, 64(SI)(CX*8), Y1, Y5
	VPUNPCKLDQ   X4, X3, X3
	VEXTRACTF128 $0x01, Y5, X4
	VPACKSSDW    X4, X5, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPD       $0x02, 96(SI)(CX*8), Y1, Y5
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSDW    X5, X5, X5
	VPACKSSWB    X5, X5, X5
	VPAND        X2, X5, X5
	VPBROADCASTD X5, X5
	VPBROADCASTD X4, X4
	VPUNPCKLDQ   X5, X4, X4
	VPBLENDD     $0x0c, X4, X3, X3
	VMOVDQU      X3, (DI)(CX*1)
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB18_4
	CMPQ         AX, DX
	JE           LBB18_7

LBB18_6:
	VUCOMISD (SI)(AX*8), X0
	SETLS    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB18_6

LBB18_7:
	VZEROUPPER
	RET

DATA dataGteNumberF32<>+0(SB)/1, $0x01
DATA dataGteNumberF32<>+1(SB)/1, $0x01
DATA dataGteNumberF32<>+2(SB)/1, $0x01
DATA dataGteNumberF32<>+3(SB)/1, $0x01
DATA dataGteNumberF32<>+4(SB)/1, $0x01
DATA dataGteNumberF32<>+5(SB)/1, $0x01
DATA dataGteNumberF32<>+6(SB)/1, $0x01
DATA dataGteNumberF32<>+7(SB)/1, $0x01
DATA dataGteNumberF32<>+8(SB)/1, $0x00
DATA dataGteNumberF32<>+9(SB)/1, $0x00
DATA dataGteNumberF32<>+10(SB)/1, $0x00
DATA dataGteNumberF32<>+11(SB)/1, $0x00
DATA dataGteNumberF32<>+12(SB)/1, $0x00
DATA dataGteNumberF32<>+13(SB)/1, $0x00
DATA dataGteNumberF32<>+14(SB)/1, $0x00
DATA dataGteNumberF32<>+15(SB)/1, $0x00
GLOBL dataGteNumberF32<>(SB), RODATA|NOPTR, $16

// func GteNumber_AVX2_F32(x []bool, y []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·GteNumber_AVX2_F32(SB), NOSPLIT, $0-52
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSS a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB19_7
	CMPQ  DX, $0x20
	JAE   LBB19_3
	XORL  AX, AX
	JMP   LBB19_6

LBB19_3:
	MOVQ         DX, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	XORL         CX, CX
	VMOVDQU      dataGteNumberF32<>+0(SB), X2

LBB19_4:
	VCMPPS       $0x02, (SI)(CX*4), Y1, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPS       $0x02, 32(SI)(CX*4), Y1, Y4
	VPAND        X2, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPS       $0x02, 64(SI)(CX*4), Y1, Y5
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSWB    X5, X5, X5
	VCMPPS       $0x02, 96(SI)(CX*4), Y1, Y6
	VPAND        X2, X5, X5
	VEXTRACTF128 $0x01, Y6, X7
	VPACKSSDW    X7, X6, X6
	VPACKSSWB    X6, X6, X6
	VPAND        X2, X6, X6
	VINSERTI128  $0x01, X6, Y5, Y5
	VINSERTI128  $0x01, X4, Y3, Y3
	VPUNPCKLQDQ  Y5, Y3, Y3
	VPERMQ       $0xd8, Y3, Y3
	VMOVDQU      Y3, (DI)(CX*1)
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB19_4
	CMPQ         AX, DX
	JE           LBB19_7

LBB19_6:
	VUCOMISS (SI)(AX*4), X0
	SETLS    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB19_6

LBB19_7:
	VZEROUPPER
	RET

DATA dataEqNumberF64<>+0(SB)/1, $0x01
DATA dataEqNumberF64<>+1(SB)/1, $0x01
DATA dataEqNumberF64<>+2(SB)/1, $0x01
DATA dataEqNumberF64<>+3(SB)/1, $0x01
DATA dataEqNumberF64<>+4(SB)/1, $0x00
DATA dataEqNumberF64<>+5(SB)/1, $0x00
DATA dataEqNumberF64<>+6(SB)/1, $0x00
DATA dataEqNumberF64<>+7(SB)/1, $0x00
DATA dataEqNumberF64<>+8(SB)/1, $0x00
DATA dataEqNumberF64<>+9(SB)/1, $0x00
DATA dataEqNumberF64<>+10(SB)/1, $0x00
DATA dataEqNumberF64<>+11(SB)/1, $0x00
DATA dataEqNumberF64<>+12(SB)/1, $0x00
DATA dataEqNumberF64<>+13(SB)/1, $0x00
DATA dataEqNumberF64<>+14(SB)/1, $0x00
DATA dataEqNumberF64<>+15(SB)/1, $0x00
GLOBL dataEqNumberF64<>(SB), RODATA|NOPTR, $16

// func EqNumber_AVX2_F64(x []bool, y []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·EqNumber_AVX2_F64(SB), NOSPLIT, $0-56
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSD a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB20_7
	CMPQ  DX, $0x10
	JAE   LBB20_3
	XORL  AX, AX
	JMP   LBB20_6

LBB20_3:
	MOVQ         DX, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	XORL         CX, CX
	VMOVDQU      dataEqNumberF64<>+0(SB), X2

LBB20_4:
	VCMPPD       $0x00, (SI)(CX*8), Y1, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPD       $0x00, 32(SI)(CX*8), Y1, Y4
	VPAND        X2, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPD       $0x00, 64(SI)(CX*8), Y1, Y5
	VPUNPCKLDQ   X4, X3, X3
	VEXTRACTF128 $0x01, Y5, X4
	VPACKSSDW    X4, X5, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPD       $0x00, 96(SI)(CX*8), Y1, Y5
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSDW    X5, X5, X5
	VPACKSSWB    X5, X5, X5
	VPAND        X2, X5, X5
	VPBROADCASTD X5, X5
	VPBROADCASTD X4, X4
	VPUNPCKLDQ   X5, X4, X4
	VPBLENDD     $0x0c, X4, X3, X3
	VMOVDQU      X3, (DI)(CX*1)
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB20_4
	CMPQ         AX, DX
	JE           LBB20_7

LBB20_6:
	VUCOMISD (SI)(AX*8), X0
	SETEQ    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB20_6

LBB20_7:
	VZEROUPPER
	RET

DATA dataEqNumberF32<>+0(SB)/1, $0x01
DATA dataEqNumberF32<>+1(SB)/1, $0x01
DATA dataEqNumberF32<>+2(SB)/1, $0x01
DATA dataEqNumberF32<>+3(SB)/1, $0x01
DATA dataEqNumberF32<>+4(SB)/1, $0x01
DATA dataEqNumberF32<>+5(SB)/1, $0x01
DATA dataEqNumberF32<>+6(SB)/1, $0x01
DATA dataEqNumberF32<>+7(SB)/1, $0x01
DATA dataEqNumberF32<>+8(SB)/1, $0x00
DATA dataEqNumberF32<>+9(SB)/1, $0x00
DATA dataEqNumberF32<>+10(SB)/1, $0x00
DATA dataEqNumberF32<>+11(SB)/1, $0x00
DATA dataEqNumberF32<>+12(SB)/1, $0x00
DATA dataEqNumberF32<>+13(SB)/1, $0x00
DATA dataEqNumberF32<>+14(SB)/1, $0x00
DATA dataEqNumberF32<>+15(SB)/1, $0x00
GLOBL dataEqNumberF32<>(SB), RODATA|NOPTR, $16

// func EqNumber_AVX2_F32(x []bool, y []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·EqNumber_AVX2_F32(SB), NOSPLIT, $0-52
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSS a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB21_7
	CMPQ  DX, $0x20
	JAE   LBB21_3
	XORL  AX, AX
	JMP   LBB21_6

LBB21_3:
	MOVQ         DX, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	XORL         CX, CX
	VMOVDQU      dataEqNumberF32<>+0(SB), X2

LBB21_4:
	VCMPPS       $0x00, (SI)(CX*4), Y1, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPS       $0x00, 32(SI)(CX*4), Y1, Y4
	VPAND        X2, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPS       $0x00, 64(SI)(CX*4), Y1, Y5
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSWB    X5, X5, X5
	VCMPPS       $0x00, 96(SI)(CX*4), Y1, Y6
	VPAND        X2, X5, X5
	VEXTRACTF128 $0x01, Y6, X7
	VPACKSSDW    X7, X6, X6
	VPACKSSWB    X6, X6, X6
	VPAND        X2, X6, X6
	VINSERTI128  $0x01, X6, Y5, Y5
	VINSERTI128  $0x01, X4, Y3, Y3
	VPUNPCKLQDQ  Y5, Y3, Y3
	VPERMQ       $0xd8, Y3, Y3
	VMOVDQU      Y3, (DI)(CX*1)
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB21_4
	CMPQ         AX, DX
	JE           LBB21_7

LBB21_6:
	VUCOMISS (SI)(AX*4), X0
	SETEQ    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB21_6

LBB21_7:
	VZEROUPPER
	RET

DATA dataNeqNumberF64<>+0(SB)/1, $0x01
DATA dataNeqNumberF64<>+1(SB)/1, $0x01
DATA dataNeqNumberF64<>+2(SB)/1, $0x01
DATA dataNeqNumberF64<>+3(SB)/1, $0x01
DATA dataNeqNumberF64<>+4(SB)/1, $0x00
DATA dataNeqNumberF64<>+5(SB)/1, $0x00
DATA dataNeqNumberF64<>+6(SB)/1, $0x00
DATA dataNeqNumberF64<>+7(SB)/1, $0x00
DATA dataNeqNumberF64<>+8(SB)/1, $0x00
DATA dataNeqNumberF64<>+9(SB)/1, $0x00
DATA dataNeqNumberF64<>+10(SB)/1, $0x00
DATA dataNeqNumberF64<>+11(SB)/1, $0x00
DATA dataNeqNumberF64<>+12(SB)/1, $0x00
DATA dataNeqNumberF64<>+13(SB)/1, $0x00
DATA dataNeqNumberF64<>+14(SB)/1, $0x00
DATA dataNeqNumberF64<>+15(SB)/1, $0x00
GLOBL dataNeqNumberF64<>(SB), RODATA|NOPTR, $16

// func NeqNumber_AVX2_F64(x []bool, y []float64, a float64)
// Requires: AVX, AVX2, SSE2
TEXT ·NeqNumber_AVX2_F64(SB), NOSPLIT, $0-56
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSD a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB22_7
	CMPQ  DX, $0x10
	JAE   LBB22_3
	XORL  AX, AX
	JMP   LBB22_6

LBB22_3:
	MOVQ         DX, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	XORL         CX, CX
	VMOVDQU      dataNeqNumberF64<>+0(SB), X2

LBB22_4:
	VCMPPD       $0x04, (SI)(CX*8), Y1, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPD       $0x04, 32(SI)(CX*8), Y1, Y4
	VPAND        X2, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPD       $0x04, 64(SI)(CX*8), Y1, Y5
	VPUNPCKLDQ   X4, X3, X3
	VEXTRACTF128 $0x01, Y5, X4
	VPACKSSDW    X4, X5, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPD       $0x04, 96(SI)(CX*8), Y1, Y5
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSDW    X5, X5, X5
	VPACKSSWB    X5, X5, X5
	VPAND        X2, X5, X5
	VPBROADCASTD X5, X5
	VPBROADCASTD X4, X4
	VPUNPCKLDQ   X5, X4, X4
	VPBLENDD     $0x0c, X4, X3, X3
	VMOVDQU      X3, (DI)(CX*1)
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB22_4
	CMPQ         AX, DX
	JE           LBB22_7

LBB22_6:
	VUCOMISD (SI)(AX*8), X0
	SETNE    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB22_6

LBB22_7:
	VZEROUPPER
	RET

DATA dataNeqNumberF32<>+0(SB)/1, $0x01
DATA dataNeqNumberF32<>+1(SB)/1, $0x01
DATA dataNeqNumberF32<>+2(SB)/1, $0x01
DATA dataNeqNumberF32<>+3(SB)/1, $0x01
DATA dataNeqNumberF32<>+4(SB)/1, $0x01
DATA dataNeqNumberF32<>+5(SB)/1, $0x01
DATA dataNeqNumberF32<>+6(SB)/1, $0x01
DATA dataNeqNumberF32<>+7(SB)/1, $0x01
DATA dataNeqNumberF32<>+8(SB)/1, $0x00
DATA dataNeqNumberF32<>+9(SB)/1, $0x00
DATA dataNeqNumberF32<>+10(SB)/1, $0x00
DATA dataNeqNumberF32<>+11(SB)/1, $0x00
DATA dataNeqNumberF32<>+12(SB)/1, $0x00
DATA dataNeqNumberF32<>+13(SB)/1, $0x00
DATA dataNeqNumberF32<>+14(SB)/1, $0x00
DATA dataNeqNumberF32<>+15(SB)/1, $0x00
GLOBL dataNeqNumberF32<>(SB), RODATA|NOPTR, $16

// func NeqNumber_AVX2_F32(x []bool, y []float32, a float32)
// Requires: AVX, AVX2, SSE
TEXT ·NeqNumber_AVX2_F32(SB), NOSPLIT, $0-52
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVSS a+48(FP), X0
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB23_7
	CMPQ  DX, $0x20
	JAE   LBB23_3
	XORL  AX, AX
	JMP   LBB23_6

LBB23_3:
	MOVQ         DX, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	XORL         CX, CX
	VMOVDQU      dataNeqNumberF32<>+0(SB), X2

LBB23_4:
	VCMPPS       $0x04, (SI)(CX*4), Y1, Y3
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSWB    X3, X3, X3
	VCMPPS       $0x04, 32(SI)(CX*4), Y1, Y4
	VPAND        X2, X3, X3
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X2, X4, X4
	VCMPPS       $0x04, 64(SI)(CX*4), Y1, Y5
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSWB    X5, X5, X5
	VCMPPS       $0x04, 96(SI)(CX*4), Y1, Y6
	VPAND        X2, X5, X5
	VEXTRACTF128 $0x01, Y6, X7
	VPACKSSDW    X7, X6, X6
	VPACKSSWB    X6, X6, X6
	VPAND        X2, X6, X6
	VINSERTI128  $0x01, X6, Y5, Y5
	VINSERTI128  $0x01, X4, Y3, Y3
	VPUNPCKLQDQ  Y5, Y3, Y3
	VPERMQ       $0xd8, Y3, Y3
	VMOVDQU      Y3, (DI)(CX*1)
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB23_4
	CMPQ         AX, DX
	JE           LBB23_7

LBB23_6:
	VUCOMISS (SI)(AX*4), X0
	SETNE    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB23_6

LBB23_7:
	VZEROUPPER
	RET

DATA dataNot<>+0(SB)/1, $0x01
DATA dataNot<>+1(SB)/1, $0x01
DATA dataNot<>+2(SB)/1, $0x01
DATA dataNot<>+3(SB)/1, $0x01
DATA dataNot<>+4(SB)/1, $0x01
DATA dataNot<>+5(SB)/1, $0x01
DATA dataNot<>+6(SB)/1, $0x01
DATA dataNot<>+7(SB)/1, $0x01
DATA dataNot<>+8(SB)/1, $0x01
DATA dataNot<>+9(SB)/1, $0x01
DATA dataNot<>+10(SB)/1, $0x01
DATA dataNot<>+11(SB)/1, $0x01
DATA dataNot<>+12(SB)/1, $0x01
DATA dataNot<>+13(SB)/1, $0x01
DATA dataNot<>+14(SB)/1, $0x01
DATA dataNot<>+15(SB)/1, $0x01
DATA dataNot<>+16(SB)/1, $0x01
DATA dataNot<>+17(SB)/1, $0x01
DATA dataNot<>+18(SB)/1, $0x01
DATA dataNot<>+19(SB)/1, $0x01
DATA dataNot<>+20(SB)/1, $0x01
DATA dataNot<>+21(SB)/1, $0x01
DATA dataNot<>+22(SB)/1, $0x01
DATA dataNot<>+23(SB)/1, $0x01
DATA dataNot<>+24(SB)/1, $0x01
DATA dataNot<>+25(SB)/1, $0x01
DATA dataNot<>+26(SB)/1, $0x01
DATA dataNot<>+27(SB)/1, $0x01
DATA dataNot<>+28(SB)/1, $0x01
DATA dataNot<>+29(SB)/1, $0x01
DATA dataNot<>+30(SB)/1, $0x01
DATA dataNot<>+31(SB)/1, $0x01
GLOBL dataNot<>(SB), RODATA|NOPTR, $32

// func Not_AVX2(x []bool)
// Requires: AVX
TEXT ·Not_AVX2(SB), NOSPLIT, $0-24
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB0_17
	CMPQ  SI, $0x10
	JAE   LBB0_3
	XORL  AX, AX
	JMP   LBB0_16

LBB0_3:
	CMPQ SI, $0x80
	JAE  LBB0_5
	XORL AX, AX
	JMP  LBB0_13

LBB0_5:
	MOVQ    SI, AX
	ANDQ    $-128, AX
	LEAQ    -128(AX), CX
	MOVQ    CX, R8
	SHRQ    $0x07, R8
	ADDQ    $0x01, R8
	TESTQ   CX, CX
	JE      LBB0_6
	MOVQ    R8, DX
	ANDQ    $-2, DX
	XORL    CX, CX
	VMOVUPS dataNot<>+0(SB), Y0

LBB0_8:
	VXORPS  (DI)(CX*1), Y0, Y1
	VXORPS  32(DI)(CX*1), Y0, Y2
	VXORPS  64(DI)(CX*1), Y0, Y3
	VXORPS  96(DI)(CX*1), Y0, Y4
	VMOVUPS Y1, (DI)(CX*1)
	VMOVUPS Y2, 32(DI)(CX*1)
	VMOVUPS Y3, 64(DI)(CX*1)
	VMOVUPS Y4, 96(DI)(CX*1)
	VXORPS  128(DI)(CX*1), Y0, Y1
	VXORPS  160(DI)(CX*1), Y0, Y2
	VXORPS  192(DI)(CX*1), Y0, Y3
	VXORPS  224(DI)(CX*1), Y0, Y4
	VMOVUPS Y1, 128(DI)(CX*1)
	VMOVUPS Y2, 160(DI)(CX*1)
	VMOVUPS Y3, 192(DI)(CX*1)
	VMOVUPS Y4, 224(DI)(CX*1)
	ADDQ    $+256, CX
	ADDQ    $-2, DX
	JNE     LBB0_8
	TESTB   $0x01, R8
	JE      LBB0_11

LBB0_10:
	VMOVUPS dataNot<>+0(SB), Y0
	VXORPS  (DI)(CX*1), Y0, Y1
	VXORPS  32(DI)(CX*1), Y0, Y2
	VXORPS  64(DI)(CX*1), Y0, Y3
	VXORPS  96(DI)(CX*1), Y0, Y0
	VMOVUPS Y1, (DI)(CX*1)
	VMOVUPS Y2, 32(DI)(CX*1)
	VMOVUPS Y3, 64(DI)(CX*1)
	VMOVUPS Y0, 96(DI)(CX*1)

LBB0_11:
	CMPQ  AX, SI
	JE    LBB0_17
	TESTB $0x70, SI
	JE    LBB0_16

LBB0_13:
	MOVQ    AX, CX
	MOVQ    SI, AX
	ANDQ    $-16, AX
	VMOVUPS dataNot<>+0(SB), X0

LBB0_14:
	VXORPS  (DI)(CX*1), X0, X1
	VMOVUPS X1, (DI)(CX*1)
	ADDQ    $0x10, CX
	CMPQ    AX, CX
	JNE     LBB0_14
	CMPQ    AX, SI
	JE      LBB0_17

LBB0_16:
	XORB $0x01, (DI)(AX*1)
	ADDQ $0x01, AX
	CMPQ SI, AX
	JNE  LBB0_16

LBB0_17:
	VZEROUPPER
	RET

LBB0_6:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB0_10
	JMP   LBB0_11

// func And_AVX2(x []bool, y []bool)
// Requires: AVX
TEXT ·And_AVX2(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB1_13
	CMPQ  DX, $0x10
	JAE   LBB1_3
	XORL  AX, AX
	JMP   LBB1_12

LBB1_3:
	CMPQ DX, $0x80
	JAE  LBB1_5
	XORL AX, AX
	JMP  LBB1_9

LBB1_5:
	MOVQ DX, AX
	ANDQ $-128, AX
	XORL CX, CX

LBB1_6:
	VMOVUPS (SI)(CX*1), Y0
	VMOVUPS 32(SI)(CX*1), Y1
	VMOVUPS 64(SI)(CX*1), Y2
	VMOVUPS 96(SI)(CX*1), Y3
	VANDPS  (DI)(CX*1), Y0, Y0
	VANDPS  32(DI)(CX*1), Y1, Y1
	VANDPS  64(DI)(CX*1), Y2, Y2
	VANDPS  96(DI)(CX*1), Y3, Y3
	VMOVUPS Y0, (DI)(CX*1)
	VMOVUPS Y1, 32(DI)(CX*1)
	VMOVUPS Y2, 64(DI)(CX*1)
	VMOVUPS Y3, 96(DI)(CX*1)
	SUBQ    $-128, CX
	CMPQ    AX, CX
	JNE     LBB1_6
	CMPQ    AX, DX
	JE      LBB1_13
	TESTB   $0x70, DL
	JE      LBB1_12

LBB1_9:
	MOVQ AX, CX
	MOVQ DX, AX
	ANDQ $-16, AX

LBB1_10:
	VMOVUPS (SI)(CX*1), X0
	VANDPS  (DI)(CX*1), X0, X0
	VMOVUPS X0, (DI)(CX*1)
	ADDQ    $0x10, CX
	CMPQ    AX, CX
	JNE     LBB1_10
	CMPQ    AX, DX
	JE      LBB1_13

LBB1_12:
	MOVBLZX (SI)(AX*1), CX
	ANDB    CL, (DI)(AX*1)
	ADDQ    $0x01, AX
	CMPQ    DX, AX
	JNE     LBB1_12

LBB1_13:
	VZEROUPPER
	RET

// func Or_AVX2(x []bool, y []bool)
// Requires: AVX
TEXT ·Or_AVX2(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB2_13
	CMPQ  DX, $0x10
	JAE   LBB2_3
	XORL  AX, AX
	JMP   LBB2_12

LBB2_3:
	CMPQ DX, $0x80
	JAE  LBB2_5
	XORL AX, AX
	JMP  LBB2_9

LBB2_5:
	MOVQ DX, AX
	ANDQ $-128, AX
	XORL CX, CX

LBB2_6:
	VMOVUPS (SI)(CX*1), Y0
	VMOVUPS 32(SI)(CX*1), Y1
	VMOVUPS 64(SI)(CX*1), Y2
	VMOVUPS 96(SI)(CX*1), Y3
	VORPS   (DI)(CX*1), Y0, Y0
	VORPS   32(DI)(CX*1), Y1, Y1
	VORPS   64(DI)(CX*1), Y2, Y2
	VORPS   96(DI)(CX*1), Y3, Y3
	VMOVUPS Y0, (DI)(CX*1)
	VMOVUPS Y1, 32(DI)(CX*1)
	VMOVUPS Y2, 64(DI)(CX*1)
	VMOVUPS Y3, 96(DI)(CX*1)
	SUBQ    $-128, CX
	CMPQ    AX, CX
	JNE     LBB2_6
	CMPQ    AX, DX
	JE      LBB2_13
	TESTB   $0x70, DL
	JE      LBB2_12

LBB2_9:
	MOVQ AX, CX
	MOVQ DX, AX
	ANDQ $-16, AX

LBB2_10:
	VMOVUPS (SI)(CX*1), X0
	VORPS   (DI)(CX*1), X0, X0
	VMOVUPS X0, (DI)(CX*1)
	ADDQ    $0x10, CX
	CMPQ    AX, CX
	JNE     LBB2_10
	CMPQ    AX, DX
	JE      LBB2_13

LBB2_12:
	MOVBLZX (SI)(AX*1), CX
	ORB     CL, (DI)(AX*1)
	ADDQ    $0x01, AX
	CMPQ    DX, AX
	JNE     LBB2_12

LBB2_13:
	VZEROUPPER
	RET

// func Xor_AVX2(x []bool, y []bool)
// Requires: AVX
TEXT ·Xor_AVX2(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB3_13
	CMPQ  DX, $0x10
	JAE   LBB3_3
	XORL  AX, AX
	JMP   LBB3_12

LBB3_3:
	CMPQ DX, $0x80
	JAE  LBB3_5
	XORL AX, AX
	JMP  LBB3_9

LBB3_5:
	MOVQ DX, AX
	ANDQ $-128, AX
	XORL CX, CX

LBB3_6:
	VMOVUPS (SI)(CX*1), Y0
	VMOVUPS 32(SI)(CX*1), Y1
	VMOVUPS 64(SI)(CX*1), Y2
	VMOVUPS 96(SI)(CX*1), Y3
	VXORPS  (DI)(CX*1), Y0, Y0
	VXORPS  32(DI)(CX*1), Y1, Y1
	VXORPS  64(DI)(CX*1), Y2, Y2
	VXORPS  96(DI)(CX*1), Y3, Y3
	VMOVUPS Y0, (DI)(CX*1)
	VMOVUPS Y1, 32(DI)(CX*1)
	VMOVUPS Y2, 64(DI)(CX*1)
	VMOVUPS Y3, 96(DI)(CX*1)
	SUBQ    $-128, CX
	CMPQ    AX, CX
	JNE     LBB3_6
	CMPQ    AX, DX
	JE      LBB3_13
	TESTB   $0x70, DL
	JE      LBB3_12

LBB3_9:
	MOVQ AX, CX
	MOVQ DX, AX
	ANDQ $-16, AX

LBB3_10:
	VMOVUPS (SI)(CX*1), X0
	VXORPS  (DI)(CX*1), X0, X0
	VMOVUPS X0, (DI)(CX*1)
	ADDQ    $0x10, CX
	CMPQ    AX, CX
	JNE     LBB3_10
	CMPQ    AX, DX
	JE      LBB3_13

LBB3_12:
	MOVBLZX (SI)(AX*1), CX
	XORB    CL, (DI)(AX*1)
	ADDQ    $0x01, AX
	CMPQ    DX, AX
	JNE     LBB3_12

LBB3_13:
	VZEROUPPER
	RET

// func All_AVX2(x []bool) int
// Requires: AVX, AVX2
TEXT ·All_AVX2(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	MOVQ  SI, AX
	XORL  CX, CX
	ANDQ  $-32, AX
	JE    LBB0_1
	VPXOR X0, X0, X0

LBB0_8:
	VPCMPEQB (DI)(CX*1), Y0, Y1
	VPTEST   Y1, Y1
	JNE      LBB0_9
	ADDQ     $0x20, CX
	CMPQ     CX, AX
	JB       LBB0_8

LBB0_1:
	MOVB $0x01, AL
	CMPQ CX, SI
	JAE  LBB0_6
	ADDQ $-1, SI

LBB0_3:
	MOVBLZX (DI)(CX*1), AX
	TESTB   AL, AL
	JE      LBB0_5
	LEAQ    1(CX), DX
	CMPQ    SI, CX
	MOVQ    DX, CX
	JNE     LBB0_3

LBB0_5:
	TESTB AL, AL
	SETNE AL

LBB0_6:
	VZEROUPPER
	MOVQ AX, ret+24(FP)
	RET

LBB0_9:
	XORL AX, AX
	VZEROUPPER
	MOVQ AX, ret+24(FP)
	RET

// func Any_AVX2(x []bool) int
// Requires: AVX
TEXT ·Any_AVX2(SB), NOSPLIT, $0-32
	MOVQ x_base+0(FP), DI
	MOVQ x_len+8(FP), SI
	MOVQ SI, CX
	XORL AX, AX
	ANDQ $-32, CX
	JE   LBB1_1

LBB1_4:
	VMOVDQU (DI)(AX*1), Y0
	VPTEST  Y0, Y0
	JNE     LBB1_5
	ADDQ    $0x20, AX
	CMPQ    AX, CX
	JB      LBB1_4

LBB1_1:
	CMPQ AX, SI
	JAE  LBB1_2
	ADDQ $-1, SI

LBB1_7:
	MOVBLZX (DI)(AX*1), CX
	TESTB   CL, CL
	JNE     LBB1_9
	LEAQ    1(AX), DX
	CMPQ    SI, AX
	MOVQ    DX, AX
	JNE     LBB1_7

LBB1_9:
	TESTB CL, CL
	SETNE AL
	VZEROUPPER
	MOVQ  AX, ret+24(FP)
	RET

LBB1_5:
	MOVB $0x01, AL
	VZEROUPPER
	MOVQ AX, ret+24(FP)
	RET

LBB1_2:
	XORL AX, AX
	VZEROUPPER
	MOVQ AX, ret+24(FP)
	RET

// func None_AVX2(x []bool) int
// Requires: AVX
TEXT ·None_AVX2(SB), NOSPLIT, $0-32
	MOVQ x_base+0(FP), DI
	MOVQ x_len+8(FP), SI
	MOVQ SI, AX
	XORL CX, CX
	ANDQ $-32, AX
	JE   LBB2_1

LBB2_7:
	VMOVDQU (DI)(CX*1), Y0
	VPTEST  Y0, Y0
	JNE     LBB2_8
	ADDQ    $0x20, CX
	CMPQ    CX, AX
	JB      LBB2_7

LBB2_1:
	MOVB $0x01, AL
	CMPQ CX, SI
	JAE  LBB2_5
	ADDQ $-1, SI

LBB2_3:
	CMPB  (DI)(CX*1), $0x00
	SETEQ AL
	JNE   LBB2_5
	LEAQ  1(CX), DX
	CMPQ  SI, CX
	MOVQ  DX, CX
	JNE   LBB2_3

LBB2_5:
	VZEROUPPER
	MOVQ AX, ret+24(FP)
	RET

LBB2_8:
	XORL AX, AX
	VZEROUPPER
	MOVQ AX, ret+24(FP)
	RET

// func Count_AVX2(x []bool) int
// Requires: AVX, AVX2
TEXT ·Count_AVX2(SB), NOSPLIT, $0-32
	MOVQ  x_base+0(FP), DI
	MOVQ  x_len+8(FP), SI
	TESTQ SI, SI
	JE    LBB9_1
	CMPQ  SI, $0x10
	JAE   LBB9_4
	XORL  CX, CX
	XORL  AX, AX
	JMP   LBB9_11

LBB9_1:
	XORL AX, AX
	MOVQ AX, ret+24(FP)
	RET

LBB9_4:
	MOVQ  SI, CX
	ANDQ  $-16, CX
	LEAQ  -16(CX), AX
	MOVQ  AX, R8
	SHRQ  $0x04, R8
	ADDQ  $0x01, R8
	TESTQ AX, AX
	JE    LBB9_5
	MOVQ  R8, DX
	ANDQ  $-2, DX
	VPXOR X0, X0, X0
	XORL  AX, AX
	VPXOR X1, X1, X1
	VPXOR X2, X2, X2
	VPXOR X3, X3, X3

LBB9_7:
	VPMOVZXBQ (DI)(AX*1), Y4
	VPADDQ    Y4, Y0, Y0
	VPMOVZXBQ 4(DI)(AX*1), Y4
	VPADDQ    Y4, Y1, Y1
	VPMOVZXBQ 8(DI)(AX*1), Y4
	VPMOVZXBQ 12(DI)(AX*1), Y5
	VPADDQ    Y4, Y2, Y2
	VPADDQ    Y5, Y3, Y3
	VPMOVZXBQ 16(DI)(AX*1), Y4
	VPADDQ    Y4, Y0, Y0
	VPMOVZXBQ 20(DI)(AX*1), Y4
	VPADDQ    Y4, Y1, Y1
	VPMOVZXBQ 24(DI)(AX*1), Y4
	VPMOVZXBQ 28(DI)(AX*1), Y5
	VPADDQ    Y4, Y2, Y2
	VPADDQ    Y5, Y3, Y3
	ADDQ      $0x20, AX
	ADDQ      $-2, DX
	JNE       LBB9_7
	TESTB     $0x01, R8
	JE        LBB9_10

LBB9_9:
	VPMOVZXBQ (DI)(AX*1), Y4
	VPMOVZXBQ 4(DI)(AX*1), Y5
	VPADDQ    Y4, Y0, Y0
	VPADDQ    Y5, Y1, Y1
	VPMOVZXBQ 8(DI)(AX*1), Y4
	VPADDQ    Y4, Y2, Y2
	VPMOVZXBQ 12(DI)(AX*1), Y4
	VPADDQ    Y4, Y3, Y3

LBB9_10:
	VPADDQ       Y3, Y1, Y1
	VPADDQ       Y2, Y0, Y0
	VPADDQ       Y1, Y0, Y0
	VEXTRACTI128 $0x01, Y0, X1
	VPADDQ       X1, X0, X0
	VPSHUFD      $0xee, X0, X1
	VPADDQ       X1, X0, X0
	VMOVQ        X0, AX
	CMPQ         CX, SI
	JE           LBB9_12

LBB9_11:
	MOVBLZX (DI)(CX*1), DX
	ADDQ    DX, AX
	ADDQ    $0x01, CX
	CMPQ    SI, CX
	JNE     LBB9_11

LBB9_12:
	VZEROUPPER
	MOVQ AX, ret+24(FP)
	RET

LBB9_5:
	VPXOR X0, X0, X0
	XORL  AX, AX
	VPXOR X1, X1, X1
	VPXOR X2, X2, X2
	VPXOR X3, X3, X3
	TESTB $0x01, R8
	JNE   LBB9_9
	JMP   LBB9_10

// func Repeat_AVX2_F64(x []float64, a float64, n int)
// Requires: AVX, AVX2, SSE2
TEXT ·Repeat_AVX2_F64(SB), NOSPLIT, $0-40
	MOVQ  x_base+0(FP), DI
	MOVSD a+24(FP), X0
	MOVQ  n+32(FP), SI
	TESTQ SI, SI
	JE    LBB0_12
	CMPQ  SI, $0x10
	JAE   LBB0_3
	XORL  AX, AX
	JMP   LBB0_11

LBB0_3:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	LEAQ         -16(AX), CX
	MOVQ         CX, DX
	SHRQ         $0x04, DX
	ADDQ         $0x01, DX
	MOVL         DX, R8
	ANDL         $0x03, R8
	CMPQ         CX, $0x30
	JAE          LBB0_5
	XORL         CX, CX
	JMP          LBB0_7

LBB0_5:
	ANDQ $-4, DX
	XORL CX, CX

LBB0_6:
	VMOVUPS Y1, (DI)(CX*8)
	VMOVUPS Y1, 32(DI)(CX*8)
	VMOVUPS Y1, 64(DI)(CX*8)
	VMOVUPS Y1, 96(DI)(CX*8)
	VMOVUPS Y1, 128(DI)(CX*8)
	VMOVUPS Y1, 160(DI)(CX*8)
	VMOVUPS Y1, 192(DI)(CX*8)
	VMOVUPS Y1, 224(DI)(CX*8)
	VMOVUPS Y1, 256(DI)(CX*8)
	VMOVUPS Y1, 288(DI)(CX*8)
	VMOVUPS Y1, 320(DI)(CX*8)
	VMOVUPS Y1, 352(DI)(CX*8)
	VMOVUPS Y1, 384(DI)(CX*8)
	VMOVUPS Y1, 416(DI)(CX*8)
	VMOVUPS Y1, 448(DI)(CX*8)
	VMOVUPS Y1, 480(DI)(CX*8)
	ADDQ    $0x40, CX
	ADDQ    $-4, DX
	JNE     LBB0_6

LBB0_7:
	TESTQ R8, R8
	JE    LBB0_10
	LEAQ  (DI)(CX*8), CX
	ADDQ  $0x60, CX
	SHLQ  $0x07, R8
	XORL  DX, DX

LBB0_9:
	VMOVUPS Y1, -96(CX)(DX*1)
	VMOVUPS Y1, -64(CX)(DX*1)
	VMOVUPS Y1, -32(CX)(DX*1)
	VMOVUPS Y1, (CX)(DX*1)
	SUBQ    $-128, DX
	CMPQ    R8, DX
	JNE     LBB0_9

LBB0_10:
	CMPQ AX, SI
	JE   LBB0_12

LBB0_11:
	VMOVSD X0, (DI)(AX*8)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB0_11

LBB0_12:
	VZEROUPPER
	RET

// func Repeat_AVX2_F32(x []float32, a float32, n int)
// Requires: AVX, AVX2, SSE
TEXT ·Repeat_AVX2_F32(SB), NOSPLIT, $0-40
	MOVQ  x_base+0(FP), DI
	MOVSS a+24(FP), X0
	MOVQ  n+32(FP), SI
	TESTQ SI, SI
	JE    LBB1_12
	CMPQ  SI, $0x20
	JAE   LBB1_3
	XORL  AX, AX
	JMP   LBB1_11

LBB1_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	LEAQ         -32(AX), CX
	MOVQ         CX, DX
	SHRQ         $0x05, DX
	ADDQ         $0x01, DX
	MOVL         DX, R8
	ANDL         $0x03, R8
	CMPQ         CX, $0x60
	JAE          LBB1_5
	XORL         CX, CX
	JMP          LBB1_7

LBB1_5:
	ANDQ $-4, DX
	XORL CX, CX

LBB1_6:
	VMOVUPS Y1, (DI)(CX*4)
	VMOVUPS Y1, 32(DI)(CX*4)
	VMOVUPS Y1, 64(DI)(CX*4)
	VMOVUPS Y1, 96(DI)(CX*4)
	VMOVUPS Y1, 128(DI)(CX*4)
	VMOVUPS Y1, 160(DI)(CX*4)
	VMOVUPS Y1, 192(DI)(CX*4)
	VMOVUPS Y1, 224(DI)(CX*4)
	VMOVUPS Y1, 256(DI)(CX*4)
	VMOVUPS Y1, 288(DI)(CX*4)
	VMOVUPS Y1, 320(DI)(CX*4)
	VMOVUPS Y1, 352(DI)(CX*4)
	VMOVUPS Y1, 384(DI)(CX*4)
	VMOVUPS Y1, 416(DI)(CX*4)
	VMOVUPS Y1, 448(DI)(CX*4)
	VMOVUPS Y1, 480(DI)(CX*4)
	SUBQ    $-128, CX
	ADDQ    $-4, DX
	JNE     LBB1_6

LBB1_7:
	TESTQ R8, R8
	JE    LBB1_10
	LEAQ  (DI)(CX*4), CX
	ADDQ  $0x60, CX
	SHLQ  $0x07, R8
	XORL  DX, DX

LBB1_9:
	VMOVUPS Y1, -96(CX)(DX*1)
	VMOVUPS Y1, -64(CX)(DX*1)
	VMOVUPS Y1, -32(CX)(DX*1)
	VMOVUPS Y1, (CX)(DX*1)
	SUBQ    $-128, DX
	CMPQ    R8, DX
	JNE     LBB1_9

LBB1_10:
	CMPQ AX, SI
	JE   LBB1_12

LBB1_11:
	VMOVSS X0, (DI)(AX*4)
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB1_11

LBB1_12:
	VZEROUPPER
	RET

DATA dataRangeF64<>+0(SB)/8, $0x0000000000000000
DATA dataRangeF64<>+8(SB)/8, $0x3ff0000000000000
DATA dataRangeF64<>+16(SB)/8, $0x4000000000000000
DATA dataRangeF64<>+24(SB)/8, $0x4008000000000000
DATA dataRangeF64<>+32(SB)/8, $0x4010000000000000
DATA dataRangeF64<>+40(SB)/8, $0x4020000000000000
DATA dataRangeF64<>+48(SB)/8, $0x4028000000000000
DATA dataRangeF64<>+56(SB)/8, $0x4030000000000000
DATA dataRangeF64<>+64(SB)/8, $0x4034000000000000
DATA dataRangeF64<>+72(SB)/8, $0x4038000000000000
DATA dataRangeF64<>+80(SB)/8, $0x403c000000000000
DATA dataRangeF64<>+88(SB)/8, $0x4040000000000000
DATA dataRangeF64<>+96(SB)/8, $0x3ff0000000000000
GLOBL dataRangeF64<>(SB), RODATA|NOPTR, $104

// func Range_AVX2_F64(x []float64, a float64, n int)
// Requires: AVX, AVX2, SSE2
TEXT ·Range_AVX2_F64(SB), NOSPLIT, $0-40
	MOVQ  x_base+0(FP), DI
	MOVSD a+24(FP), X0
	MOVQ  n+32(FP), SI
	TESTQ SI, SI
	JE    LBB2_13
	CMPQ  SI, $0x10
	JAE   LBB2_3
	XORL  AX, AX
	JMP   LBB2_11

LBB2_3:
	MOVQ         SI, AX
	ANDQ         $-16, AX
	VBROADCASTSD X0, Y1
	VADDPD       dataRangeF64<>+0(SB), Y1, Y1
	LEAQ         -16(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x04, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB2_4
	MOVQ         R8, DX
	ANDQ         $-2, DX
	XORL         CX, CX
	VBROADCASTSD dataRangeF64<>+32(SB), Y2
	VBROADCASTSD dataRangeF64<>+40(SB), Y3
	VBROADCASTSD dataRangeF64<>+48(SB), Y4
	VBROADCASTSD dataRangeF64<>+56(SB), Y5
	VBROADCASTSD dataRangeF64<>+64(SB), Y6
	VBROADCASTSD dataRangeF64<>+72(SB), Y7
	VBROADCASTSD dataRangeF64<>+80(SB), Y8
	VBROADCASTSD dataRangeF64<>+88(SB), Y9

LBB2_6:
	VADDPD  Y2, Y1, Y10
	VADDPD  Y3, Y1, Y11
	VADDPD  Y4, Y1, Y12
	VMOVUPD Y1, (DI)(CX*8)
	VMOVUPD Y10, 32(DI)(CX*8)
	VMOVUPD Y11, 64(DI)(CX*8)
	VMOVUPD Y12, 96(DI)(CX*8)
	VADDPD  Y5, Y1, Y10
	VADDPD  Y6, Y1, Y11
	VADDPD  Y7, Y1, Y12
	VADDPD  Y1, Y8, Y13
	VMOVUPD Y10, 128(DI)(CX*8)
	VMOVUPD Y11, 160(DI)(CX*8)
	VMOVUPD Y12, 192(DI)(CX*8)
	VMOVUPD Y13, 224(DI)(CX*8)
	ADDQ    $0x20, CX
	VADDPD  Y1, Y9, Y1
	ADDQ    $-2, DX
	JNE     LBB2_6
	TESTB   $0x01, R8
	JE      LBB2_9

LBB2_8:
	VBROADCASTSD dataRangeF64<>+32(SB), Y2
	VADDPD       Y2, Y1, Y2
	VBROADCASTSD dataRangeF64<>+40(SB), Y3
	VADDPD       Y3, Y1, Y3
	VBROADCASTSD dataRangeF64<>+48(SB), Y4
	VADDPD       Y4, Y1, Y4
	VMOVUPD      Y1, (DI)(CX*8)
	VMOVUPD      Y2, 32(DI)(CX*8)
	VMOVUPD      Y3, 64(DI)(CX*8)
	VMOVUPD      Y4, 96(DI)(CX*8)

LBB2_9:
	CMPQ       AX, SI
	JE         LBB2_13
	VCVTSI2SDQ AX, X14, X1
	VADDSD     X0, X1, X0

LBB2_11:
	VMOVSD dataRangeF64<>+96(SB), X1

LBB2_12:
	VMOVSD X0, (DI)(AX*8)
	VADDSD X1, X0, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB2_12

LBB2_13:
	VZEROUPPER
	RET

LBB2_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB2_8
	JMP   LBB2_9

DATA dataRangeF32<>+0(SB)/4, $0x00000000
DATA dataRangeF32<>+4(SB)/4, $0x3f800000
DATA dataRangeF32<>+8(SB)/4, $0x40000000
DATA dataRangeF32<>+12(SB)/4, $0x40400000
DATA dataRangeF32<>+16(SB)/4, $0x40800000
DATA dataRangeF32<>+20(SB)/4, $0x40a00000
DATA dataRangeF32<>+24(SB)/4, $0x40c00000
DATA dataRangeF32<>+28(SB)/4, $0x40e00000
DATA dataRangeF32<>+32(SB)/4, $0x41000000
DATA dataRangeF32<>+36(SB)/4, $0x41800000
DATA dataRangeF32<>+40(SB)/4, $0x41c00000
DATA dataRangeF32<>+44(SB)/4, $0x42000000
DATA dataRangeF32<>+48(SB)/4, $0x42200000
DATA dataRangeF32<>+52(SB)/4, $0x42400000
DATA dataRangeF32<>+56(SB)/4, $0x42600000
DATA dataRangeF32<>+60(SB)/4, $0x42800000
DATA dataRangeF32<>+64(SB)/4, $0x3f800000
GLOBL dataRangeF32<>(SB), RODATA|NOPTR, $68

// func Range_AVX2_F32(x []float32, a float32, n int)
// Requires: AVX, AVX2, SSE
TEXT ·Range_AVX2_F32(SB), NOSPLIT, $0-40
	MOVQ  x_base+0(FP), DI
	MOVSS a+24(FP), X0
	MOVQ  n+32(FP), SI
	TESTQ SI, SI
	JE    LBB3_13
	CMPQ  SI, $0x20
	JAE   LBB3_3
	XORL  AX, AX
	JMP   LBB3_11

LBB3_3:
	MOVQ         SI, AX
	ANDQ         $-32, AX
	VBROADCASTSS X0, Y1
	VADDPS       dataRangeF32<>+0(SB), Y1, Y1
	LEAQ         -32(AX), CX
	MOVQ         CX, R8
	SHRQ         $0x05, R8
	ADDQ         $0x01, R8
	TESTQ        CX, CX
	JE           LBB3_4
	MOVQ         R8, DX
	ANDQ         $-2, DX
	XORL         CX, CX
	VBROADCASTSS dataRangeF32<>+32(SB), Y2
	VBROADCASTSS dataRangeF32<>+36(SB), Y3
	VBROADCASTSS dataRangeF32<>+40(SB), Y4
	VBROADCASTSS dataRangeF32<>+44(SB), Y5
	VBROADCASTSS dataRangeF32<>+48(SB), Y6
	VBROADCASTSS dataRangeF32<>+52(SB), Y7
	VBROADCASTSS dataRangeF32<>+56(SB), Y8
	VBROADCASTSS dataRangeF32<>+60(SB), Y9

LBB3_6:
	VADDPS  Y2, Y1, Y10
	VADDPS  Y3, Y1, Y11
	VADDPS  Y4, Y1, Y12
	VMOVUPS Y1, (DI)(CX*4)
	VMOVUPS Y10, 32(DI)(CX*4)
	VMOVUPS Y11, 64(DI)(CX*4)
	VMOVUPS Y12, 96(DI)(CX*4)
	VADDPS  Y5, Y1, Y10
	VADDPS  Y6, Y1, Y11
	VADDPS  Y7, Y1, Y12
	VADDPS  Y1, Y8, Y13
	VMOVUPS Y10, 128(DI)(CX*4)
	VMOVUPS Y11, 160(DI)(CX*4)
	VMOVUPS Y12, 192(DI)(CX*4)
	VMOVUPS Y13, 224(DI)(CX*4)
	ADDQ    $0x40, CX
	VADDPS  Y1, Y9, Y1
	ADDQ    $-2, DX
	JNE     LBB3_6
	TESTB   $0x01, R8
	JE      LBB3_9

LBB3_8:
	VBROADCASTSS dataRangeF32<>+32(SB), Y2
	VADDPS       Y2, Y1, Y2
	VBROADCASTSS dataRangeF32<>+36(SB), Y3
	VADDPS       Y3, Y1, Y3
	VBROADCASTSS dataRangeF32<>+40(SB), Y4
	VADDPS       Y4, Y1, Y4
	VMOVUPS      Y1, (DI)(CX*4)
	VMOVUPS      Y2, 32(DI)(CX*4)
	VMOVUPS      Y3, 64(DI)(CX*4)
	VMOVUPS      Y4, 96(DI)(CX*4)

LBB3_9:
	CMPQ       AX, SI
	JE         LBB3_13
	VCVTSI2SSQ AX, X14, X1
	VADDSS     X0, X1, X0

LBB3_11:
	VMOVSS dataRangeF32<>+64(SB), X1

LBB3_12:
	VMOVSS X0, (DI)(AX*4)
	VADDSS X1, X0, X0
	ADDQ   $0x01, AX
	CMPQ   SI, AX
	JNE    LBB3_12

LBB3_13:
	VZEROUPPER
	RET

LBB3_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB3_8
	JMP   LBB3_9

DATA dataFromBoolF64<>+0(SB)/4, $+1
DATA dataFromBoolF64<>+4(SB)/8, $0x3ff0000000000000
GLOBL dataFromBoolF64<>(SB), RODATA|NOPTR, $12

// func FromBool_AVX2_F64(x []float64, y []bool)
// Requires: AVX, AVX2
TEXT ·FromBool_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB4_10
	CMPQ  DX, $0x10
	JAE   LBB4_3
	XORL  AX, AX
	JMP   LBB4_6

LBB4_3:
	MOVQ         DX, AX
	ANDQ         $-16, AX
	XORL         CX, CX
	VPXOR        X0, X0, X0
	VPCMPEQD     X1, X1, X1
	VPBROADCASTD dataFromBoolF64<>+0(SB), X2

LBB4_4:
	VMOVD     (SI)(CX*1), X3
	VMOVD     4(SI)(CX*1), X4
	VMOVD     8(SI)(CX*1), X5
	VMOVD     12(SI)(CX*1), X6
	VPCMPEQB  X0, X3, X3
	VPXOR     X1, X3, X3
	VPMOVZXBD X3, X3
	VPAND     X2, X3, X3
	VCVTDQ2PD X3, Y3
	VPCMPEQB  X0, X4, X4
	VPXOR     X1, X4, X4
	VPMOVZXBD X4, X4
	VPAND     X2, X4, X4
	VCVTDQ2PD X4, Y4
	VPCMPEQB  X0, X5, X5
	VPXOR     X1, X5, X5
	VPMOVZXBD X5, X5
	VPAND     X2, X5, X5
	VCVTDQ2PD X5, Y5
	VPCMPEQB  X0, X6, X6
	VPXOR     X1, X6, X6
	VPMOVZXBD X6, X6
	VPAND     X2, X6, X6
	VCVTDQ2PD X6, Y6
	VMOVUPS   Y3, (DI)(CX*8)
	VMOVUPS   Y4, 32(DI)(CX*8)
	VMOVUPS   Y5, 64(DI)(CX*8)
	VMOVUPS   Y6, 96(DI)(CX*8)
	ADDQ      $0x10, CX
	CMPQ      AX, CX
	JNE       LBB4_4
	CMPQ      AX, DX
	JNE       LBB4_6

LBB4_10:
	VZEROUPPER
	RET

LBB4_6:
	VMOVQ dataFromBoolF64<>+4(SB), X0
	JMP   LBB4_7

LBB4_9:
	VMOVQ X1, (DI)(AX*8)
	ADDQ  $0x01, AX
	CMPQ  DX, AX
	JE    LBB4_10

LBB4_7:
	CMPB    (SI)(AX*1), $0x00
	VMOVDQA X0, X1
	JNE     LBB4_9
	VPXOR   X1, X1, X1
	JMP     LBB4_9

DATA dataFromBoolF32<>+0(SB)/4, $+1
DATA dataFromBoolF32<>+4(SB)/4, $+1065353216
GLOBL dataFromBoolF32<>(SB), RODATA|NOPTR, $8

// func FromBool_AVX2_F32(x []float32, y []bool)
// Requires: AVX, AVX2
TEXT ·FromBool_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB5_10
	CMPQ  DX, $0x20
	JAE   LBB5_3
	XORL  AX, AX
	JMP   LBB5_6

LBB5_3:
	MOVQ         DX, AX
	ANDQ         $-32, AX
	XORL         CX, CX
	VPXOR        X0, X0, X0
	VPCMPEQD     X1, X1, X1
	VPBROADCASTD dataFromBoolF32<>+0(SB), Y2

LBB5_4:
	VMOVQ     (SI)(CX*1), X3
	VMOVQ     8(SI)(CX*1), X4
	VMOVQ     16(SI)(CX*1), X5
	VMOVQ     24(SI)(CX*1), X6
	VPCMPEQB  X0, X3, X3
	VPXOR     X1, X3, X3
	VPMOVZXBD X3, Y3
	VPAND     Y2, Y3, Y3
	VCVTDQ2PS Y3, Y3
	VPCMPEQB  X0, X4, X4
	VPXOR     X1, X4, X4
	VPMOVZXBD X4, Y4
	VPAND     Y2, Y4, Y4
	VCVTDQ2PS Y4, Y4
	VPCMPEQB  X0, X5, X5
	VPXOR     X1, X5, X5
	VPMOVZXBD X5, Y5
	VPAND     Y2, Y5, Y5
	VCVTDQ2PS Y5, Y5
	VPCMPEQB  X0, X6, X6
	VPXOR     X1, X6, X6
	VPMOVZXBD X6, Y6
	VPAND     Y2, Y6, Y6
	VCVTDQ2PS Y6, Y6
	VMOVUPS   Y3, (DI)(CX*4)
	VMOVUPS   Y4, 32(DI)(CX*4)
	VMOVUPS   Y5, 64(DI)(CX*4)
	VMOVUPS   Y6, 96(DI)(CX*4)
	ADDQ      $0x20, CX
	CMPQ      AX, CX
	JNE       LBB5_4
	CMPQ      AX, DX
	JNE       LBB5_6

LBB5_10:
	VZEROUPPER
	RET

LBB5_6:
	VMOVD dataFromBoolF32<>+4(SB), X0
	JMP   LBB5_7

LBB5_9:
	VMOVD X1, (DI)(AX*4)
	ADDQ  $0x01, AX
	CMPQ  DX, AX
	JE    LBB5_10

LBB5_7:
	CMPB    (SI)(AX*1), $0x00
	VMOVDQA X0, X1
	JNE     LBB5_9
	VPXOR   X1, X1, X1
	JMP     LBB5_9

// func FromInt32_AVX2_F64(x []float64, y []int32)
// Requires: AVX
TEXT ·FromInt32_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB10_11
	CMPQ  DX, $0x10
	JAE   LBB10_3
	XORL  AX, AX
	JMP   LBB10_10

LBB10_3:
	MOVQ  DX, AX
	ANDQ  $-16, AX
	LEAQ  -16(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x04, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB10_4
	MOVQ  R8, R9
	ANDQ  $-2, R9
	XORL  CX, CX

LBB10_6:
	VCVTDQ2PD (SI)(CX*4), Y0
	VCVTDQ2PD 16(SI)(CX*4), Y1
	VCVTDQ2PD 32(SI)(CX*4), Y2
	VCVTDQ2PD 48(SI)(CX*4), Y3
	VMOVUPS   Y0, (DI)(CX*8)
	VMOVUPS   Y1, 32(DI)(CX*8)
	VMOVUPS   Y2, 64(DI)(CX*8)
	VMOVUPS   Y3, 96(DI)(CX*8)
	VCVTDQ2PD 64(SI)(CX*4), Y0
	VCVTDQ2PD 80(SI)(CX*4), Y1
	VCVTDQ2PD 96(SI)(CX*4), Y2
	VCVTDQ2PD 112(SI)(CX*4), Y3
	VMOVUPD   Y0, 128(DI)(CX*8)
	VMOVUPS   Y1, 160(DI)(CX*8)
	VMOVUPS   Y2, 192(DI)(CX*8)
	VMOVUPS   Y3, 224(DI)(CX*8)
	ADDQ      $0x20, CX
	ADDQ      $-2, R9
	JNE       LBB10_6
	TESTB     $0x01, R8
	JE        LBB10_9

LBB10_8:
	VCVTDQ2PD (SI)(CX*4), Y0
	VCVTDQ2PD 16(SI)(CX*4), Y1
	VCVTDQ2PD 32(SI)(CX*4), Y2
	VCVTDQ2PD 48(SI)(CX*4), Y3
	VMOVUPD   Y0, (DI)(CX*8)
	VMOVUPS   Y1, 32(DI)(CX*8)
	VMOVUPS   Y2, 64(DI)(CX*8)
	VMOVUPS   Y3, 96(DI)(CX*8)

LBB10_9:
	CMPQ AX, DX
	JE   LBB10_11

LBB10_10:
	VCVTSI2SDL (SI)(AX*4), X4, X0
	VMOVSD     X0, (DI)(AX*8)
	ADDQ       $0x01, AX
	CMPQ       DX, AX
	JNE        LBB10_10

LBB10_11:
	VZEROUPPER
	RET

LBB10_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB10_8
	JMP   LBB10_9

// func FromInt32_AVX2_F32(x []float32, y []int32)
// Requires: AVX
TEXT ·FromInt32_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB11_11
	CMPQ  DX, $0x20
	JAE   LBB11_3
	XORL  AX, AX
	JMP   LBB11_10

LBB11_3:
	MOVQ  DX, AX
	ANDQ  $-32, AX
	LEAQ  -32(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x05, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB11_4
	MOVQ  R8, R9
	ANDQ  $-2, R9
	XORL  CX, CX

LBB11_6:
	VCVTDQ2PS (SI)(CX*4), Y0
	VCVTDQ2PS 32(SI)(CX*4), Y1
	VCVTDQ2PS 64(SI)(CX*4), Y2
	VCVTDQ2PS 96(SI)(CX*4), Y3
	VMOVUPS   Y0, (DI)(CX*4)
	VMOVUPS   Y1, 32(DI)(CX*4)
	VMOVUPS   Y2, 64(DI)(CX*4)
	VMOVUPS   Y3, 96(DI)(CX*4)
	VCVTDQ2PS 128(SI)(CX*4), Y0
	VCVTDQ2PS 160(SI)(CX*4), Y1
	VCVTDQ2PS 192(SI)(CX*4), Y2
	VCVTDQ2PS 224(SI)(CX*4), Y3
	VMOVUPS   Y0, 128(DI)(CX*4)
	VMOVUPS   Y1, 160(DI)(CX*4)
	VMOVUPS   Y2, 192(DI)(CX*4)
	VMOVUPS   Y3, 224(DI)(CX*4)
	ADDQ      $0x40, CX
	ADDQ      $-2, R9
	JNE       LBB11_6
	TESTB     $0x01, R8
	JE        LBB11_9

LBB11_8:
	VCVTDQ2PS (SI)(CX*4), Y0
	VCVTDQ2PS 32(SI)(CX*4), Y1
	VCVTDQ2PS 64(SI)(CX*4), Y2
	VCVTDQ2PS 96(SI)(CX*4), Y3
	VMOVUPS   Y0, (DI)(CX*4)
	VMOVUPS   Y1, 32(DI)(CX*4)
	VMOVUPS   Y2, 64(DI)(CX*4)
	VMOVUPS   Y3, 96(DI)(CX*4)

LBB11_9:
	CMPQ AX, DX
	JE   LBB11_11

LBB11_10:
	VCVTSI2SSL (SI)(AX*4), X4, X0
	VMOVSS     X0, (DI)(AX*4)
	ADDQ       $0x01, AX
	CMPQ       DX, AX
	JNE        LBB11_10

LBB11_11:
	VZEROUPPER
	RET

LBB11_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB11_8
	JMP   LBB11_9

// func FromInt64_AVX2_F64(x []float64, y []int64)
// Requires: AVX
TEXT ·FromInt64_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB8_11
	CMPQ  DX, $0x10
	JAE   LBB8_3
	XORL  R10, R10
	JMP   LBB8_10

LBB8_3:
	MOVQ  DX, R10
	ANDQ  $-16, R10
	LEAQ  -16(R10), CX
	MOVQ  CX, R8
	SHRQ  $0x04, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB8_4
	MOVQ  R8, R9
	ANDQ  $-2, R9
	XORL  CX, CX

LBB8_6:
	VMOVDQU    (SI)(CX*8), X0
	VMOVDQU    16(SI)(CX*8), X1
	VPEXTRQ    $0x01, X0, AX
	VCVTSI2SDQ AX, X11, X2
	VMOVDQU    32(SI)(CX*8), X3
	VMOVQ      X0, AX
	VCVTSI2SDQ AX, X11, X0
	VPEXTRQ    $0x01, X1, AX
	VCVTSI2SDQ AX, X11, X4
	VMOVDQU    48(SI)(CX*8), X5
	VMOVQ      X1, AX
	VCVTSI2SDQ AX, X11, X1
	VPEXTRQ    $0x01, X5, AX
	VCVTSI2SDQ AX, X11, X6
	VUNPCKLPD  X2, X0, X8
	VMOVQ      X5, AX
	VCVTSI2SDQ AX, X11, X2
	VPEXTRQ    $0x01, X3, AX
	VCVTSI2SDQ AX, X11, X5
	VUNPCKLPD  X4, X1, X10
	VMOVQ      X3, AX
	VCVTSI2SDQ AX, X11, X3
	VUNPCKLPD  X6, X2, X9
	VMOVDQU    80(SI)(CX*8), X4
	VPEXTRQ    $0x01, X4, AX
	VUNPCKLPD  X5, X3, X3
	VCVTSI2SDQ AX, X11, X5
	VMOVQ      X4, AX
	VCVTSI2SDQ AX, X11, X4
	VUNPCKLPD  X5, X4, X4
	VMOVDQU    64(SI)(CX*8), X5
	VPEXTRQ    $0x01, X5, AX
	VCVTSI2SDQ AX, X11, X6
	VMOVQ      X5, AX
	VCVTSI2SDQ AX, X11, X5
	VMOVDQU    112(SI)(CX*8), X7
	VPEXTRQ    $0x01, X7, AX
	VCVTSI2SDQ AX, X11, X0
	VMOVQ      X7, AX
	VCVTSI2SDQ AX, X11, X7
	VMOVDQU    96(SI)(CX*8), X2
	VPEXTRQ    $0x01, X2, AX
	VCVTSI2SDQ AX, X11, X1
	VUNPCKLPD  X6, X5, X5
	VMOVQ      X2, AX
	VCVTSI2SDQ AX, X11, X2
	VUNPCKLPD  X0, X7, X0
	VUNPCKLPD  X1, X2, X1
	VMOVUPD    X10, 16(DI)(CX*8)
	VMOVUPD    X8, (DI)(CX*8)
	VMOVUPD    X3, 32(DI)(CX*8)
	VMOVUPD    X9, 48(DI)(CX*8)
	VMOVUPD    X5, 64(DI)(CX*8)
	VMOVUPD    X4, 80(DI)(CX*8)
	VMOVUPD    X1, 96(DI)(CX*8)
	VMOVUPD    X0, 112(DI)(CX*8)
	VMOVDQU    128(SI)(CX*8), X0
	VMOVDQU    144(SI)(CX*8), X1
	VPEXTRQ    $0x01, X0, AX
	VCVTSI2SDQ AX, X11, X2
	VMOVDQU    160(SI)(CX*8), X3
	VMOVQ      X0, AX
	VCVTSI2SDQ AX, X11, X0
	VPEXTRQ    $0x01, X1, AX
	VCVTSI2SDQ AX, X11, X4
	VMOVDQU    176(SI)(CX*8), X5
	VMOVQ      X1, AX
	VCVTSI2SDQ AX, X11, X1
	VPEXTRQ    $0x01, X5, AX
	VCVTSI2SDQ AX, X11, X6
	VUNPCKLPD  X2, X0, X8
	VMOVQ      X5, AX
	VCVTSI2SDQ AX, X11, X2
	VPEXTRQ    $0x01, X3, AX
	VCVTSI2SDQ AX, X11, X5
	VUNPCKLPD  X4, X1, X10
	VMOVQ      X3, AX
	VCVTSI2SDQ AX, X11, X3
	VUNPCKLPD  X6, X2, X9
	VMOVDQU    208(SI)(CX*8), X4
	VPEXTRQ    $0x01, X4, AX
	VUNPCKLPD  X5, X3, X3
	VCVTSI2SDQ AX, X11, X5
	VMOVQ      X4, AX
	VCVTSI2SDQ AX, X11, X4
	VUNPCKLPD  X5, X4, X4
	VMOVDQU    192(SI)(CX*8), X5
	VPEXTRQ    $0x01, X5, AX
	VCVTSI2SDQ AX, X11, X6
	VMOVQ      X5, AX
	VCVTSI2SDQ AX, X11, X5
	VMOVDQU    240(SI)(CX*8), X7
	VPEXTRQ    $0x01, X7, AX
	VCVTSI2SDQ AX, X11, X0
	VMOVQ      X7, AX
	VCVTSI2SDQ AX, X11, X7
	VMOVDQU    224(SI)(CX*8), X2
	VPEXTRQ    $0x01, X2, AX
	VCVTSI2SDQ AX, X11, X1
	VUNPCKLPD  X6, X5, X5
	VMOVQ      X2, AX
	VCVTSI2SDQ AX, X11, X2
	VUNPCKLPD  X0, X7, X0
	VUNPCKLPD  X1, X2, X1
	VMOVUPD    X10, 144(DI)(CX*8)
	VMOVUPD    X8, 128(DI)(CX*8)
	VMOVUPD    X3, 160(DI)(CX*8)
	VMOVUPD    X9, 176(DI)(CX*8)
	VMOVUPD    X5, 192(DI)(CX*8)
	VMOVUPD    X4, 208(DI)(CX*8)
	VMOVUPD    X1, 224(DI)(CX*8)
	VMOVUPD    X0, 240(DI)(CX*8)
	ADDQ       $0x20, CX
	ADDQ       $-2, R9
	JNE        LBB8_6
	TESTB      $0x01, R8
	JE         LBB8_9

LBB8_8:
	VMOVDQU    (SI)(CX*8), X0
	VMOVDQU    16(SI)(CX*8), X1
	VMOVDQU    32(SI)(CX*8), X3
	VMOVDQU    48(SI)(CX*8), X2
	VPEXTRQ    $0x01, X0, AX
	VCVTSI2SDQ AX, X11, X4
	VMOVQ      X0, AX
	VCVTSI2SDQ AX, X11, X0
	VUNPCKLPD  X4, X0, X8
	VPEXTRQ    $0x01, X1, AX
	VCVTSI2SDQ AX, X11, X4
	VMOVQ      X1, AX
	VCVTSI2SDQ AX, X11, X1
	VUNPCKLPD  X4, X1, X1
	VPEXTRQ    $0x01, X2, AX
	VCVTSI2SDQ AX, X11, X4
	VMOVQ      X2, AX
	VCVTSI2SDQ AX, X11, X2
	VUNPCKLPD  X4, X2, X2
	VPEXTRQ    $0x01, X3, AX
	VCVTSI2SDQ AX, X11, X4
	VMOVQ      X3, AX
	VCVTSI2SDQ AX, X11, X3
	VMOVDQU    80(SI)(CX*8), X5
	VPEXTRQ    $0x01, X5, AX
	VCVTSI2SDQ AX, X11, X6
	VMOVQ      X5, AX
	VCVTSI2SDQ AX, X11, X5
	VMOVDQU    64(SI)(CX*8), X7
	VPEXTRQ    $0x01, X7, AX
	VCVTSI2SDQ AX, X11, X0
	VUNPCKLPD  X4, X3, X3
	VMOVQ      X7, AX
	VCVTSI2SDQ AX, X11, X4
	VUNPCKLPD  X6, X5, X5
	VMOVDQU    112(SI)(CX*8), X6
	VPEXTRQ    $0x01, X6, AX
	VUNPCKLPD  X0, X4, X0
	VCVTSI2SDQ AX, X11, X4
	VMOVQ      X6, AX
	VCVTSI2SDQ AX, X11, X6
	VUNPCKLPD  X4, X6, X4
	VMOVDQU    96(SI)(CX*8), X6
	VPEXTRQ    $0x01, X6, AX
	VCVTSI2SDQ AX, X11, X7
	VMOVQ      X6, AX
	VCVTSI2SDQ AX, X11, X6
	VUNPCKLPD  X7, X6, X6
	VMOVUPD    X1, 16(DI)(CX*8)
	VMOVUPD    X8, (DI)(CX*8)
	VMOVUPD    X3, 32(DI)(CX*8)
	VMOVUPD    X2, 48(DI)(CX*8)
	VMOVUPD    X0, 64(DI)(CX*8)
	VMOVUPD    X5, 80(DI)(CX*8)
	VMOVUPD    X6, 96(DI)(CX*8)
	VMOVUPD    X4, 112(DI)(CX*8)

LBB8_9:
	CMPQ R10, DX
	JE   LBB8_11

LBB8_10:
	VCVTSI2SDQ (SI)(R10*8), X11, X0
	VMOVSD     X0, (DI)(R10*8)
	ADDQ       $0x01, R10
	CMPQ       DX, R10
	JNE        LBB8_10

LBB8_11:
	RET

LBB8_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB8_8
	JMP   LBB8_9

// func FromInt64_AVX2_F32(x []float32, y []int64)
// Requires: AVX
TEXT ·FromInt64_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB9_11
	CMPQ  DX, $0x10
	JAE   LBB9_3
	XORL  R11, R11
	JMP   LBB9_10

LBB9_3:
	MOVQ  DX, R11
	ANDQ  $-16, R11
	LEAQ  -16(R11), CX
	MOVQ  CX, R8
	SHRQ  $0x04, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB9_4
	MOVQ  R8, R9
	ANDQ  $-2, R9
	XORL  CX, CX

LBB9_6:
	VMOVDQU    (SI)(CX*8), X0
	VPEXTRQ    $0x01, X0, R10
	VMOVDQU    16(SI)(CX*8), X1
	VCVTSI2SSQ R10, X8, X2
	VMOVQ      X0, AX
	VCVTSI2SSQ AX, X8, X0
	VMOVQ      X1, AX
	VCVTSI2SSQ AX, X8, X3
	VPEXTRQ    $0x01, X1, AX
	VCVTSI2SSQ AX, X8, X1
	VMOVDQU    32(SI)(CX*8), X4
	VPEXTRQ    $0x01, X4, AX
	VMOVDQU    48(SI)(CX*8), X5
	VCVTSI2SSQ AX, X8, X6
	VMOVQ      X4, AX
	VCVTSI2SSQ AX, X8, X4
	VMOVQ      X5, AX
	VCVTSI2SSQ AX, X8, X7
	VINSERTPS  $0x10, X2, X0, X0
	VINSERTPS  $0x20, X3, X0, X0
	VPEXTRQ    $0x01, X5, AX
	VINSERTPS  $0x30, X1, X0, X0
	VCVTSI2SSQ AX, X8, X1
	VINSERTPS  $0x10, X6, X4, X2
	VMOVDQU    64(SI)(CX*8), X3
	VPEXTRQ    $0x01, X3, AX
	VCVTSI2SSQ AX, X8, X4
	VMOVQ      X3, AX
	VCVTSI2SSQ AX, X8, X3
	VMOVDQU    80(SI)(CX*8), X5
	VMOVQ      X5, AX
	VCVTSI2SSQ AX, X8, X6
	VINSERTPS  $0x20, X7, X2, X2
	VINSERTPS  $0x30, X1, X2, X1
	VPEXTRQ    $0x01, X5, AX
	VINSERTPS  $0x10, X4, X3, X2
	VCVTSI2SSQ AX, X8, X3
	VINSERTPS  $0x20, X6, X2, X2
	VMOVDQU    96(SI)(CX*8), X4
	VPEXTRQ    $0x01, X4, AX
	VCVTSI2SSQ AX, X8, X5
	VMOVQ      X4, AX
	VCVTSI2SSQ AX, X8, X4
	VMOVDQU    112(SI)(CX*8), X6
	VMOVQ      X6, AX
	VCVTSI2SSQ AX, X8, X7
	VINSERTPS  $0x30, X3, X2, X2
	VINSERTPS  $0x10, X5, X4, X3
	VPEXTRQ    $0x01, X6, AX
	VINSERTPS  $0x20, X7, X3, X3
	VCVTSI2SSQ AX, X8, X4
	VINSERTPS  $0x30, X4, X3, X3
	VMOVUPS    X0, (DI)(CX*4)
	VMOVUPS    X1, 16(DI)(CX*4)
	VMOVUPS    X2, 32(DI)(CX*4)
	VMOVUPS    X3, 48(DI)(CX*4)
	VMOVDQU    128(SI)(CX*8), X0
	VPEXTRQ    $0x01, X0, AX
	VMOVDQU    144(SI)(CX*8), X1
	VCVTSI2SSQ AX, X8, X2
	VMOVQ      X0, AX
	VCVTSI2SSQ AX, X8, X0
	VMOVQ      X1, AX
	VCVTSI2SSQ AX, X8, X3
	VPEXTRQ    $0x01, X1, AX
	VCVTSI2SSQ AX, X8, X1
	VMOVDQU    160(SI)(CX*8), X4
	VPEXTRQ    $0x01, X4, AX
	VCVTSI2SSQ AX, X8, X5
	VMOVQ      X4, AX
	VCVTSI2SSQ AX, X8, X4
	VINSERTPS  $0x10, X2, X0, X0
	VMOVDQU    176(SI)(CX*8), X2
	VPEXTRQ    $0x01, X2, R10
	VMOVQ      X2, AX
	VCVTSI2SSQ AX, X8, X2
	VINSERTPS  $0x20, X3, X0, X0
	VCVTSI2SSQ R10, X8, X3
	VINSERTPS  $0x30, X1, X0, X0
	VMOVDQU    192(SI)(CX*8), X1
	VPEXTRQ    $0x01, X1, AX
	VINSERTPS  $0x10, X5, X4, X4
	VCVTSI2SSQ AX, X8, X5
	VMOVQ      X1, AX
	VCVTSI2SSQ AX, X8, X1
	VINSERTPS  $0x20, X2, X4, X2
	VMOVDQU    208(SI)(CX*8), X4
	VPEXTRQ    $0x01, X4, R10
	VMOVQ      X4, AX
	VCVTSI2SSQ AX, X8, X4
	VINSERTPS  $0x30, X3, X2, X2
	VCVTSI2SSQ R10, X8, X3
	VINSERTPS  $0x10, X5, X1, X1
	VMOVDQU    224(SI)(CX*8), X5
	VPEXTRQ    $0x01, X5, AX
	VINSERTPS  $0x20, X4, X1, X1
	VCVTSI2SSQ AX, X8, X4
	VMOVQ      X5, AX
	VCVTSI2SSQ AX, X8, X5
	VINSERTPS  $0x30, X3, X1, X1
	VMOVDQU    240(SI)(CX*8), X3
	VPEXTRQ    $0x01, X3, R10
	VMOVQ      X3, AX
	VCVTSI2SSQ AX, X8, X3
	VINSERTPS  $0x10, X4, X5, X4
	VCVTSI2SSQ R10, X8, X5
	VINSERTPS  $0x20, X3, X4, X3
	VINSERTPS  $0x30, X5, X3, X3
	VMOVUPS    X0, 64(DI)(CX*4)
	VMOVUPS    X2, 80(DI)(CX*4)
	VMOVUPS    X1, 96(DI)(CX*4)
	VMOVUPS    X3, 112(DI)(CX*4)
	ADDQ       $0x20, CX
	ADDQ       $-2, R9
	JNE        LBB9_6
	TESTB      $0x01, R8
	JE         LBB9_9

LBB9_8:
	VMOVDQU    (SI)(CX*8), X0
	VPEXTRQ    $0x01, X0, AX
	VMOVDQU    16(SI)(CX*8), X1
	VCVTSI2SSQ AX, X8, X2
	VMOVQ      X0, AX
	VCVTSI2SSQ AX, X8, X0
	VMOVQ      X1, AX
	VCVTSI2SSQ AX, X8, X3
	VPEXTRQ    $0x01, X1, AX
	VCVTSI2SSQ AX, X8, X1
	VMOVDQU    32(SI)(CX*8), X4
	VMOVDQU    48(SI)(CX*8), X5
	VPEXTRQ    $0x01, X4, AX
	VINSERTPS  $0x10, X2, X0, X0
	VCVTSI2SSQ AX, X8, X2
	VMOVQ      X4, AX
	VCVTSI2SSQ AX, X8, X4
	VMOVQ      X5, AX
	VCVTSI2SSQ AX, X8, X6
	VINSERTPS  $0x20, X3, X0, X0
	VINSERTPS  $0x30, X1, X0, X0
	VPEXTRQ    $0x01, X5, AX
	VINSERTPS  $0x10, X2, X4, X1
	VCVTSI2SSQ AX, X8, X2
	VINSERTPS  $0x20, X6, X1, X1
	VMOVDQU    64(SI)(CX*8), X3
	VPEXTRQ    $0x01, X3, AX
	VCVTSI2SSQ AX, X8, X4
	VMOVQ      X3, AX
	VCVTSI2SSQ AX, X8, X3
	VMOVDQU    80(SI)(CX*8), X5
	VMOVQ      X5, AX
	VCVTSI2SSQ AX, X8, X6
	VINSERTPS  $0x30, X2, X1, X1
	VINSERTPS  $0x10, X4, X3, X2
	VPEXTRQ    $0x01, X5, AX
	VINSERTPS  $0x20, X6, X2, X2
	VCVTSI2SSQ AX, X8, X3
	VINSERTPS  $0x30, X3, X2, X2
	VMOVDQU    96(SI)(CX*8), X3
	VPEXTRQ    $0x01, X3, AX
	VCVTSI2SSQ AX, X8, X4
	VMOVQ      X3, AX
	VCVTSI2SSQ AX, X8, X3
	VMOVDQU    112(SI)(CX*8), X5
	VMOVQ      X5, AX
	VCVTSI2SSQ AX, X8, X6
	VINSERTPS  $0x10, X4, X3, X3
	VINSERTPS  $0x20, X6, X3, X3
	VPEXTRQ    $0x01, X5, AX
	VCVTSI2SSQ AX, X8, X4
	VINSERTPS  $0x30, X4, X3, X3
	VMOVUPS    X0, (DI)(CX*4)
	VMOVUPS    X1, 16(DI)(CX*4)
	VMOVUPS    X2, 32(DI)(CX*4)
	VMOVUPS    X3, 48(DI)(CX*4)

LBB9_9:
	CMPQ R11, DX
	JE   LBB9_11

LBB9_10:
	VCVTSI2SSQ (SI)(R11*8), X8, X0
	VMOVSS     X0, (DI)(R11*4)
	ADDQ       $0x01, R11
	CMPQ       DX, R11
	JNE        LBB9_10

LBB9_11:
	RET

LBB9_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB9_8
	JMP   LBB9_9

// func FromFloat32_AVX2_F64(x []float64, y []float32)
// Requires: AVX
TEXT ·FromFloat32_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB6_11
	CMPQ  DX, $0x10
	JAE   LBB6_3
	XORL  AX, AX
	JMP   LBB6_10

LBB6_3:
	MOVQ  DX, AX
	ANDQ  $-16, AX
	LEAQ  -16(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x04, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB6_4
	MOVQ  R8, R9
	ANDQ  $-2, R9
	XORL  CX, CX

LBB6_6:
	VCVTPS2PD (SI)(CX*4), Y0
	VCVTPS2PD 16(SI)(CX*4), Y1
	VCVTPS2PD 32(SI)(CX*4), Y2
	VCVTPS2PD 48(SI)(CX*4), Y3
	VMOVUPS   Y0, (DI)(CX*8)
	VMOVUPS   Y1, 32(DI)(CX*8)
	VMOVUPS   Y2, 64(DI)(CX*8)
	VMOVUPS   Y3, 96(DI)(CX*8)
	VCVTPS2PD 64(SI)(CX*4), Y0
	VCVTPS2PD 80(SI)(CX*4), Y1
	VCVTPS2PD 96(SI)(CX*4), Y2
	VCVTPS2PD 112(SI)(CX*4), Y3
	VMOVUPS   Y0, 128(DI)(CX*8)
	VMOVUPS   Y1, 160(DI)(CX*8)
	VMOVUPS   Y2, 192(DI)(CX*8)
	VMOVUPS   Y3, 224(DI)(CX*8)
	ADDQ      $0x20, CX
	ADDQ      $-2, R9
	JNE       LBB6_6
	TESTB     $0x01, R8
	JE        LBB6_9

LBB6_8:
	VCVTPS2PD (SI)(CX*4), Y0
	VCVTPS2PD 16(SI)(CX*4), Y1
	VCVTPS2PD 32(SI)(CX*4), Y2
	VCVTPS2PD 48(SI)(CX*4), Y3
	VMOVUPS   Y0, (DI)(CX*8)
	VMOVUPS   Y1, 32(DI)(CX*8)
	VMOVUPS   Y2, 64(DI)(CX*8)
	VMOVUPS   Y3, 96(DI)(CX*8)

LBB6_9:
	CMPQ AX, DX
	JE   LBB6_11

LBB6_10:
	VMOVSS    (SI)(AX*4), X0
	VCVTSS2SD X0, X0, X0
	VMOVSD    X0, (DI)(AX*8)
	ADDQ      $0x01, AX
	CMPQ      DX, AX
	JNE       LBB6_10

LBB6_11:
	VZEROUPPER
	RET

LBB6_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB6_8
	JMP   LBB6_9

// func FromFloat64_AVX2_F32(x []float32, y []float64)
// Requires: AVX
TEXT ·FromFloat64_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB7_11
	CMPQ  DX, $0x10
	JAE   LBB7_3
	XORL  AX, AX
	JMP   LBB7_10

LBB7_3:
	MOVQ  DX, AX
	ANDQ  $-16, AX
	LEAQ  -16(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x04, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB7_4
	MOVQ  R8, R9
	ANDQ  $-2, R9
	XORL  CX, CX

LBB7_6:
	VCVTPD2PSY (SI)(CX*8), X0
	VCVTPD2PSY 32(SI)(CX*8), X1
	VCVTPD2PSY 64(SI)(CX*8), X2
	VCVTPD2PSY 96(SI)(CX*8), X3
	VMOVUPD    X0, (DI)(CX*4)
	VMOVUPD    X1, 16(DI)(CX*4)
	VMOVUPD    X2, 32(DI)(CX*4)
	VMOVUPD    X3, 48(DI)(CX*4)
	VCVTPD2PSY 128(SI)(CX*8), X0
	VCVTPD2PSY 160(SI)(CX*8), X1
	VCVTPD2PSY 192(SI)(CX*8), X2
	VCVTPD2PSY 224(SI)(CX*8), X3
	VMOVUPD    X0, 64(DI)(CX*4)
	VMOVUPD    X1, 80(DI)(CX*4)
	VMOVUPD    X2, 96(DI)(CX*4)
	VMOVUPD    X3, 112(DI)(CX*4)
	ADDQ       $0x20, CX
	ADDQ       $-2, R9
	JNE        LBB7_6
	TESTB      $0x01, R8
	JE         LBB7_9

LBB7_8:
	VCVTPD2PSY (SI)(CX*8), X0
	VCVTPD2PSY 32(SI)(CX*8), X1
	VCVTPD2PSY 64(SI)(CX*8), X2
	VCVTPD2PSY 96(SI)(CX*8), X3
	VMOVUPD    X0, (DI)(CX*4)
	VMOVUPD    X1, 16(DI)(CX*4)
	VMOVUPD    X2, 32(DI)(CX*4)
	VMOVUPD    X3, 48(DI)(CX*4)

LBB7_9:
	CMPQ AX, DX
	JE   LBB7_11

LBB7_10:
	VMOVSD    (SI)(AX*8), X0
	VCVTSD2SS X0, X0, X0
	VMOVSS    X0, (DI)(AX*4)
	ADDQ      $0x01, AX
	CMPQ      DX, AX
	JNE       LBB7_10

LBB7_11:
	RET

LBB7_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB7_8
	JMP   LBB7_9

DATA dataToBoolF64<>+0(SB)/1, $+1
DATA dataToBoolF64<>+1(SB)/1, $+1
DATA dataToBoolF64<>+2(SB)/1, $+1
DATA dataToBoolF64<>+3(SB)/1, $+1
DATA dataToBoolF64<>+4(SB)/1, $+0
DATA dataToBoolF64<>+5(SB)/1, $+0
DATA dataToBoolF64<>+6(SB)/1, $+0
DATA dataToBoolF64<>+7(SB)/1, $+0
DATA dataToBoolF64<>+8(SB)/1, $+0
DATA dataToBoolF64<>+9(SB)/1, $+0
DATA dataToBoolF64<>+10(SB)/1, $+0
DATA dataToBoolF64<>+11(SB)/1, $+0
DATA dataToBoolF64<>+12(SB)/1, $+0
DATA dataToBoolF64<>+13(SB)/1, $+0
DATA dataToBoolF64<>+14(SB)/1, $+0
DATA dataToBoolF64<>+15(SB)/1, $+0
GLOBL dataToBoolF64<>(SB), RODATA|NOPTR, $16

// func ToBool_AVX2_F64(x []bool, y []float64)
// Requires: AVX, AVX2
TEXT ·ToBool_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB12_8
	CMPQ  DX, $0x10
	JAE   LBB12_3
	XORL  AX, AX
	JMP   LBB12_6

LBB12_3:
	MOVQ    DX, AX
	ANDQ    $-16, AX
	XORL    CX, CX
	VXORPD  X0, X0, X0
	VMOVDQU dataToBoolF64<>+0(SB), X1

LBB12_4:
	VCMPPD       $0x04, (SI)(CX*8), Y0, Y2
	VEXTRACTF128 $0x01, Y2, X3
	VPACKSSDW    X3, X2, X2
	VPACKSSDW    X2, X2, X2
	VPACKSSWB    X2, X2, X2
	VCMPPD       $0x04, 32(SI)(CX*8), Y0, Y3
	VPAND        X1, X2, X2
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X1, X3, X3
	VCMPPD       $0x04, 64(SI)(CX*8), Y0, Y4
	VPUNPCKLDQ   X3, X2, X2
	VEXTRACTF128 $0x01, Y4, X3
	VPACKSSDW    X3, X4, X3
	VPACKSSDW    X3, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X1, X3, X3
	VCMPPD       $0x04, 96(SI)(CX*8), Y0, Y4
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSDW    X4, X4, X4
	VPACKSSWB    X4, X4, X4
	VPAND        X1, X4, X4
	VPBROADCASTD X4, X4
	VPBROADCASTD X3, X3
	VPUNPCKLDQ   X4, X3, X3
	VPBLENDD     $0x0c, X3, X2, X2
	VMOVDQU      X2, (DI)(CX*1)
	ADDQ         $0x10, CX
	CMPQ         AX, CX
	JNE          LBB12_4
	CMPQ         AX, DX
	JE           LBB12_8

LBB12_6:
	VXORPD X0, X0, X0

LBB12_7:
	VUCOMISD (SI)(AX*8), X0
	SETNE    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB12_7

LBB12_8:
	VZEROUPPER
	RET

DATA dataToBoolF32<>+0(SB)/1, $+1
DATA dataToBoolF32<>+1(SB)/1, $+1
DATA dataToBoolF32<>+2(SB)/1, $+1
DATA dataToBoolF32<>+3(SB)/1, $+1
DATA dataToBoolF32<>+4(SB)/1, $+1
DATA dataToBoolF32<>+5(SB)/1, $+1
DATA dataToBoolF32<>+6(SB)/1, $+1
DATA dataToBoolF32<>+7(SB)/1, $+1
DATA dataToBoolF32<>+8(SB)/1, $+0
DATA dataToBoolF32<>+9(SB)/1, $+0
DATA dataToBoolF32<>+10(SB)/1, $+0
DATA dataToBoolF32<>+11(SB)/1, $+0
DATA dataToBoolF32<>+12(SB)/1, $+0
DATA dataToBoolF32<>+13(SB)/1, $+0
DATA dataToBoolF32<>+14(SB)/1, $+0
DATA dataToBoolF32<>+15(SB)/1, $+0
GLOBL dataToBoolF32<>(SB), RODATA|NOPTR, $16

// func ToBool_AVX2_F32(x []bool, y []float32)
// Requires: AVX, AVX2
TEXT ·ToBool_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB13_8
	CMPQ  DX, $0x20
	JAE   LBB13_3
	XORL  AX, AX
	JMP   LBB13_6

LBB13_3:
	MOVQ    DX, AX
	ANDQ    $-32, AX
	XORL    CX, CX
	VXORPS  X0, X0, X0
	VMOVDQU dataToBoolF32<>+0(SB), X1

LBB13_4:
	VCMPPS       $0x04, (SI)(CX*4), Y0, Y2
	VEXTRACTF128 $0x01, Y2, X3
	VPACKSSDW    X3, X2, X2
	VPACKSSWB    X2, X2, X2
	VCMPPS       $0x04, 32(SI)(CX*4), Y0, Y3
	VPAND        X1, X2, X2
	VEXTRACTF128 $0x01, Y3, X4
	VPACKSSDW    X4, X3, X3
	VPACKSSWB    X3, X3, X3
	VPAND        X1, X3, X3
	VCMPPS       $0x04, 64(SI)(CX*4), Y0, Y4
	VEXTRACTF128 $0x01, Y4, X5
	VPACKSSDW    X5, X4, X4
	VPACKSSWB    X4, X4, X4
	VCMPPS       $0x04, 96(SI)(CX*4), Y0, Y5
	VPAND        X1, X4, X4
	VEXTRACTF128 $0x01, Y5, X6
	VPACKSSDW    X6, X5, X5
	VPACKSSWB    X5, X5, X5
	VPAND        X1, X5, X5
	VINSERTI128  $0x01, X5, Y4, Y4
	VINSERTI128  $0x01, X3, Y2, Y2
	VPUNPCKLQDQ  Y4, Y2, Y2
	VPERMQ       $0xd8, Y2, Y2
	VMOVDQU      Y2, (DI)(CX*1)
	ADDQ         $0x20, CX
	CMPQ         AX, CX
	JNE          LBB13_4
	CMPQ         AX, DX
	JE           LBB13_8

LBB13_6:
	VXORPS X0, X0, X0

LBB13_7:
	VUCOMISS (SI)(AX*4), X0
	SETNE    (DI)(AX*1)
	ADDQ     $0x01, AX
	CMPQ     DX, AX
	JNE      LBB13_7

LBB13_8:
	VZEROUPPER
	RET

// func ToInt32_AVX2_F64(x []int32, y []float64)
// Requires: AVX
TEXT ·ToInt32_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB16_11
	CMPQ  DX, $0x10
	JAE   LBB16_3
	XORL  AX, AX
	JMP   LBB16_10

LBB16_3:
	MOVQ  DX, AX
	ANDQ  $-16, AX
	LEAQ  -16(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x04, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB16_4
	MOVQ  R8, R9
	ANDQ  $-2, R9
	XORL  CX, CX

LBB16_6:
	VCVTTPD2DQY (SI)(CX*8), X0
	VCVTTPD2DQY 32(SI)(CX*8), X1
	VCVTTPD2DQY 64(SI)(CX*8), X2
	VCVTTPD2DQY 96(SI)(CX*8), X3
	VMOVUPD     X0, (DI)(CX*4)
	VMOVUPD     X1, 16(DI)(CX*4)
	VMOVUPD     X2, 32(DI)(CX*4)
	VMOVUPD     X3, 48(DI)(CX*4)
	VCVTTPD2DQY 128(SI)(CX*8), X0
	VCVTTPD2DQY 160(SI)(CX*8), X1
	VCVTTPD2DQY 192(SI)(CX*8), X2
	VCVTTPD2DQY 224(SI)(CX*8), X3
	VMOVUPD     X0, 64(DI)(CX*4)
	VMOVUPD     X1, 80(DI)(CX*4)
	VMOVUPD     X2, 96(DI)(CX*4)
	VMOVUPD     X3, 112(DI)(CX*4)
	ADDQ        $0x20, CX
	ADDQ        $-2, R9
	JNE         LBB16_6
	TESTB       $0x01, R8
	JE          LBB16_9

LBB16_8:
	VCVTTPD2DQY (SI)(CX*8), X0
	VCVTTPD2DQY 32(SI)(CX*8), X1
	VCVTTPD2DQY 64(SI)(CX*8), X2
	VCVTTPD2DQY 96(SI)(CX*8), X3
	VMOVUPD     X0, (DI)(CX*4)
	VMOVUPD     X1, 16(DI)(CX*4)
	VMOVUPD     X2, 32(DI)(CX*4)
	VMOVUPD     X3, 48(DI)(CX*4)

LBB16_9:
	CMPQ AX, DX
	JE   LBB16_11

LBB16_10:
	VCVTTSD2SI (SI)(AX*8), CX
	MOVL       CX, (DI)(AX*4)
	ADDQ       $0x01, AX
	CMPQ       DX, AX
	JNE        LBB16_10

LBB16_11:
	RET

LBB16_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB16_8
	JMP   LBB16_9

// func ToInt32_AVX2_F32(x []int32, y []float32)
// Requires: AVX
TEXT ·ToInt32_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB17_11
	CMPQ  DX, $0x20
	JAE   LBB17_3
	XORL  AX, AX
	JMP   LBB17_10

LBB17_3:
	MOVQ  DX, AX
	ANDQ  $-32, AX
	LEAQ  -32(AX), CX
	MOVQ  CX, R8
	SHRQ  $0x05, R8
	ADDQ  $0x01, R8
	TESTQ CX, CX
	JE    LBB17_4
	MOVQ  R8, R9
	ANDQ  $-2, R9
	XORL  CX, CX

LBB17_6:
	VCVTTPS2DQ (SI)(CX*4), Y0
	VCVTTPS2DQ 32(SI)(CX*4), Y1
	VCVTTPS2DQ 64(SI)(CX*4), Y2
	VCVTTPS2DQ 96(SI)(CX*4), Y3
	VMOVUPS    Y0, (DI)(CX*4)
	VMOVUPS    Y1, 32(DI)(CX*4)
	VMOVUPS    Y2, 64(DI)(CX*4)
	VMOVUPS    Y3, 96(DI)(CX*4)
	VCVTTPS2DQ 128(SI)(CX*4), Y0
	VCVTTPS2DQ 160(SI)(CX*4), Y1
	VCVTTPS2DQ 192(SI)(CX*4), Y2
	VCVTTPS2DQ 224(SI)(CX*4), Y3
	VMOVUPS    Y0, 128(DI)(CX*4)
	VMOVUPS    Y1, 160(DI)(CX*4)
	VMOVUPS    Y2, 192(DI)(CX*4)
	VMOVUPS    Y3, 224(DI)(CX*4)
	ADDQ       $0x40, CX
	ADDQ       $-2, R9
	JNE        LBB17_6
	TESTB      $0x01, R8
	JE         LBB17_9

LBB17_8:
	VCVTTPS2DQ (SI)(CX*4), Y0
	VCVTTPS2DQ 32(SI)(CX*4), Y1
	VCVTTPS2DQ 64(SI)(CX*4), Y2
	VCVTTPS2DQ 96(SI)(CX*4), Y3
	VMOVUPS    Y0, (DI)(CX*4)
	VMOVUPS    Y1, 32(DI)(CX*4)
	VMOVUPS    Y2, 64(DI)(CX*4)
	VMOVUPS    Y3, 96(DI)(CX*4)

LBB17_9:
	CMPQ AX, DX
	JE   LBB17_11

LBB17_10:
	VCVTTSS2SI (SI)(AX*4), CX
	MOVL       CX, (DI)(AX*4)
	ADDQ       $0x01, AX
	CMPQ       DX, AX
	JNE        LBB17_10

LBB17_11:
	VZEROUPPER
	RET

LBB17_4:
	XORL  CX, CX
	TESTB $0x01, R8
	JNE   LBB17_8
	JMP   LBB17_9

// func ToInt64_AVX2_F64(x []int64, y []float64)
// Requires: AVX
TEXT ·ToInt64_AVX2_F64(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB14_8
	LEAQ  -1(DX), CX
	MOVL  DX, R8
	ANDL  $0x03, R8
	CMPQ  CX, $0x03
	JAE   LBB14_3
	XORL  CX, CX
	JMP   LBB14_5

LBB14_3:
	ANDQ $-4, DX
	XORL CX, CX

LBB14_4:
	VCVTTSD2SIQ (SI)(CX*8), AX
	MOVQ        AX, (DI)(CX*8)
	VCVTTSD2SIQ 8(SI)(CX*8), AX
	MOVQ        AX, 8(DI)(CX*8)
	VCVTTSD2SIQ 16(SI)(CX*8), AX
	MOVQ        AX, 16(DI)(CX*8)
	VCVTTSD2SIQ 24(SI)(CX*8), AX
	MOVQ        AX, 24(DI)(CX*8)
	ADDQ        $0x04, CX
	CMPQ        DX, CX
	JNE         LBB14_4

LBB14_5:
	TESTQ R8, R8
	JE    LBB14_8
	LEAQ  (DI)(CX*8), DX
	LEAQ  (SI)(CX*8), CX
	XORL  SI, SI

LBB14_7:
	VCVTTSD2SIQ (CX)(SI*8), AX
	MOVQ        AX, (DX)(SI*8)
	ADDQ        $0x01, SI
	CMPQ        R8, SI
	JNE         LBB14_7

LBB14_8:
	RET

// func ToInt64_AVX2_F32(x []int64, y []float32)
// Requires: AVX
TEXT ·ToInt64_AVX2_F32(SB), NOSPLIT, $0-48
	MOVQ  x_base+0(FP), DI
	MOVQ  y_base+24(FP), SI
	MOVQ  x_len+8(FP), DX
	TESTQ DX, DX
	JE    LBB15_8
	LEAQ  -1(DX), CX
	MOVL  DX, R8
	ANDL  $0x03, R8
	CMPQ  CX, $0x03
	JAE   LBB15_3
	XORL  CX, CX
	JMP   LBB15_5

LBB15_3:
	ANDQ $-4, DX
	XORL CX, CX

LBB15_4:
	VCVTTSS2SIQ (SI)(CX*4), AX
	MOVQ        AX, (DI)(CX*8)
	VCVTTSS2SIQ 4(SI)(CX*4), AX
	MOVQ        AX, 8(DI)(CX*8)
	VCVTTSS2SIQ 8(SI)(CX*4), AX
	MOVQ        AX, 16(DI)(CX*8)
	VCVTTSS2SIQ 12(SI)(CX*4), AX
	MOVQ        AX, 24(DI)(CX*8)
	ADDQ        $0x04, CX
	CMPQ        DX, CX
	JNE         LBB15_4

LBB15_5:
	TESTQ R8, R8
	JE    LBB15_8
	LEAQ  (DI)(CX*8), DX
	LEAQ  (SI)(CX*4), CX
	XORL  SI, SI

LBB15_7:
	VCVTTSS2SIQ (CX)(SI*4), AX
	MOVQ        AX, (DX)(SI*8)
	ADDQ        $0x01, SI
	CMPQ        R8, SI
	JNE         LBB15_7

LBB15_8:
	RET
